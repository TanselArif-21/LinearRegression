
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{LinearRegression}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \hypertarget{linear-regression}{%
\section{Linear Regression}\label{linear-regression}}

\emph{Linear Regression} is one of the simplest yet fundamental
statistical learning techniques. It is a great initial step towards more
advanced and computationally demanding methods.

This article aims to cover a statistically sound approach to Linear
Regression and its inferences while tying these to popular statistical
packages and reproducing the results.

We first begin with a brief description of Linear Regression and move on
to investigate it in light of a dataset.

    \hypertarget{description}{%
\subsection{1 - Description}\label{description}}

Linear regression examines the relationaship between a dependent
variable and one or more independent variables. Linear regression with
\(p\) independent variables focusses on fitting a straight line in
\(p+1\)-dimensions that passes as close as possible to the data points
in order to reduce error.

General Characteristics:

\begin{itemize}
\tightlist
\item
  A supervised learning technique
\item
  Useful for predicting a quantitative response
\item
  Linear Regression attempts to fit a function to predict a response
  variable
\item
  The problem is reduced to a parametric problem of finding a set of
  parameters
\item
  The function shape is limited (as a function of the parameters)
\end{itemize}

    \hypertarget{advertising-and-housing-datasets}{%
\subsection{2- Advertising and Housing
Datasets}\label{advertising-and-housing-datasets}}

Here we will use two datasets in order to get a feel of what Linear
Regression is capable of.

First we use the Advertising dataset which is obtained from
http://www-bcf.usc.edu/\textasciitilde{}gareth/ISL/data.html and
contains 200 datapoints of sales of a particular product, and TV,
newspaper and radio advertising budgets (all figures are in units of
\$1,000s). We will predict sales of a product given its advertising
budgets.

Then we use the HousePrice dataset which is obtained from
https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data
and contains 1460 houses along with many properties (only quantitative
properties) including their sales prices. We will preduct the sale price
of a property given certain parameters that characterise it.

First we import the required libraries

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{c+c1}{\PYZsh{} Import modules}
        \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
        \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
        \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
        \PY{k+kn}{import} \PY{n+nn}{seaborn} \PY{k}{as} \PY{n+nn}{sns}
        \PY{k+kn}{import} \PY{n+nn}{random}
        \PY{k+kn}{from} \PY{n+nn}{numpy}\PY{n+nn}{.}\PY{n+nn}{random} \PY{k}{import} \PY{n}{RandomState}
        \PY{k+kn}{import} \PY{n+nn}{math}
        \PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline
        
        \PY{k+kn}{import} \PY{n+nn}{statsmodels}\PY{n+nn}{.}\PY{n+nn}{api} \PY{k}{as} \PY{n+nn}{sm}
        \PY{k+kn}{from} \PY{n+nn}{scipy} \PY{k}{import} \PY{n}{stats}
        
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{linear\PYZus{}model} \PY{k}{import} \PY{n}{LinearRegression}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k}{import} \PY{n}{r2\PYZus{}score}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{train\PYZus{}test\PYZus{}split}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k}{import} \PY{n}{r2\PYZus{}score}\PY{p}{,}\PY{n}{mean\PYZus{}squared\PYZus{}error}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{model\PYZus{}selection} \PY{k}{import} \PY{n}{cross\PYZus{}val\PYZus{}score}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{metrics} \PY{k}{import} \PY{n}{mean\PYZus{}squared\PYZus{}error}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{linear\PYZus{}model} \PY{k}{import} \PY{n}{Ridge}
        
        \PY{k+kn}{import} \PY{n+nn}{random}
\end{Verbatim}


    Then we import the datasets

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{c+c1}{\PYZsh{} Import Advertising dataset (http://www\PYZhy{}bcf.usc.edu/\PYZti{}gareth/ISL/data.html)}
        \PY{n}{advert} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Advertising.csv}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}
        
        \PY{c+c1}{\PYZsh{} Import House Prices dataset \PYZhy{} Only quantitative fields and cleaned (https://www.kaggle.com/c/house\PYZhy{}prices\PYZhy{}advanced\PYZhy{}regression\PYZhy{}techniques/data)}
        \PY{n}{housePrice} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{HousePrice.csv}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}\PY{o}{.}\PY{n}{iloc}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Number of observations (n) in advertising file =}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{advert}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Number of predictor variables (p) in advertising file =}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{advert}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Advertising.csv}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n}{display}\PY{p}{(}\PY{n}{advert}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Number of observations (n) in advertising file = 200
Number of predictor variables (p) in advertising file = 3

Advertising.csv

    \end{Verbatim}

    
    \begin{verbatim}
      TV  radio  newspaper  sales
0  230.1   37.8       69.2   22.1
1   44.5   39.3       45.1   10.4
2   17.2   45.9       69.3    9.3
3  151.5   41.3       58.5   18.5
4  180.8   10.8       58.4   12.9
    \end{verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Number of observations (n) in house\PYZhy{}prices file =}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{housePrice}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Number of predictor variables (p) in house\PYZhy{}prices file =}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{housePrice}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{HousePrice.csv}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n}{display}\PY{p}{(}\PY{n}{housePrice}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Number of observations (n) in house-prices file = 1460
Number of predictor variables (p) in house-prices file = 34

HousePrice.csv

    \end{Verbatim}

    
    \begin{verbatim}
   LotArea  OverallQual  OverallCond  YearBuilt  YearRemodAdd  MasVnrArea  \
0     8450            7            5       2003          2003       196.0   
1     9600            6            8       1976          1976         0.0   
2    11250            7            5       2001          2002       162.0   
3     9550            7            5       1915          1970         0.0   
4    14260            8            5       2000          2000       350.0   

   BsmtFinSF1  BsmtFinSF2  BsmtUnfSF  TotalBsmtSF  ...  WoodDeckSF  \
0         706           0        150          856  ...           0   
1         978           0        284         1262  ...         298   
2         486           0        434          920  ...           0   
3         216           0        540          756  ...           0   
4         655           0        490         1145  ...         192   

   OpenPorchSF  EnclosedPorch  3SsnPorch  ScreenPorch  PoolArea  MiscVal  \
0           61              0          0            0         0        0   
1            0              0          0            0         0        0   
2           42              0          0            0         0        0   
3           35            272          0            0         0        0   
4           84              0          0            0         0        0   

   MoSold  YrSold  SalePrice  
0       2    2008     208500  
1       5    2007     181500  
2       9    2008     223500  
3       2    2006     140000  
4      12    2008     250000  

[5 rows x 35 columns]
    \end{verbatim}

    
    For the Advertising dataset the response variable is ``sales''. The
predictor variables are ``TV'', ``radio'' and ``newspaper''. It's useful
to visually inspect the data and see how each variable relates to the
others. Using seaborn we can produce a pairplot of the data seen below:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{n}{ax} \PY{o}{=} \PY{n}{sns}\PY{o}{.}\PY{n}{pairplot}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{advert}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_9_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    By looking at a pairplot to see the simple relationships between the
variables, we see a strong positive correlation between sales and TV. A
similar relationship between sales and radio is also observed. Newspaper
and radio seem to have a slight positive correlation also. We can use
the Pearson correlation given by:

\[corr=\frac{Cov(X,Y)}{\sigma_{X}\sigma_{Y}}\]

where \(X\) and \(Y\) are random variables, \(Cov(X,Y)\) is the
Covariance of \(X\) and \(Y\) and \(\sigma_{X}\) is the standard
deviation of \(X\). This allows us to examine the correlations between
the parameters as seen in the correlation matrix below.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{n}{advert}\PY{o}{.}\PY{n}{corr}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}6}]:}                  TV     radio  newspaper     sales
        TV         1.000000  0.054809   0.056648  0.782224
        radio      0.054809  1.000000   0.354104  0.576223
        newspaper  0.056648  0.354104   1.000000  0.228299
        sales      0.782224  0.576223   0.228299  1.000000
\end{Verbatim}
            
    We may want to fit a line to this data which is as close as possible. We
describe the Linear Regression model next and then apply it to this
data.

    \hypertarget{linear-regression}{%
\subsection{3- Linear Regression}\label{linear-regression}}

The idea behind \emph{Linear Regression} is that we reduce the problem
of estimating the response variable, \(Y\) = sales, by assuming there is
a linear function of the predictor variables, \(X_1\) = TV, \(X_2\) =
radio and \(X_3\) = newspaper which describes \(Y\). This reduces the
problem to that of solving for the parameters \(\beta_0\), \(\beta_1\),
\(\beta_2\) and \(\beta_3\) in the equation:

\[Y \approx \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 + \epsilon\]

where \(\epsilon\) is an error term. After approximating the
coefficients \(\beta_i\) as \(\hat{\beta}_i\), we obtain an
approximation, \(\hat{Y}\) of \(Y\). The coefficients \(\hat{\beta}_i\)
are obtained using the observed realisations of the random variables
\(X_i\). Namely, \(X_i = (x_{1i},x_{2i},x_{3i},...,x_{ni})\) are n
observations of \(X_i\) where \(i = 1,2,...,p\).

We first limit the problem to \(p=1\). For example, we are looking to
estimate the coefficients in the equation

\[Y \approx \beta_0 + \beta_1 X_1 + \epsilon\]

using the \(n\) data points
\((x_{11},y_{11}),(x_{21},y_{21}),...,(x_{n1},y_{n1})\). We can define
the prediction discrepency of a particular prediction as the difference
between the observed value and the predicted value. This is
representated in mathematical notation for observation \(i\) as
\(y_i - \hat{y}_i\). Letting
\(\hat{Y} = \hat{\beta}_0 + \hat{\beta}_1 X_1\) we have
\(y_i - \hat{y}_i = \epsilon_i\). i.e.~the error in the prediction of
point observation \(i\) (also called the ith \emph{residual}).

In summary, we are looking for a straight line to fit to the following
data points as well as possible:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{c+c1}{\PYZsh{} Get the figure handle and set figure size}
        \PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,}\PY{l+m+mi}{8}\PY{p}{)}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Get the axis}
        \PY{n}{axes} \PY{o}{=} \PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}axes}\PY{p}{(}\PY{p}{[}\PY{l+m+mf}{0.1}\PY{p}{,}\PY{l+m+mf}{0.1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Plot onto the axis}
        \PY{n}{axes}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{n}{advert}\PY{p}{,} \PY{n}{x}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{TV}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{y}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{sales}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Set the labels and title}
        \PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{x}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{f\PYZus{}1(x)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{The relationship between Y = Sales and X = TV in }\PY{l+s+se}{\PYZbs{}}
        \PY{l+s+s1}{the advertising dataset}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_14_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    In order to calculate appropriate values for parameters \(\beta_i\), we
would need a method of defining what it means for a line to be a good
fit. A popular method is ``Ordinary Least Squares''. This method relies
on minimising the Residual Sum of Squared errors (RSS). i.e.~we are
looking to minimise \(RSS = \sum_{i=1}^n \epsilon_i^2\). While this
intuitively makes sense, this can also be arrived at using a
\emph{Maximum Likelihood Estimation} (MLE) approach (see Appendix A2).

For the 1-parameter case we have that (the semi-colon below means `the
value of the parameters' given `the data we have observed')

\[RSS(\hat{\beta}_0,\hat{\beta}_1;X) = \sum_{i=1}^n \epsilon_i^2 = \sum_{i=1}^n (y_i-\hat{\beta}_0 - \hat{\beta}_1 x_i)^2\]

We would like to find the parameters \((\beta_0,\beta_1)\) which
minimise RSS. We first find the partial derivates:

\[\frac{\partial RSS}{\partial \hat{\beta_0}} = -2 [ \sum_{i=1}^n y_i - \sum_{i=1}^n \hat{\beta}_0 - \sum_{i=1}^n \hat{\beta}_1 x_i]\]

\[\frac{\partial RSS}{\partial \hat{\beta_1}} = -2 [ \sum_{i=1}^n y_i x_i - \sum_{i=1}^n \hat{\beta}_0 x_i - \sum_{i=1}^n \hat{\beta}_1 x_i^2]\]

Then setting these to zero and solving

\[\frac{\partial RSS}{\partial \hat{\beta_0}} = 0 \implies  \hat{\beta}_0 = \frac{\sum_{i=1}^n y_i - \hat{\beta}_1 \sum_{i=1}^n y_i}{n} = \frac{n \bar{y} - \hat{\beta}_1 n \bar{x}}{n} = \bar{y} - \hat{\beta}_1 \bar{x}\]

\[\frac{\partial RSS}{\partial \hat{\beta_1}} = 0 \implies  \sum_{i=1}^n y_i x_i - \hat{\beta}_0 \sum_{i=1}^n x_i - \hat{\beta}_1 \sum_{i=1}^n x_i^2 = 0\]

\[\implies \hat{\beta}_1 = \frac{n \bar{y} \bar{x} - \sum_{i=1}^n y_i x_i}{n \bar{x}^2 - \sum_{i=1}^n x_i^2} = \frac{\sum_{i=1}^n y_i x_i - n \bar{y} \bar{x}}{\sum_{i=1}^n x_i^2 - n \bar{x}^2} = \frac{\sum_{i=1}^n y_i x_i - n \bar{y} \bar{x} - n \bar{y} \bar{x} + n\bar{y} \bar{x}}{\sum_{i=1}^n x_i^2 - n \bar{x}^2 -n\bar{x}^2 + n\bar{x}^2}\]

\[= \frac{\sum_{i=1}^n x_i y_i - \sum_{i=1}^n y_i \bar{x} - \sum_{i=1}^n x_i \bar{y}  + \sum_{i=1}^n \bar{y} \bar{x}}{\sum_{i=1}^n x_i^2 - \sum_{i=1}^n x_i \bar{x} - \sum_{i=1}^n x_i \bar{x} + \sum_{i=1}^n \bar{x}^2},\]

where we used
\(n\bar{y} \bar{x} = \sum_{i=1}^n y_i \bar{x} = \sum_{i=1}^n x_i \bar{y}\)
and \(n\bar{x}^2 = n\bar{x} \bar{x} = \sum_{i=1}^n x_i \bar{x}\).
Factorising

\[\hat{\beta}_1 = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2}\]

Additionally, we can show that the point \((\bar{x},\bar{y})\) lies on
the regression line (see Appendix A3).

We have now found the values of \((\hat{\beta}_0,\hat{\beta}_1)\) which
corresponds to the extrema of RSS. We will still need to show that this
is indeed a minima.

From Calculus, we know that if
\(\frac{\partial^2 RSS}{\partial \hat{\beta}_0 ^2} \frac{\partial^2 RSS}{\partial \hat{\beta}_1 ^2} - (\frac{\partial^2 RSS}{\partial \hat{\beta}_0 \partial \hat{\beta}_1})^2 > 0\),
this is an extrema and not an inflexion point. Additionally, if
\(\frac{\partial^2 RSS}{\partial \hat{\beta}_0 ^2} > 0\) and
\(\frac{\partial^2 RSS}{\partial \hat{\beta}_1 ^2} > 0\) this is a
minima.

We have that

\[\frac{\partial^2 RSS}{\partial \hat{\beta}_0 ^2} = 2n > 0\]
\[\frac{\partial^2 RSS}{\partial \hat{\beta}_1 ^2} = 2 \sum_{i=1}^n x_i^2 > 0\]
\[\frac{\partial^2 RSS}{\partial \hat{\beta}_0 \partial \hat{\beta}_1} = 2 \sum_{i=1}^n x_i\]

So,

\(\frac{\partial^2 RSS}{\partial \hat{\beta}_0 ^2} \frac{\partial^2 RSS}{\partial \hat{\beta}_1 ^2} - (\frac{\partial^2 RSS}{\partial \hat{\beta}_0 \partial \hat{\beta}_1})^2 = (2n) (2 \sum_{i=1}^n x_i^2) - (2 \sum_{i=1}^n x_i)^2 > 0 \; \forall \; n>1\)
(see Appendix A1).

This means that this is indeed a minima (since we have satisfied the
conditions stated above).

The equation

\[\hat{Y} = \hat{\beta}_0 + \hat{\beta}_1 X_1\]

then defines a straight line of best fit which minimises the expected
value of the errors (residuals). From the form of this line, we can see
that \(\hat{\beta}_0\) corresponds to the value of \(\hat{Y}\) if the
independent variable \(X_1\) is zero. \(\hat{\beta}_1\) is then the
gradient.

In the following we construct 3 functions dependent on a single
independent variable and attach an error term and calculate the best
fit. The three functions are chosen as:

1- \(f_1(x) = 4.67 + 5.07*x\)

2- \(f_2(x) = 4.67 + 5.07*x^2\)

3- \(f_3(x) = 4.67 + 5.07*sin(x/20)\)

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{c+c1}{\PYZsh{}f\PYZus{}1(x)=4.67+5.07∗x}
        \PY{k}{def} \PY{n+nf}{f\PYZus{}1}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{:}
            \PY{k}{return} \PY{l+m+mf}{4.67} \PY{o}{+} \PY{l+m+mf}{5.07}\PY{o}{*}\PY{n}{x}
        
        \PY{c+c1}{\PYZsh{}f\PYZus{}2(x)=4.67+5.07∗x2}
        \PY{k}{def} \PY{n+nf}{f\PYZus{}2}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{:}
            \PY{k}{return} \PY{l+m+mf}{4.67} \PY{o}{+} \PY{l+m+mf}{5.07}\PY{o}{*}\PY{n}{x}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}
        
        \PY{c+c1}{\PYZsh{}f\PYZus{}3(x)=4.67+5.07∗sin(x/20)}
        \PY{k}{def} \PY{n+nf}{f\PYZus{}3}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{:}
            \PY{k}{return} \PY{l+m+mf}{4.67} \PY{o}{+} \PY{l+m+mf}{5.07}\PY{o}{*}\PY{n}{math}\PY{o}{.}\PY{n}{sin}\PY{p}{(}\PY{n}{x}\PY{o}{/}\PY{l+m+mi}{20}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} \PY{c+c1}{\PYZsh{} Set the seed}
        \PY{n}{r} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{RandomState}\PY{p}{(}\PY{l+m+mi}{101}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Choose 1000 random observations for x between 0 and 100}
        \PY{n}{X} \PY{o}{=} \PY{l+m+mi}{100}\PY{o}{*}\PY{n}{r}\PY{o}{.}\PY{n}{rand}\PY{p}{(}\PY{l+m+mi}{1000}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{}Error term with sigma = 10, mu = 0, randn samples from the standard normal distribution}
        \PY{n}{E\PYZus{}1} \PY{o}{=} \PY{l+m+mi}{10}\PY{o}{*}\PY{n}{r}\PY{o}{.}\PY{n}{randn}\PY{p}{(}\PY{l+m+mi}{1000}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{}Error term with sigma = 500, mu = 0}
        \PY{n}{E\PYZus{}2} \PY{o}{=} \PY{l+m+mi}{500}\PY{o}{*}\PY{n}{r}\PY{o}{.}\PY{n}{randn}\PY{p}{(}\PY{l+m+mi}{1000}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{}Error term with sigma = 1, mu = 0}
        \PY{n}{E\PYZus{}3} \PY{o}{=} \PY{l+m+mi}{1}\PY{o}{*}\PY{n}{r}\PY{o}{.}\PY{n}{randn}\PY{p}{(}\PY{l+m+mi}{1000}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{}Response variables}
        \PY{n}{Y\PYZus{}1} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n+nb}{map}\PY{p}{(}\PY{n}{f\PYZus{}1}\PY{p}{,}\PY{n}{X}\PY{p}{)}\PY{p}{)}\PY{o}{+}\PY{n}{E\PYZus{}1}
        \PY{n}{Y\PYZus{}2} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n+nb}{map}\PY{p}{(}\PY{n}{f\PYZus{}2}\PY{p}{,}\PY{n}{X}\PY{p}{)}\PY{p}{)}\PY{o}{+}\PY{n}{E\PYZus{}2}
        \PY{n}{Y\PYZus{}3} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n+nb}{map}\PY{p}{(}\PY{n}{f\PYZus{}3}\PY{p}{,}\PY{n}{X}\PY{p}{)}\PY{p}{)}\PY{o}{+}\PY{n}{E\PYZus{}3}
\end{Verbatim}


    In the above, \emph{s \(\times\) r.randn(n)} samples n points from the
\(N(0,s^2)\) distribution. First we look at what \(f_1\) looks like

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{c+c1}{\PYZsh{} Plot}
         \PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,}\PY{l+m+mi}{8}\PY{p}{)}\PY{p}{)}
         \PY{n}{axes} \PY{o}{=} \PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}axes}\PY{p}{(}\PY{p}{[}\PY{l+m+mf}{0.1}\PY{p}{,}\PY{l+m+mf}{0.1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
         \PY{n}{axes}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{X}\PY{p}{,}\PY{n}{Y\PYZus{}1}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Set labels and title}
         \PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{x}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{f\PYZus{}1(x)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Scatter plot of f\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_19_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    The task is to fit the model
\(\hat{Y} = \hat{\beta}_0 + \hat{\beta}_1 X_1\) to the data. We know
that

\[\hat{\beta}_1 = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2}\]

and

\[\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}\]

We can calculate these as below

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{c+c1}{\PYZsh{}Find the mean of the data for f\PYZus{}1}
         \PY{n}{x\PYZus{}bar1} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{X}\PY{p}{)}
         \PY{n}{y\PYZus{}bar1} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{Y\PYZus{}1}\PY{p}{)}
         
         \PY{n}{numerator} \PY{o}{=} \PY{l+m+mi}{0}
         \PY{n}{denominator} \PY{o}{=} \PY{l+m+mi}{0}
         
         \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{Y\PYZus{}1}\PY{p}{)}\PY{p}{)}\PY{p}{:}
             \PY{c+c1}{\PYZsh{} Add to the numerator for beta\PYZus{}1}
             \PY{n}{numerator} \PY{o}{+}\PY{o}{=} \PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{n}{x\PYZus{}bar1}\PY{p}{)}\PY{o}{*}\PY{p}{(}\PY{n}{Y\PYZus{}1}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{n}{y\PYZus{}bar1}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{} Add to the denominator for beta\PYZus{}1}
             \PY{n}{denominator} \PY{o}{+}\PY{o}{=} \PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{n}{x\PYZus{}bar1}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}
             
         \PY{n}{beta1\PYZus{}1} \PY{o}{=} \PY{n}{numerator}\PY{o}{/}\PY{n}{denominator}
         \PY{n}{beta1\PYZus{}0} \PY{o}{=} \PY{n}{y\PYZus{}bar1} \PY{o}{\PYZhy{}} \PY{n}{beta1\PYZus{}1}\PY{o}{*}\PY{n}{x\PYZus{}bar1}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Y = }\PY{l+s+si}{\PYZob{}beta\PYZus{}0\PYZcb{}}\PY{l+s+s1}{ + }\PY{l+s+si}{\PYZob{}beta\PYZus{}1\PYZcb{}}\PY{l+s+s1}{ * X}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PYZbs{}
               \PY{n+nb}{format}\PY{p}{(}\PY{n}{beta\PYZus{}0} \PY{o}{=} \PY{n}{beta1\PYZus{}0}\PY{p}{,} \PY{n}{beta\PYZus{}1} \PY{o}{=} \PY{n}{beta1\PYZus{}1}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Y = 5.50124312485292 + 5.064254524922961 * X

    \end{Verbatim}

    Below, we see how the line defined by the equation above fits the data
for \(f_1\)

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{c+c1}{\PYZsh{} 1000 linearly spaced numbers}
         \PY{n}{x1} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{99}\PY{p}{,}\PY{l+m+mi}{1000}\PY{p}{)} 
         
         \PY{c+c1}{\PYZsh{} The equation using the betas above}
         \PY{n}{y1} \PY{o}{=} \PY{n}{beta1\PYZus{}0} \PY{o}{+} \PY{n}{beta1\PYZus{}1} \PY{o}{*} \PY{n}{x1} 
         
         \PY{c+c1}{\PYZsh{} Plot the observed data}
         \PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,}\PY{l+m+mi}{8}\PY{p}{)}\PY{p}{)}
         \PY{n}{axes} \PY{o}{=} \PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}axes}\PY{p}{(}\PY{p}{[}\PY{l+m+mf}{0.1}\PY{p}{,}\PY{l+m+mf}{0.1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
         \PY{n}{axes}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{X}\PY{p}{,}\PY{n}{Y\PYZus{}1}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Plot the regression line}
         \PY{n}{axes}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x1}\PY{p}{,}\PY{n}{y1}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Set labels and title}
         \PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{x}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{f\PYZus{}1(x)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{A plot of the data for f\PYZus{}1 and the regression line}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_23_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Let's see what the residuals look like by plotting them. The residuals
require the knowledge of the actual response variables so that we can
compare them with the predicted response variables. So we use the
regression line above to predict the response variable using the
observed predictor variables. Then we plot them using a histogram to
gain some insight into their distribution

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} \PY{c+c1}{\PYZsh{} The fitted values are the predicted values given the observed values}
         \PY{n}{y1\PYZus{}fitted} \PY{o}{=} \PY{n}{beta1\PYZus{}0} \PY{o}{+} \PY{n}{beta1\PYZus{}1} \PY{o}{*} \PY{n}{X}
         
         \PY{c+c1}{\PYZsh{} The residuals are the differences between our predicted values and }
         \PY{c+c1}{\PYZsh{} the observed responses}
         \PY{n}{Res\PYZus{}1} \PY{o}{=} \PY{n}{y1\PYZus{}fitted} \PY{o}{\PYZhy{}} \PY{n}{Y\PYZus{}1}
         
         \PY{c+c1}{\PYZsh{} Plot the residuals}
         \PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,}\PY{l+m+mi}{8}\PY{p}{)}\PY{p}{)}
         \PY{n}{axes} \PY{o}{=} \PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}axes}\PY{p}{(}\PY{p}{[}\PY{l+m+mf}{0.1}\PY{p}{,}\PY{l+m+mf}{0.1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
         \PY{n}{axes}\PY{o}{.}\PY{n}{hist}\PY{p}{(}\PY{n}{Res\PYZus{}1}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Set labels and title}
         \PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Residuals}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Count}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{A histogram of the residuals for f\PYZus{}1 when }\PY{l+s+se}{\PYZbs{}}
         \PY{l+s+s1}{fitted with the regression line}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{This is roughly a normal distribution with mean }\PY{l+s+si}{\PYZob{}mean\PYZcb{}}\PY{l+s+s1}{ }\PY{l+s+se}{\PYZbs{}n}\PY{l+s+se}{\PYZbs{}}
         \PY{l+s+s1}{and standard deviation }\PY{l+s+si}{\PYZob{}std\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{mean}\PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{Res\PYZus{}1}\PY{p}{)}\PY{p}{,}\PY{n}{std}\PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{std}\PY{p}{(}\PY{n}{Res\PYZus{}1}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_25_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
This is roughly a normal distribution with mean -1.2157386208855315e-14 
and standard deviation 10.08588495757817

    \end{Verbatim}

    Since the residuals are roughly normally distributed, our model may be a
good choice. In fact, the standard deviation for the residuals was
roughly equal to the standard deviation for the error term when we
constructed the function \(f_1\). A model may suffer from two types of
error: * error due to a discrepancy between the chosen function shape
(here a linear model) and the true function shape (this is the reducible
error), and * error due to random noise (this is the irreducible error).
We can see here that the residuals are from irreducible error.

Above we fitted a linear model to our `designed' linear data. The error
terms we expect to get are irreducible and a result of the error term E1
added above.

Now let's do the same for f\_2.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{c+c1}{\PYZsh{} Get figure handle}
         \PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,}\PY{l+m+mi}{8}\PY{p}{)}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Get axis handle and specify size}
         \PY{n}{axes} \PY{o}{=} \PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}axes}\PY{p}{(}\PY{p}{[}\PY{l+m+mf}{0.1}\PY{p}{,}\PY{l+m+mf}{0.1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Plot onto this axis}
         \PY{n}{axes}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{X}\PY{p}{,}\PY{n}{Y\PYZus{}2}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Set the axis labels}
         \PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{x}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{f\PYZus{}2(x)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Scatter plot of f\PYZus{}2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}14}]:} Text(0.5,1,'Scatter plot of f\_2')
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_27_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}15}]:} \PY{c+c1}{\PYZsh{}Find the mean of the data for f\PYZus{}2}
         \PY{n}{x\PYZus{}bar2} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{X}\PY{p}{)}
         \PY{n}{y\PYZus{}bar2} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{Y\PYZus{}2}\PY{p}{)}
         
         \PY{n}{numerator} \PY{o}{=} \PY{l+m+mi}{0}
         \PY{n}{denominator} \PY{o}{=} \PY{l+m+mi}{0}
         
         \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{Y\PYZus{}2}\PY{p}{)}\PY{p}{)}\PY{p}{:}
             \PY{c+c1}{\PYZsh{} Add to the numerator for beta\PYZus{}1}
             \PY{n}{numerator} \PY{o}{+}\PY{o}{=} \PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{n}{x\PYZus{}bar2}\PY{p}{)}\PY{o}{*}\PY{p}{(}\PY{n}{Y\PYZus{}2}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{n}{y\PYZus{}bar2}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{} Add to the denominator for beta\PYZus{}1}
             \PY{n}{denominator} \PY{o}{+}\PY{o}{=} \PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{n}{x\PYZus{}bar2}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}
             
         \PY{n}{beta2\PYZus{}1} \PY{o}{=} \PY{n}{numerator}\PY{o}{/}\PY{n}{denominator}
         \PY{n}{beta2\PYZus{}0} \PY{o}{=} \PY{n}{y\PYZus{}bar2} \PY{o}{\PYZhy{}} \PY{n}{beta2\PYZus{}1}\PY{o}{*}\PY{n}{x\PYZus{}bar2}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Y = }\PY{l+s+si}{\PYZob{}beta\PYZus{}0\PYZcb{}}\PY{l+s+s1}{ + }\PY{l+s+si}{\PYZob{}beta\PYZus{}1\PYZcb{}}\PY{l+s+s1}{ * X}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{beta\PYZus{}0} \PY{o}{=} \PY{n}{beta2\PYZus{}0}\PY{p}{,} \PY{n}{beta\PYZus{}1} \PY{o}{=} \PY{n}{beta2\PYZus{}1}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Y = -8445.98030682202 + 506.16066894401735 * X

    \end{Verbatim}

    Below, we see how the line defined by the equation above fits the data
for \(f_2\)

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}16}]:} \PY{c+c1}{\PYZsh{} 1000 linearly spaced numbers}
         \PY{n}{x2} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{99}\PY{p}{,}\PY{l+m+mi}{1000}\PY{p}{)} 
         
         \PY{c+c1}{\PYZsh{} The predicted responses of these 1000 numbers}
         \PY{n}{y2} \PY{o}{=} \PY{n}{beta2\PYZus{}0} \PY{o}{+} \PY{n}{beta2\PYZus{}1} \PY{o}{*} \PY{n}{x2}
         
         \PY{c+c1}{\PYZsh{} Plot}
         \PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,}\PY{l+m+mi}{8}\PY{p}{)}\PY{p}{)}
         \PY{n}{axes} \PY{o}{=} \PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}axes}\PY{p}{(}\PY{p}{[}\PY{l+m+mf}{0.1}\PY{p}{,}\PY{l+m+mf}{0.1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
         \PY{n}{axes}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{X}\PY{p}{,}\PY{n}{Y\PYZus{}2}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{axes}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x2}\PY{p}{,}\PY{n}{y2}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Set labels and title}
         \PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{x}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{f\PYZus{}2(x)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{A plot of the data and the regression fit for f\PYZus{}2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_30_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    We can then look at the residuals plot as we did before

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}17}]:} \PY{c+c1}{\PYZsh{} The fitted values are the predicted values given the observed values}
         \PY{n}{y2\PYZus{}fitted} \PY{o}{=} \PY{n}{beta2\PYZus{}0} \PY{o}{+} \PY{n}{beta2\PYZus{}1} \PY{o}{*} \PY{n}{X}
         
         \PY{c+c1}{\PYZsh{} The residuals are the differences between our predicted values and }
         \PY{c+c1}{\PYZsh{} the observed responses}
         \PY{n}{Res\PYZus{}2} \PY{o}{=} \PY{n}{y2\PYZus{}fitted} \PY{o}{\PYZhy{}} \PY{n}{Y\PYZus{}2}
         
         
         \PY{c+c1}{\PYZsh{} Plot the residuals}
         \PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,}\PY{l+m+mi}{8}\PY{p}{)}\PY{p}{)}
         \PY{n}{axes} \PY{o}{=} \PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}axes}\PY{p}{(}\PY{p}{[}\PY{l+m+mf}{0.1}\PY{p}{,}\PY{l+m+mf}{0.1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
         \PY{n}{axes}\PY{o}{.}\PY{n}{hist}\PY{p}{(}\PY{n}{Res\PYZus{}2}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Set labels and title}
         \PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Residuals}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Count}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{A histogram of the residuals for f\PYZus{}2 when fitted with the regression line}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{The residuals are certainly not from a normal distribution}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_32_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
The residuals are certainly not from a normal distribution

    \end{Verbatim}

    This shows that the linear model we have chosen may not be a good
choice. We can try \(X^2\) as a parameter instead of \(X\) in our linear
model. This way, we are transforming an existing parameter to form a new
parameter.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}18}]:} \PY{c+c1}{\PYZsh{} Create X\PYZca{}2 parameter}
         \PY{n}{X\PYZus{}2} \PY{o}{=} \PY{n}{X}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}
         
         \PY{c+c1}{\PYZsh{}Find the mean of the data for f\PYZus{}2}
         \PY{n}{x\PYZus{}bar22} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{X\PYZus{}2}\PY{p}{)}
         \PY{n}{y\PYZus{}bar22} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{Y\PYZus{}2}\PY{p}{)}
         
         \PY{n}{numerator} \PY{o}{=} \PY{l+m+mi}{0}
         \PY{n}{denominator} \PY{o}{=} \PY{l+m+mi}{0}
         
         \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{Y\PYZus{}2}\PY{p}{)}\PY{p}{)}\PY{p}{:}
             \PY{c+c1}{\PYZsh{} Calculate the numerator for beta\PYZus{}1}
             \PY{n}{numerator} \PY{o}{+}\PY{o}{=} \PY{p}{(}\PY{n}{X\PYZus{}2}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{n}{x\PYZus{}bar22}\PY{p}{)}\PY{o}{*}\PY{p}{(}\PY{n}{Y\PYZus{}2}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{n}{y\PYZus{}bar22}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{} Calculate the denominator for beta\PYZus{}1}
             \PY{n}{denominator} \PY{o}{+}\PY{o}{=} \PY{p}{(}\PY{n}{X\PYZus{}2}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{n}{x\PYZus{}bar22}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}
             
         \PY{n}{beta22\PYZus{}1} \PY{o}{=} \PY{n}{numerator}\PY{o}{/}\PY{n}{denominator}
         \PY{n}{beta22\PYZus{}0} \PY{o}{=} \PY{n}{y\PYZus{}bar22} \PY{o}{\PYZhy{}} \PY{n}{beta22\PYZus{}1}\PY{o}{*}\PY{n}{x\PYZus{}bar22}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Y = }\PY{l+s+si}{\PYZob{}beta\PYZus{}0\PYZcb{}}\PY{l+s+s1}{ + }\PY{l+s+si}{\PYZob{}beta\PYZus{}1\PYZcb{}}\PY{l+s+s1}{ * X\PYZca{}2}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{beta\PYZus{}0} \PY{o}{=} \PY{n}{beta22\PYZus{}0}\PY{p}{,} \PY{n}{beta\PYZus{}1} \PY{o}{=} \PY{n}{beta22\PYZus{}1}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Y = 14.470063153316005 + 5.075020979320466 * X\^{}2

    \end{Verbatim}

    Below, we see how the new line defined by the equation above fits the
data for \(f_2\)

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}19}]:} \PY{c+c1}{\PYZsh{} 1000 linearly spaced numbers}
         \PY{n}{x22} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{99}\PY{p}{,}\PY{l+m+mi}{1000}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Predicted responses to the 1000 numbers}
         \PY{n}{y22} \PY{o}{=} \PY{n}{beta22\PYZus{}0} \PY{o}{+} \PY{n}{beta22\PYZus{}1} \PY{o}{*} \PY{p}{(}\PY{p}{(}\PY{n}{x22}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Plot this regression line and the data}
         \PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,}\PY{l+m+mi}{8}\PY{p}{)}\PY{p}{)}
         \PY{n}{axes} \PY{o}{=} \PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}axes}\PY{p}{(}\PY{p}{[}\PY{l+m+mf}{0.1}\PY{p}{,}\PY{l+m+mf}{0.1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
         \PY{n}{axes}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{X}\PY{p}{,}\PY{n}{Y\PYZus{}2}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{axes}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x22}\PY{p}{,}\PY{n}{y22}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Set labels and title}
         \PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{x}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{f\PYZus{}2(x)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{A plot of the data and the new regression fit for f\PYZus{}2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_36_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    We see a much better fit. Now we investigate the residuals to see if the
new regression fit using \(X^2\) as a parameter yields residuals that
look more normally distributed as has been assumed by the model
architecture

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}20}]:} \PY{c+c1}{\PYZsh{} The fitted values are the predicted values given the observed values}
         \PY{n}{y22\PYZus{}fitted} \PY{o}{=} \PY{n}{beta22\PYZus{}0} \PY{o}{+} \PY{n}{beta22\PYZus{}1} \PY{o}{*} \PY{n}{X}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}
         
         \PY{c+c1}{\PYZsh{} The residuals are the differences between our predicted values and }
         \PY{c+c1}{\PYZsh{} the observed responses}
         \PY{n}{Res\PYZus{}22} \PY{o}{=} \PY{n}{y22\PYZus{}fitted} \PY{o}{\PYZhy{}} \PY{n}{Y\PYZus{}2}
         
         \PY{c+c1}{\PYZsh{} Plot the residuals}
         \PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,}\PY{l+m+mi}{8}\PY{p}{)}\PY{p}{)}
         \PY{n}{axes} \PY{o}{=} \PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}axes}\PY{p}{(}\PY{p}{[}\PY{l+m+mf}{0.1}\PY{p}{,}\PY{l+m+mf}{0.1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
         \PY{n}{axes}\PY{o}{.}\PY{n}{hist}\PY{p}{(}\PY{n}{Res\PYZus{}22}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Set labels and title}
         \PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Residuals}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Count}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{A histogram of the residuals for f\PYZus{}2 when fitted with the new regression line}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{This is roughly a normal distribution with mean }\PY{l+s+si}{\PYZob{}mean\PYZcb{}}\PY{l+s+s1}{ and standard deviation }\PY{l+s+si}{\PYZob{}std\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PYZbs{}
               \PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{mean}\PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{Res\PYZus{}22}\PY{p}{)}\PY{p}{,}\PY{n}{std}\PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{std}\PY{p}{(}\PY{n}{Res\PYZus{}22}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_38_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
This is roughly a normal distribution with mean -1.1250449460931123e-12 and standard deviation 518.5583078222347

    \end{Verbatim}

    This shows that we can transform an independent variable and apply
linear regression in order to \emph{regress} the response variable onto
the transformed explanatory variable. This increases the power of linear
regression techniques. Note also that the standard deviation from the
residual distribution is close to the 500 for the errors when the
function was created.

Now let's apply linear regression to f\_3 in a similar manner

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}21}]:} \PY{c+c1}{\PYZsh{} Get figure handle}
         \PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,}\PY{l+m+mi}{8}\PY{p}{)}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Get axis handle and specify size}
         \PY{n}{axes} \PY{o}{=} \PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}axes}\PY{p}{(}\PY{p}{[}\PY{l+m+mf}{0.1}\PY{p}{,}\PY{l+m+mf}{0.1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Plot onto this axis}
         \PY{n}{axes}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{X}\PY{p}{,}\PY{n}{Y\PYZus{}3}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Set the axis labels}
         \PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{x}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{f\PYZus{}3(x)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Scatter plot of f\PYZus{}3}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_40_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    It is very clear from the above scatter plot that we will not be able to
get away with fitting a linear line to the data. This is a hint that we
should use transformed variables. But let's carry out a linear fit to
show that the results can be misleading when we only consider the
residuals plot to assess the quality of fit

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}22}]:} \PY{c+c1}{\PYZsh{}Find the mean of the data for f\PYZus{}3}
         \PY{n}{x\PYZus{}bar3} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{X}\PY{p}{)}
         \PY{n}{y\PYZus{}bar3} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{Y\PYZus{}3}\PY{p}{)}
         
         \PY{n}{numerator} \PY{o}{=} \PY{l+m+mi}{0}
         \PY{n}{denominator} \PY{o}{=} \PY{l+m+mi}{0}
         
         \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{Y\PYZus{}3}\PY{p}{)}\PY{p}{)}\PY{p}{:}
             \PY{n}{numerator} \PY{o}{+}\PY{o}{=} \PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{n}{x\PYZus{}bar3}\PY{p}{)}\PY{o}{*}\PY{p}{(}\PY{n}{Y\PYZus{}3}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{n}{y\PYZus{}bar3}\PY{p}{)}
             \PY{n}{denominator} \PY{o}{+}\PY{o}{=} \PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{n}{x\PYZus{}bar3}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}
             
         \PY{n}{beta3\PYZus{}1} \PY{o}{=} \PY{n}{numerator}\PY{o}{/}\PY{n}{denominator}
         \PY{n}{beta3\PYZus{}0} \PY{o}{=} \PY{n}{y\PYZus{}bar3} \PY{o}{\PYZhy{}} \PY{n}{beta3\PYZus{}1}\PY{o}{*}\PY{n}{x\PYZus{}bar3}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Y = }\PY{l+s+si}{\PYZob{}beta\PYZus{}0\PYZcb{}}\PY{l+s+s1}{ + }\PY{l+s+si}{\PYZob{}beta\PYZus{}1\PYZcb{}}\PY{l+s+s1}{ * X}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{beta\PYZus{}0} \PY{o}{=} \PY{n}{beta3\PYZus{}0}\PY{p}{,} \PY{n}{beta\PYZus{}1} \PY{o}{=} \PY{n}{beta3\PYZus{}1}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Y = 10.511143457700811 + -0.1011987818100197 * X

    \end{Verbatim}

    Below, we see how the line defined by the equation above fits the data
for \(f_3\)

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}23}]:} \PY{c+c1}{\PYZsh{} 1000 linearly spaced numbers}
         \PY{n}{x3} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{99}\PY{p}{,}\PY{l+m+mi}{1000}\PY{p}{)} 
         
         \PY{c+c1}{\PYZsh{} Predict the response for those numbers}
         \PY{n}{y3} \PY{o}{=} \PY{n}{beta3\PYZus{}0} \PY{o}{+} \PY{n}{beta3\PYZus{}1} \PY{o}{*} \PY{n}{x3}
         
         \PY{c+c1}{\PYZsh{} Plot both the data and the fit}
         \PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,}\PY{l+m+mi}{8}\PY{p}{)}\PY{p}{)}
         \PY{n}{axes} \PY{o}{=} \PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}axes}\PY{p}{(}\PY{p}{[}\PY{l+m+mf}{0.1}\PY{p}{,}\PY{l+m+mf}{0.1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
         \PY{n}{axes}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{X}\PY{p}{,}\PY{n}{Y\PYZus{}3}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{axes}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x3}\PY{p}{,}\PY{n}{y3}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Set the labels and title}
         \PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{x}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{f\PYZus{}3(x)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{A plot of the data and the new regression fit for f\PYZus{}3}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_44_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    We now assess the residuals

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}24}]:} \PY{c+c1}{\PYZsh{} The fitted values are the predicted values given the observed values}
         \PY{n}{y3\PYZus{}fitted} \PY{o}{=} \PY{n}{beta3\PYZus{}0} \PY{o}{+} \PY{n}{beta3\PYZus{}1} \PY{o}{*} \PY{n}{X}
         
         \PY{c+c1}{\PYZsh{} The residuals are the differences between our predicted values and }
         \PY{c+c1}{\PYZsh{} the observed responses}
         \PY{n}{Res\PYZus{}3} \PY{o}{=} \PY{n}{y3\PYZus{}fitted} \PY{o}{\PYZhy{}} \PY{n}{Y\PYZus{}3}
         
         \PY{c+c1}{\PYZsh{} Plot the residuals}
         \PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,}\PY{l+m+mi}{8}\PY{p}{)}\PY{p}{)}
         \PY{n}{axes} \PY{o}{=} \PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}axes}\PY{p}{(}\PY{p}{[}\PY{l+m+mf}{0.1}\PY{p}{,}\PY{l+m+mf}{0.1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
         \PY{n}{axes}\PY{o}{.}\PY{n}{hist}\PY{p}{(}\PY{n}{Res\PYZus{}3}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Set labels and title}
         \PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Residuals}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Count}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{A histogram of the residuals for f\PYZus{}3 when fitted with the regression line}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{This not a normal distribution but it is not that far off.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_46_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
This not a normal distribution but it is not that far off.

    \end{Verbatim}

    \hypertarget{alternative-view}{%
\subsubsection{Alternative View}\label{alternative-view}}

Often in Machine Learning, we pose a hypothesis (\(h_{\theta}(X)\)) and
a cost function (\(J(\theta)\)) and proceed to minimise this cost
function. Here, \(X\) is the data and \(\theta\) is a vector of
parameters (such as the \(\beta\) in the Linear Regression models
above).

For Linear Regression as stated above, the hypothesis function is that
there is a straight line passing through all the data points:

\[h_{\theta}(X) = \theta_0 + \theta_1 X_1 + \theta_2 X_2 + \theta_3 X_3 + ... = X \theta\]

The Cost function is the least squares sum residuals (eventually written
in index notation):

\[J(\theta) = \sum_{i=1}^n e_i^2 = \sum_{i=1}^n (h_{\theta}(X^{(i)}) - Y^{(i)})^2 = (X \theta - Y)^T (X \theta - Y) = (X \theta)^T X\theta - 2 (X \theta)^T Y + Y^T Y = \theta_j x_{ji} x_{ij} \theta_{j} - 2 \theta_j x_{ji} y_i\]

where the superscript \(^{(i)}\) refers to the ith observation. Taking
the derivative of the cost function:

\[\frac{\partial J(\theta)}{\partial \theta_k} = 2 x_{ki} x_{ik} \theta_k - 2 x_{ki} y_i\]

Setting this to zero for all \(k\) and solving:

\[\theta = (X^T X)^{-1} X^T Y\]

Let's test this out with the sine curve above by considering
\(X\),\(X^2\) and \(X^3\) as predictor variables.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}25}]:} \PY{c+c1}{\PYZsh{} Calculate x\PYZca{}2}
         \PY{n}{X2} \PY{o}{=} \PY{n}{X}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}
         
         \PY{c+c1}{\PYZsh{} Calculate X\PYZca{}3}
         \PY{n}{X3} \PY{o}{=} \PY{n}{X}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{3}
         
         \PY{c+c1}{\PYZsh{} Combine into a single array (n X 3)}
         \PY{n}{X\PYZus{}full} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{concatenate}\PY{p}{(}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}\PY{n}{X2}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}\PY{n}{X3}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{,}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Create transpose (3 X n)}
         \PY{n}{X\PYZus{}fullT} \PY{o}{=} \PY{n}{X\PYZus{}full}\PY{o}{.}\PY{n}{transpose}\PY{p}{(}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Calculate X\PYZca{}T X}
         \PY{n}{XTX} \PY{o}{=} \PY{n}{X\PYZus{}fullT}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{X\PYZus{}full}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} calculate inverse of XTX}
         \PY{n}{XTX\PYZus{}inv} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{inv}\PY{p}{(}\PY{n}{XTX}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Calculate theta}
         \PY{n}{theta} \PY{o}{=} \PY{n}{XTX\PYZus{}inv}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{X\PYZus{}fullT}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{Y\PYZus{}3}\PY{p}{)}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Y\PYZus{}3 = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{ * X + }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{ * X\PYZca{}2 + }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{ * X\PYZca{}3}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{theta}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PY{n}{theta}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{n}{theta}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Y\_3 = 0.7593137639575493 * X + -0.01661989155701321 * X\^{}2 + 9.044569969888496e-05 * X\^{}3

    \end{Verbatim}

    We can add a constant to the above by create an extra predictor full of
ones

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}26}]:} \PY{c+c1}{\PYZsh{} Calculate x\PYZca{}2}
         \PY{n}{X2} \PY{o}{=} \PY{n}{X}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}
         
         \PY{c+c1}{\PYZsh{} Calculate X\PYZca{}3}
         \PY{n}{X3} \PY{o}{=} \PY{n}{X}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{3}
         
         \PY{c+c1}{\PYZsh{} Combine into a single array (n X 4)}
         \PY{n}{X\PYZus{}full} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{concatenate}\PY{p}{(}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{ones}\PY{p}{(}\PY{p}{(}\PY{n}{Y\PYZus{}3}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{,}\PY{n}{X}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}\PY{n}{X2}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}\PY{n}{X3}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{,}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Create transpose (4 X n)}
         \PY{n}{X\PYZus{}fullT} \PY{o}{=} \PY{n}{X\PYZus{}full}\PY{o}{.}\PY{n}{transpose}\PY{p}{(}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Calculate X\PYZca{}T X (4 X 4)}
         \PY{n}{XTX} \PY{o}{=} \PY{n}{X\PYZus{}fullT}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{X\PYZus{}full}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} calculate inverse of XTX (4 X 4)}
         \PY{n}{XTX\PYZus{}inv} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{inv}\PY{p}{(}\PY{n}{XTX}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Calculate theta}
         \PY{n}{theta} \PY{o}{=} \PY{n}{XTX\PYZus{}inv}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{X\PYZus{}fullT}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{Y\PYZus{}3}\PY{p}{)}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Y\PYZus{}3 = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{ + }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{ * X + }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{ * X\PYZca{}2 + }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{ * X\PYZca{}3}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{theta}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PY{n}{theta}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{n}{theta}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,}\PY{n}{theta}\PY{p}{[}\PY{l+m+mi}{3}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Y\_3 = 3.664431201640241 + 0.48709842203752807 * X + -0.011179330358443984 * X\^{}2 + 5.867605764941358e-05 * X\^{}3

    \end{Verbatim}

    We plot the original data along with this solution to the parameters to
see how well it fits the data.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}27}]:} \PY{c+c1}{\PYZsh{} 1000 linearly spaced numbers}
         \PY{n}{x3} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{99}\PY{p}{,}\PY{l+m+mi}{1000}\PY{p}{)} 
         \PY{n}{x3\PYZus{}2} \PY{o}{=} \PY{n}{x3}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}
         \PY{n}{x3\PYZus{}3} \PY{o}{=} \PY{n}{x3}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{3}
         
         \PY{c+c1}{\PYZsh{} Predict the response for those numbers}
         \PY{n}{y3} \PY{o}{=} \PY{n}{theta}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{+} \PY{n}{theta}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]} \PY{o}{*} \PY{n}{x3} \PY{o}{+} \PY{n}{theta}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]} \PY{o}{*} \PY{n}{x3\PYZus{}2} \PY{o}{+} \PY{n}{theta}\PY{p}{[}\PY{l+m+mi}{3}\PY{p}{]} \PY{o}{*} \PY{n}{x3\PYZus{}3}
         
         \PY{c+c1}{\PYZsh{} Plot both the data and the fit}
         \PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,}\PY{l+m+mi}{8}\PY{p}{)}\PY{p}{)}
         \PY{n}{axes} \PY{o}{=} \PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}axes}\PY{p}{(}\PY{p}{[}\PY{l+m+mf}{0.1}\PY{p}{,}\PY{l+m+mf}{0.1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
         \PY{n}{axes}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{X}\PY{p}{,}\PY{n}{Y\PYZus{}3}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{axes}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x3}\PY{p}{,}\PY{n}{y3}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Set the labels and title}
         \PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{x}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{f\PYZus{}3(x)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{A plot of the data and the new regression fit for f\PYZus{}3}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_52_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{r2-statistic}{%
\paragraph{\texorpdfstring{\(R^2\)-Statistic}{R\^{}2-Statistic}}\label{r2-statistic}}

Even though a plot of the residuals above does not show a clear
divergence from a normal distribution, it is clear from the
predicted-observed plot that this is not a good model and does not fit
the data in a satisfactory manner. We therefore need additional tools in
order to asses the level of fit.

A metric we can use in order to assess the goodness of the fit is the
\emph{R-Squared} (\(R^2\)) statistic. The \(R^2\) statistic measures the
percentage of variability of the response variable that is explained by
the explanatory variable. This is mathematically expressed as:

\[R^2 = \frac{TSS-RSS}{TSS}\]

where \(TSS = \sum_{i=1}^n(y_i - \bar{y})^2\) is the \emph{total sum of
squares} and \(RSS = \sum_{i=1}^n (y_i - \hat{y}_i)^2\) is the
\emph{residual sum of squares}.

Note: Another way to assess the lack of fit is through the
\emph{Residual Squared Error} \(RSE=\sqrt{ \frac{ RSS }{ n-2 } }\).

\(R^2\), as the form above suggests, is the proportion of variance that
is explained. For a simple linear regression with 1 parameter (see
Appendix A4):

\[R^2 = Cor(X,Y)^2 = \left( \frac{ \sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y}) }{ \sqrt{ \sum_{i=1}^n (x_i - \bar{x})^2 \sum_{i=1}^n (y_i - \bar{y})^2 } } \right) ^2\]

However, for multiple linear regression this does not hold. It is not
clear how to adapt the Correlation in order to explain the fit of a
multiple regression model. \(R^2\) however, is a clearly defined metric
which is easily extended to multiple regression.

Below, we calculate this metric for \(f_3\)

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}28}]:} \PY{c+c1}{\PYZsh{} TSS}
         \PY{n}{TSS\PYZus{}3} \PY{o}{=} \PY{l+m+mi}{0}
         
         \PY{c+c1}{\PYZsh{} RSS}
         \PY{n}{RSS\PYZus{}3} \PY{o}{=} \PY{l+m+mi}{0}
         
         \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{)}\PY{p}{:}
             \PY{n}{TSS\PYZus{}3} \PY{o}{+}\PY{o}{=} \PY{p}{(}\PY{n}{Y\PYZus{}3}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{n}{y\PYZus{}bar3}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}
             \PY{n}{RSS\PYZus{}3} \PY{o}{+}\PY{o}{=} \PY{p}{(}\PY{n}{Y\PYZus{}3}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{n}{y3\PYZus{}fitted}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}
         
         \PY{c+c1}{\PYZsh{} R\PYZca{}2 for f\PYZus{}3}
         \PY{n}{R\PYZus{}sq\PYZus{}3} \PY{o}{=} \PY{p}{(}\PY{n}{TSS\PYZus{}3} \PY{o}{\PYZhy{}} \PY{n}{RSS\PYZus{}3}\PY{p}{)}\PY{o}{/}\PY{n}{TSS\PYZus{}3}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{R\PYZca{}2 = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{R\PYZus{}sq\PYZus{}3}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
R\^{}2 = 0.5940625125965683

    \end{Verbatim}

    This means that roughly 59\% of the variability in \(Y_3\) is explained
by \(X\). Let's calculate the \(R^2\) statistic for all the models
above. To do this, we create a function that accepts observed and fitted
values and returns the TSS and RSS of the fit

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}29}]:} \PY{k}{def} \PY{n+nf}{TSS\PYZus{}RSS}\PY{p}{(}\PY{n}{y\PYZus{}observed}\PY{p}{,}\PY{n}{y\PYZus{}fitted}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}
         \PY{l+s+sd}{    A function that calculates the TSS and RSS of a fit given observed }
         \PY{l+s+sd}{        and fitted values}
         \PY{l+s+sd}{    y\PYZus{}observed := Observed data as a list}
         \PY{l+s+sd}{    y\PYZus{}fitted := Fitted data as a list}
         \PY{l+s+sd}{    output := A (TSS,RSS) tuple of floats}
         \PY{l+s+sd}{    \PYZsq{}\PYZsq{}\PYZsq{}}
             
             \PY{c+c1}{\PYZsh{} TSS}
             \PY{n}{TSS} \PY{o}{=} \PY{l+m+mi}{0}
             
             \PY{c+c1}{\PYZsh{} RSS}
             \PY{n}{RSS} \PY{o}{=} \PY{l+m+mi}{0}
             
             \PY{c+c1}{\PYZsh{} Get the mean of the observed values}
             \PY{n}{y\PYZus{}bar} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{y\PYZus{}observed}\PY{p}{)}
         
             \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{y\PYZus{}observed}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                 \PY{n}{TSS} \PY{o}{+}\PY{o}{=} \PY{p}{(}\PY{n}{y\PYZus{}observed}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{n}{y\PYZus{}bar}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}
                 \PY{n}{RSS} \PY{o}{+}\PY{o}{=} \PY{p}{(}\PY{n}{y\PYZus{}observed}\PY{p}{[}\PY{n}{i}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{n}{y\PYZus{}fitted}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}
                 
             \PY{k}{return} \PY{n}{TSS}\PY{p}{,}\PY{n}{RSS}
\end{Verbatim}


    Then we apply this function to the three fitted models

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}30}]:} \PY{c+c1}{\PYZsh{} Calculate the TSS and RSS for the fitted regression line to f\PYZus{}1}
         \PY{n}{TSS\PYZus{}1}\PY{p}{,} \PY{n}{RSS\PYZus{}1} \PY{o}{=} \PY{n}{TSS\PYZus{}RSS}\PY{p}{(}\PY{n}{Y\PYZus{}1}\PY{p}{,}\PY{n}{y1\PYZus{}fitted}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Calculate the R\PYZca{}2 for the fit to f\PYZus{}1}
         \PY{n}{R\PYZus{}sq\PYZus{}1} \PY{o}{=} \PY{p}{(}\PY{n}{TSS\PYZus{}1} \PY{o}{\PYZhy{}} \PY{n}{RSS\PYZus{}1}\PY{p}{)}\PY{o}{/}\PY{n}{TSS\PYZus{}1}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model for Y\PYZus{}1: Explanatory variable X for Y\PYZus{}1 \PYZhy{} R\PYZca{}2 = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PYZbs{}
               \PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{R\PYZus{}sq\PYZus{}1}\PY{p}{)}\PY{p}{)}
         
         
         \PY{c+c1}{\PYZsh{} Calculate the TSS and RSS for the fitted regression line to f\PYZus{}2}
         \PY{n}{TSS\PYZus{}2}\PY{p}{,}\PY{n}{RSS\PYZus{}2} \PY{o}{=} \PY{n}{TSS\PYZus{}RSS}\PY{p}{(}\PY{n}{Y\PYZus{}2}\PY{p}{,}\PY{n}{y2\PYZus{}fitted}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Calculate the R\PYZca{}2 for the fit to f\PYZus{}2}
         \PY{n}{R\PYZus{}sq\PYZus{}2} \PY{o}{=} \PY{p}{(}\PY{n}{TSS\PYZus{}2} \PY{o}{\PYZhy{}} \PY{n}{RSS\PYZus{}2}\PY{p}{)}\PY{o}{/}\PY{n}{TSS\PYZus{}2}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model for Y\PYZus{}2: Explanatory variable X for Y\PYZus{}2 \PYZhy{} R\PYZca{}2 = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PYZbs{}
               \PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{R\PYZus{}sq\PYZus{}2}\PY{p}{)}\PY{p}{)}
         
         
         \PY{c+c1}{\PYZsh{} Calculate the TSS and RSS for the new fitted regression line to f\PYZus{}2}
         \PY{n}{TSS\PYZus{}22}\PY{p}{,}\PY{n}{RSS\PYZus{}22} \PY{o}{=} \PY{n}{TSS\PYZus{}RSS}\PY{p}{(}\PY{n}{Y\PYZus{}2}\PY{p}{,}\PY{n}{y22\PYZus{}fitted}\PY{p}{)}
            
         \PY{c+c1}{\PYZsh{} Calculate the R\PYZca{}2 for the new fit to f\PYZus{}2}
         \PY{n}{R\PYZus{}sq\PYZus{}22} \PY{o}{=} \PY{p}{(}\PY{n}{TSS\PYZus{}22} \PY{o}{\PYZhy{}} \PY{n}{RSS\PYZus{}22}\PY{p}{)}\PY{o}{/}\PY{n}{TSS\PYZus{}22}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model for Y\PYZus{}2: Explanatory variable X\PYZca{}2 for Y\PYZus{}2 \PYZhy{} R\PYZca{}2 = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PYZbs{}
               \PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{R\PYZus{}sq\PYZus{}22}\PY{p}{)}\PY{p}{)}
         
         
         \PY{c+c1}{\PYZsh{} Calculate the TSS and RSS for the fitted regression line to f\PYZus{}3}
         \PY{n}{TSS\PYZus{}3}\PY{p}{,}\PY{n}{RSS\PYZus{}3} \PY{o}{=} \PY{n}{TSS\PYZus{}RSS}\PY{p}{(}\PY{n}{Y\PYZus{}3}\PY{p}{,}\PY{n}{y3\PYZus{}fitted}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Calculate the R\PYZca{}2 for the fit to f\PYZus{}3}
         \PY{n}{R\PYZus{}sq\PYZus{}3} \PY{o}{=} \PY{p}{(}\PY{n}{TSS\PYZus{}3} \PY{o}{\PYZhy{}} \PY{n}{RSS\PYZus{}3}\PY{p}{)}\PY{o}{/}\PY{n}{TSS\PYZus{}3}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model for Y\PYZus{}3: Explanatory variable X for Y\PYZus{}3 \PYZhy{} R\PYZca{}2 = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PYZbs{}
               \PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{R\PYZus{}sq\PYZus{}3}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Model for Y\_1: Explanatory variable X for Y\_1 - R\^{}2 = 0.9951845734408926
Model for Y\_2: Explanatory variable X for Y\_2 - R\^{}2 = 0.9336613222418227
Model for Y\_2: Explanatory variable X\^{}2 for Y\_2 - R\^{}2 = 0.99880452106502
Model for Y\_3: Explanatory variable X for Y\_3 - R\^{}2 = 0.5940625125965683

    \end{Verbatim}

    From the above we can see that the model for \(Y_1\) that is linear in
\(X\) is satisfactory; The model for \(Y_2\) that is non-linear explains
more variability of the response variable than the linear model (note
that in this case, the \(R^2\) metric alone wouldn't tell us whether the
fit linear in \(X\) was terrible. But along with the residual plot we
would arrive at the correct conclusion); The model for \(Y_3\) shows
that we are probably not fitting the correct form of the function,
i.e.~we have introduced bias in that the real function is not of the
form \(a+bX\) for constants \(a\) and \(b\) and that applying a model
non-linear in \(X\) may provide a boost to the explained variance. We
can try combinations of \(X\), \(X^2\), \(X^3\) as well. We do this
after we have introduced a much simpler way of obtaining the above fits
using Scikit-Learn packages.

Below, we use \emph{sklearn.linear\_model.LinearRegression()} in order
to fit and \emph{sklearn.metrics.r2\_score()} in order to calculate the
\(R^2\) statistic. We will see that the results match the manual results
above

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}31}]:} \PY{c+c1}{\PYZsh{} Create the model object}
         \PY{n}{lm1} \PY{o}{=} \PY{n}{LinearRegression}\PY{p}{(}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Fit this model to the data for f\PYZus{}1}
         \PY{n}{lm1}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}\PY{n}{Y\PYZus{}1}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model for Y\PYZus{}1: Explanatory variable X for Y\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{beta\PYZus{}0 = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{lm1}\PY{o}{.}\PY{n}{intercept\PYZus{}}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{beta\PYZus{}1 = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{lm1}\PY{o}{.}\PY{n}{coef\PYZus{}}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Get the fitted values and print it}
         \PY{n}{y1\PYZus{}fitted\PYZus{}sklearn} \PY{o}{=} \PY{n}{lm1}\PY{o}{.}\PY{n}{intercept\PYZus{}}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{+} \PY{n}{lm1}\PY{o}{.}\PY{n}{coef\PYZus{}}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{*}\PY{n}{X}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{R\PYZca{}2 = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{r2\PYZus{}score}\PY{p}{(}\PY{n}{Y\PYZus{}1}\PY{p}{,}\PY{n}{y1\PYZus{}fitted\PYZus{}sklearn}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
         
         \PY{n}{lm2} \PY{o}{=} \PY{n}{LinearRegression}\PY{p}{(}\PY{p}{)}
         \PY{n}{lm2}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}\PY{n}{Y\PYZus{}2}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model for Y\PYZus{}2: Explanatory variable X for Y\PYZus{}2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{beta\PYZus{}0 = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{lm2}\PY{o}{.}\PY{n}{intercept\PYZus{}}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{beta\PYZus{}1 = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{lm2}\PY{o}{.}\PY{n}{coef\PYZus{}}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}
         \PY{n}{y2\PYZus{}fitted\PYZus{}sklearn} \PY{o}{=} \PY{n}{lm2}\PY{o}{.}\PY{n}{intercept\PYZus{}}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{+} \PY{n}{lm2}\PY{o}{.}\PY{n}{coef\PYZus{}}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{*}\PY{n}{X}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{R\PYZca{}2 = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{r2\PYZus{}score}\PY{p}{(}\PY{n}{Y\PYZus{}2}\PY{p}{,}\PY{n}{y2\PYZus{}fitted\PYZus{}sklearn}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
         
         \PY{n}{lm22} \PY{o}{=} \PY{n}{LinearRegression}\PY{p}{(}\PY{p}{)}
         \PY{n}{lm22}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{p}{(}\PY{n}{X}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}\PY{n}{Y\PYZus{}2}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model for Y\PYZus{}2: Explanatory variable X\PYZca{}2 for Y\PYZus{}2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{beta\PYZus{}0 = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{lm22}\PY{o}{.}\PY{n}{intercept\PYZus{}}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{beta\PYZus{}1 = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{lm22}\PY{o}{.}\PY{n}{coef\PYZus{}}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}
         \PY{n}{y22\PYZus{}fitted\PYZus{}sklearn} \PY{o}{=} \PY{n}{lm22}\PY{o}{.}\PY{n}{intercept\PYZus{}}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{+} \PY{n}{lm22}\PY{o}{.}\PY{n}{coef\PYZus{}}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{*}\PY{n}{X}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{R\PYZca{}2 = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{r2\PYZus{}score}\PY{p}{(}\PY{n}{Y\PYZus{}2}\PY{p}{,}\PY{n}{y22\PYZus{}fitted\PYZus{}sklearn}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
         
         \PY{n}{lm3} \PY{o}{=} \PY{n}{LinearRegression}\PY{p}{(}\PY{p}{)}
         \PY{n}{lm3}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}\PY{n}{Y\PYZus{}3}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model for Y\PYZus{}3: Explanatory variable X for Y\PYZus{}3}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{beta\PYZus{}0 = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{lm3}\PY{o}{.}\PY{n}{intercept\PYZus{}}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{beta\PYZus{}1 = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{lm3}\PY{o}{.}\PY{n}{coef\PYZus{}}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}
         \PY{n}{y3\PYZus{}fitted\PYZus{}sklearn} \PY{o}{=} \PY{n}{lm3}\PY{o}{.}\PY{n}{intercept\PYZus{}}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{+} \PY{n}{lm3}\PY{o}{.}\PY{n}{coef\PYZus{}}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{*}\PY{n}{X}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{R\PYZca{}2 = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{r2\PYZus{}score}\PY{p}{(}\PY{n}{Y\PYZus{}3}\PY{p}{,}\PY{n}{y3\PYZus{}fitted\PYZus{}sklearn}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Now we try adding the variables X,X\PYZca{}2 and X\PYZca{}3}
         
         \PY{c+c1}{\PYZsh{}Create transformed variables}
         \PY{n}{X2} \PY{o}{=} \PY{n}{X}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}
         \PY{n}{X3} \PY{o}{=} \PY{n}{X}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{3}
         
         \PY{n}{lm32} \PY{o}{=} \PY{n}{LinearRegression}\PY{p}{(}\PY{p}{)}
         \PY{n}{X3\PYZus{}collection} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{concat}\PY{p}{(}\PY{p}{[}\PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{X}\PY{p}{,}\PY{n}{columns}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{p}{,}\PYZbs{}
                         \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{X}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{,}\PY{n}{columns}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{p}{,}\PYZbs{}
                         \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{X}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{3}\PY{p}{,}\PY{n}{columns}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X3}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{p}{]}\PY{p}{,}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
         \PY{n}{lm32}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X3\PYZus{}collection}\PY{p}{,}\PY{n}{Y\PYZus{}3}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model for Y\PYZus{}3: Explanatory variables X,X\PYZca{}2,X\PYZca{}3 for Y\PYZus{}3}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{beta\PYZus{}0 = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{lm32}\PY{o}{.}\PY{n}{intercept\PYZus{}}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{beta\PYZus{}1 = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{lm32}\PY{o}{.}\PY{n}{coef\PYZus{}}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{beta\PYZus{}2 = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{lm32}\PY{o}{.}\PY{n}{coef\PYZus{}}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{beta\PYZus{}3 = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{lm32}\PY{o}{.}\PY{n}{coef\PYZus{}}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}\PY{p}{)}
         \PY{n}{y32\PYZus{}fitted\PYZus{}sklearn} \PY{o}{=} \PY{n}{lm32}\PY{o}{.}\PY{n}{intercept\PYZus{}}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{+} \PY{n}{lm32}\PY{o}{.}\PY{n}{coef\PYZus{}}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{*}\PY{n}{X} \PY{o}{+} \PYZbs{}
                             \PY{n}{lm32}\PY{o}{.}\PY{n}{coef\PYZus{}}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{*}\PY{n}{X}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2} \PY{o}{+} \PY{n}{lm32}\PY{o}{.}\PY{n}{coef\PYZus{}}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{o}{*}\PY{n}{X}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{3}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{R\PYZca{}2 = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{r2\PYZus{}score}\PY{p}{(}\PY{n}{Y\PYZus{}3}\PY{p}{,}\PY{n}{y32\PYZus{}fitted\PYZus{}sklearn}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Model for Y\_1: Explanatory variable X for Y\_1
beta\_0 = 5.501243124853005
beta\_1 = 5.064254524922959
R\^{}2 = 0.9951845734408926


Model for Y\_2: Explanatory variable X for Y\_2
beta\_0 = -8445.980306821988
beta\_1 = 506.16066894401666
R\^{}2 = 0.9336613222418227


Model for Y\_2: Explanatory variable X\^{}2 for Y\_2
beta\_0 = 14.470063153308729
beta\_1 = 5.075020979320469
R\^{}2 = 0.99880452106502


Model for Y\_3: Explanatory variable X for Y\_3
beta\_0 = 10.511143457700808
beta\_1 = -0.10119878181001966
R\^{}2 = 0.5940625125965684


Model for Y\_3: Explanatory variables X,X\^{}2,X\^{}3 for Y\_3
beta\_0 = 3.6644312016355887
beta\_1 = 0.48709842203796766
beta\_2 = -0.011179330358454217
beta\_3 = 5.8676057649481236e-05
R\^{}2 = 0.9229011520420615

    \end{Verbatim}

    In the above, we fit a model using 3 explanatory variables, namely
\(X\), \(X^2\), \(X^3\) with coefficients \(\beta_1\), \(\beta_2\),
\(\beta_3\) respectively. We can see that we have a much improved
\(R^2\) statistic for the fitted model to \(f_3\) meaning we have
managed to explain much more of the data using the transformed variables
we have created. We can plot the model to see how well it follows the
response variable.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}32}]:} \PY{c+c1}{\PYZsh{} 1000 linearly spaced numbers}
         \PY{n}{x32} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{99}\PY{p}{,}\PY{l+m+mi}{1000}\PY{p}{)} 
         \PY{n}{y32} \PY{o}{=} \PY{n}{lm32}\PY{o}{.}\PY{n}{intercept\PYZus{}}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{+} \PY{n}{lm32}\PY{o}{.}\PY{n}{coef\PYZus{}}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{*}\PY{n}{x32} \PY{o}{+} \PY{n}{lm32}\PY{o}{.}\PY{n}{coef\PYZus{}}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{*}\PY{n}{x32}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PYZbs{}
             \PY{o}{+} \PY{n}{lm32}\PY{o}{.}\PY{n}{coef\PYZus{}}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{o}{*}\PY{n}{x32}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{3}
         
         \PY{c+c1}{\PYZsh{} Plot the data and the fit}
         \PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,}\PY{l+m+mi}{8}\PY{p}{)}\PY{p}{)}
         \PY{n}{axes} \PY{o}{=} \PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}axes}\PY{p}{(}\PY{p}{[}\PY{l+m+mf}{0.1}\PY{p}{,}\PY{l+m+mf}{0.1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
         \PY{n}{axes}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{X}\PY{p}{,}\PY{n}{Y\PYZus{}3}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{axes}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x32}\PY{p}{,}\PY{n}{y32}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Set the lables and title}
         \PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{x}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{f\PYZus{}3(x)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{A plot of the data and the model using X,X\PYZca{}2 and X\PYZca{}3 as }\PY{l+s+se}{\PYZbs{}}
         \PY{l+s+s1}{predictors}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_62_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    We can also check the residuals plot

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}33}]:} \PY{c+c1}{\PYZsh{} Calculate the fitted values using the observed values}
         \PY{n}{y32\PYZus{}fitted\PYZus{}sklearn} \PY{o}{=} \PY{n}{lm32}\PY{o}{.}\PY{n}{intercept\PYZus{}}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{+} \PY{n}{lm32}\PY{o}{.}\PY{n}{coef\PYZus{}}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{*}\PY{n}{X} \PY{o}{+} \PYZbs{}
                             \PY{n}{lm32}\PY{o}{.}\PY{n}{coef\PYZus{}}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{*}\PY{n}{X}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2} \PY{o}{+} \PY{n}{lm32}\PY{o}{.}\PY{n}{coef\PYZus{}}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{o}{*}\PY{n}{X}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{3}
         
         \PY{c+c1}{\PYZsh{} Calculate the residuals}
         \PY{n}{Res\PYZus{}32} \PY{o}{=} \PY{n}{y32\PYZus{}fitted\PYZus{}sklearn} \PY{o}{\PYZhy{}} \PY{n}{Y\PYZus{}3}
         
         \PY{c+c1}{\PYZsh{} Plot the residuals}
         \PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,}\PY{l+m+mi}{8}\PY{p}{)}\PY{p}{)}
         \PY{n}{axes} \PY{o}{=} \PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}axes}\PY{p}{(}\PY{p}{[}\PY{l+m+mf}{0.1}\PY{p}{,}\PY{l+m+mf}{0.1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
         \PY{n}{axes}\PY{o}{.}\PY{n}{hist}\PY{p}{(}\PY{n}{Res\PYZus{}32}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Set the lables and title}
         \PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Residuals}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Count}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{A plot of the residuals of the model using X,X\PYZca{}2 and }\PY{l+s+se}{\PYZbs{}}
         \PY{l+s+s1}{X\PYZca{}3 as predictors}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{This is roughly a normal distribution with mean }\PY{l+s+si}{\PYZob{}mean\PYZcb{}}\PY{l+s+s1}{ and }\PY{l+s+se}{\PYZbs{}}
         \PY{l+s+s1}{standard deviation }\PY{l+s+si}{\PYZob{}std\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{mean}\PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{Res\PYZus{}32}\PY{p}{)}\PY{p}{,}\PY{n}{std}\PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{std}\PY{p}{(}\PY{n}{Res\PYZus{}32}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_64_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
This is roughly a normal distribution with mean -4.387601393318619e-15 and standard deviation 1.0437970768534388

    \end{Verbatim}

    It is not a surprise that we were able to fit a function of the form
\(f(x) = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3\). Using taylor
expansion, \(f(x) = sin(x)\) estimated around the point \(x=0\) is

\[f(x=0) = f(0) + f^{(1)}(0)x + f^{(2)}(0)x^2/(2!) + f^{(3)}(0)x^3/(3!) + O(x^4)\]
\[= \sin(0) + \cos(0)x - \sin(0)x^2/(2!) -\cos(0)x^3/(3!)\]
\[= x - x^3/(6)\]

If we apply Taylor series expansion to \(f(x) = 4.67 + 5.07 sin(x/20)\)
instead:

\[f(x=0) = 4.67 + \frac{5.07}{20}\cos(0)x-\frac{5.07}{20^3}\cos(0)x^3/(3!)=4.67 + 0.25x - 1 \times 10^{-4} x^3\]

Let's plot this along with the above for smaller values of X for which
this approximation of sin(x) is acceptable.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}34}]:} \PY{c+c1}{\PYZsh{} 1000 linearly spaced numbers}
         \PY{n}{x32} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{50}\PY{p}{,}\PY{l+m+mi}{1000}\PY{p}{)} 
         
         \PY{c+c1}{\PYZsh{} Predictions}
         \PY{n}{y32} \PY{o}{=} \PY{n}{lm32}\PY{o}{.}\PY{n}{intercept\PYZus{}}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{+} \PY{n}{lm32}\PY{o}{.}\PY{n}{coef\PYZus{}}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{*}\PY{n}{x32} \PY{o}{+} \PY{n}{lm32}\PY{o}{.}\PY{n}{coef\PYZus{}}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{*}\PY{n}{x32}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PYZbs{}
             \PY{o}{+} \PY{n}{lm32}\PY{o}{.}\PY{n}{coef\PYZus{}}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{o}{*}\PY{n}{x32}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{3}
         
         \PY{c+c1}{\PYZsh{} Prediction using Taylor expansion}
         \PY{n}{y\PYZus{}taylor\PYZus{}32} \PY{o}{=} \PY{l+m+mf}{4.67} \PY{o}{+} \PY{p}{(}\PY{l+m+mf}{5.07}\PY{o}{/}\PY{l+m+mi}{20}\PY{p}{)}\PY{o}{*}\PY{n}{x32} \PY{o}{+} \PY{l+m+mi}{0}\PY{o}{*}\PY{n}{x32}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2} \PY{o}{\PYZhy{}} \PY{p}{(}\PY{l+m+mf}{5.07}\PY{o}{/}\PY{p}{(}\PY{l+m+mi}{20}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{3} \PY{o}{*} \PY{l+m+mi}{6}\PY{p}{)}\PY{p}{)}\PY{o}{*}\PY{n}{x32}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{3}
         
         \PY{c+c1}{\PYZsh{} Only get the observed predictors and response where the predictors are less }
         \PY{c+c1}{\PYZsh{} than 50}
         \PY{n}{X\PYZus{}small} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n+nb}{filter}\PY{p}{(}\PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n}{x} \PY{o}{\PYZlt{}} \PY{l+m+mi}{50}\PY{p}{,}\PY{n}{X}\PY{p}{)}\PY{p}{)}
         \PY{n}{Y\PYZus{}small} \PY{o}{=} \PY{n}{Y\PYZus{}3}\PY{p}{[}\PY{n+nb}{list}\PY{p}{(}\PY{n+nb}{map}\PY{p}{(}\PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{n}{x} \PY{o}{\PYZlt{}} \PY{l+m+mi}{50}\PY{p}{,}\PY{n}{X}\PY{p}{)}\PY{p}{)}\PY{p}{]}
         
         \PY{c+c1}{\PYZsh{} Plot the data, the fitted model and the taylor expansion}
         \PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,}\PY{l+m+mi}{8}\PY{p}{)}\PY{p}{)}
         \PY{n}{axes} \PY{o}{=} \PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}axes}\PY{p}{(}\PY{p}{[}\PY{l+m+mf}{0.1}\PY{p}{,}\PY{l+m+mf}{0.1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
         \PY{n}{axes}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{X\PYZus{}small}\PY{p}{,}\PY{n}{Y\PYZus{}small}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Data}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{axes}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x32}\PY{p}{,}\PY{n}{y32}\PY{p}{,}\PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Linear Model}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{axes}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x32}\PY{p}{,}\PY{n}{y\PYZus{}taylor\PYZus{}32}\PY{p}{,}\PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Taylor Expansion}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Set the labels and title}
         \PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{x}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{f\PYZus{}3(x)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{A comparison of the fits from the regression model and the }\PY{l+s+se}{\PYZbs{}}
         \PY{l+s+s1}{Taylor exansion of f\PYZus{}3}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Add the legend}
         \PY{n}{axes}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
         
         \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_66_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{statistical-significance-of-regression-coefficients}{%
\paragraph{Statistical significance of regression
coefficients}\label{statistical-significance-of-regression-coefficients}}

In addition to the \(R^2\) statistic, it is useful to assess whether a
variable is statistically significant. To do this for a variable \(X\)
with coefficient \(\beta_1\), we test the null hypothesis

\[H_O: \beta_1 = 0\]

against

\[H_A: \beta_1 \neq 0\]

For the first model we have the fitted model

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}35}]:} \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{f(x) = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{ + }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{ X}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{lm1}\PY{o}{.}\PY{n}{intercept\PYZus{}}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PY{n}{lm1}\PY{o}{.}\PY{n}{coef\PYZus{}}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
f(x) = 5.501243124853005 + 5.064254524922959 X

    \end{Verbatim}

    The standard errors of the estimators \(\hat{\beta}_0\) and
\(\hat{\beta}_1\) for the coefficients have the form (See Appendix A5):

\[SE(\beta_0) = \sqrt{\sigma^2 \left[\frac{1}{n} + \frac{\bar{x}^2}{\sum_{i=1}^n (x_i - \bar{x})^2}\right]} \approx  RSE\sqrt{ \left[\frac{1}{n} + \frac{\bar{x}^2}{\sum_{i=1}^n (x_i - \bar{x})^2}\right]}\]

where RSE is the \emph{residual standard error} estimating the
population \(\sigma =\sqrt{Var(\epsilon)}\) and has the form
\(RSE = \sqrt{\frac{\sum_{i=1}^n \epsilon_i^2}{n-2}} = \sqrt{\frac{RSS}{n-2}}\).

In addition we can show that:

\[SE(\beta_1) = \sqrt{   \frac{ \sigma^2 }{ \sum_{i=1}^n (x_i - \bar{x})^2 }   } \approx RSE\sqrt{   \frac{ 1 }{ \sum_{i=1}^n (x_i - \bar{x})^2 }   }\]

Using the standard errors, we can then conduct the hypothesis test above
as a t-test. We have that

\[\frac{ \hat{\beta_0} - \beta_0^{(0)} }{ SE(\beta_0) } \sim t_{n-2}\]

\[\frac{ \hat{\beta_1} - \beta_1^{(0)} }{ SE(\beta_1) } \sim t_{n-2}\]

where \(^{(0)}\) denotes the null value (the null hypothesis above sets
both \(\beta_0^{(0)}=0\) and \(\beta_1^{(0)}=0\)).

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}36}]:} \PY{c+c1}{\PYZsh{} number of observations n}
         \PY{n}{n} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} residual standard error}
         \PY{n}{RSE\PYZus{}1} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{RSS\PYZus{}1}\PY{o}{/}\PY{p}{(}\PY{n}{n}\PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} variance of x = sum (x\PYZus{}i \PYZhy{} x\PYZus{}bar)\PYZca{}2. Note that this is the }
         \PY{c+c1}{\PYZsh{} population variance calculation}
         \PY{c+c1}{\PYZsh{} so we would need to multiply by n}
         \PY{n}{varx\PYZus{}1} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{var}\PY{p}{(}\PY{n}{X}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} mean of x}
         \PY{n}{meanx\PYZus{}1} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{X}\PY{p}{)}
         
         \PY{n}{SE\PYZus{}beta\PYZus{}0} \PY{o}{=} \PY{n}{RSE\PYZus{}1} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{l+m+mf}{1.0}\PY{o}{/}\PY{n}{n} \PY{o}{+} \PY{n}{meanx\PYZus{}1}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{o}{/}\PY{p}{(}\PY{n}{n}\PY{o}{*}\PY{n}{varx\PYZus{}1}\PY{p}{)}\PY{p}{)}
         \PY{n}{SE\PYZus{}beta\PYZus{}1} \PY{o}{=} \PY{n}{RSE\PYZus{}1} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{l+m+mf}{1.0}\PY{o}{/}\PY{p}{(}\PY{n}{n}\PY{o}{*}\PY{n}{varx\PYZus{}1}\PY{p}{)}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SE(beta\PYZus{}0) = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{, SE(beta\PYZus{}1) = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{SE\PYZus{}beta\PYZus{}0}\PY{p}{,}\PY{n}{SE\PYZus{}beta\PYZus{}1}\PY{p}{)}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} null hypothesis}
         \PY{n}{betanull\PYZus{}0} \PY{o}{=} \PY{l+m+mi}{0}
         \PY{n}{betanull\PYZus{}1} \PY{o}{=} \PY{l+m+mi}{0}
         
         \PY{n}{tstatistic1\PYZus{}0} \PY{o}{=} \PY{p}{(}\PY{n}{beta1\PYZus{}0} \PY{o}{\PYZhy{}} \PY{n}{betanull\PYZus{}0}\PY{p}{)}\PY{o}{/}\PY{n}{SE\PYZus{}beta\PYZus{}0}
         \PY{n}{tstatistic1\PYZus{}1} \PY{o}{=} \PY{p}{(}\PY{n}{beta1\PYZus{}1} \PY{o}{\PYZhy{}} \PY{n}{betanull\PYZus{}1}\PY{p}{)}\PY{o}{/}\PY{n}{SE\PYZus{}beta\PYZus{}1}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{beta\PYZus{}0 t\PYZhy{}statistic = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{tstatistic1\PYZus{}0}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{beta\PYZus{}1 t\PYZhy{}statistic = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{tstatistic1\PYZus{}1}\PY{p}{)}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} p\PYZhy{}value}
         \PY{c+c1}{\PYZsh{} the following function calculates the area under the student t pdf with }
         \PY{c+c1}{\PYZsh{} 2 degrees of freedom that is less than \PYZhy{}4.303}
         \PY{n}{stats}\PY{o}{.}\PY{n}{t}\PY{o}{.}\PY{n}{cdf}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mf}{4.303}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} calculate the p\PYZhy{}value using the tstatistic and degrees of freedom n\PYZhy{}2}
         \PY{n}{pval1\PYZus{}0} \PY{o}{=} \PY{n}{stats}\PY{o}{.}\PY{n}{t}\PY{o}{.}\PY{n}{cdf}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{n}{tstatistic1\PYZus{}0}\PY{p}{,}\PY{n}{n}\PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{)}
         \PY{n}{pval1\PYZus{}1} \PY{o}{=} \PY{n}{stats}\PY{o}{.}\PY{n}{t}\PY{o}{.}\PY{n}{cdf}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{n}{tstatistic1\PYZus{}1}\PY{p}{,}\PY{n}{n}\PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{p\PYZhy{}value for beta\PYZus{}0 = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{pval1\PYZus{}0}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{p\PYZhy{}value for beta\PYZus{}1 = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{pval1\PYZus{}1}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{These are both statistically significant!}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
SE(beta\_0) = 0.6406034056188337, SE(beta\_1) = 0.011151051418375258
beta\_0 t-statistic = 8.587595814509644
beta\_1 t-statistic = 454.150405635995
p-value for beta\_0 = 1.685985282508196e-17
p-value for beta\_1 = 0.0
These are both statistically significant!

    \end{Verbatim}

    We can put this into a function

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}37}]:} \PY{k}{def} \PY{n+nf}{calcpvalue}\PY{p}{(}\PY{n}{X}\PY{p}{,}\PY{n}{y\PYZus{}observed}\PY{p}{,}\PY{n}{y\PYZus{}fitted}\PY{p}{,}\PY{n}{beta\PYZus{}0}\PY{p}{,}\PY{n}{beta\PYZus{}1}\PY{p}{,}\PY{n}{betanull\PYZus{}0}\PY{p}{,}\PY{n}{betanull\PYZus{}1}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}
         \PY{l+s+sd}{    A function to calculate whether the coefficients in a model with 1 }
         \PY{l+s+sd}{        variable is statistically significant.}
         \PY{l+s+sd}{    X = a list for the data for the variable}
         \PY{l+s+sd}{    y\PYZus{}observed = the observed values for the response variable}
         \PY{l+s+sd}{    y\PYZus{}fitted = the predicted values of the model}
         \PY{l+s+sd}{    beta\PYZus{}0 = the intercept of the model}
         \PY{l+s+sd}{    beta\PYZus{}1 = the coefficient of the explanatory variable in the model}
         \PY{l+s+sd}{    betanull\PYZus{}0 = null hypothesis value for the intercept (usually 0)}
         \PY{l+s+sd}{    betanull\PYZus{}1 = null hypothesis value for the coefficient of the response }
         \PY{l+s+sd}{        variable (usually 0)}
         \PY{l+s+sd}{    \PYZsq{}\PYZsq{}\PYZsq{}}
             \PY{c+c1}{\PYZsh{} number of observations n}
             \PY{n}{n} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}
         
             \PY{c+c1}{\PYZsh{} calculate RSS}
             \PY{n}{temp}\PY{p}{,}\PY{n}{RSS} \PY{o}{=} \PY{n}{TSS\PYZus{}RSS}\PY{p}{(}\PY{n}{y\PYZus{}observed}\PY{p}{,}\PY{n}{y\PYZus{}fitted}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{} residual standard error}
             \PY{n}{RSE} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{RSS}\PY{o}{/}\PY{p}{(}\PY{n}{n}\PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
         
             \PY{c+c1}{\PYZsh{} variance of x = sum (x\PYZus{}i \PYZhy{} x\PYZus{}bar)\PYZca{}2. Note that this is the population }
             \PY{c+c1}{\PYZsh{} variance calculation}
             \PY{c+c1}{\PYZsh{} so we would need to multiply by n}
             \PY{n}{varx} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{var}\PY{p}{(}\PY{n}{X}\PY{p}{)}
         
             \PY{c+c1}{\PYZsh{} mean of x}
             \PY{n}{meanx} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{X}\PY{p}{)}
         
             \PY{n}{SE\PYZus{}beta\PYZus{}0} \PY{o}{=} \PY{n}{RSE} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{l+m+mf}{1.0}\PY{o}{/}\PY{n}{n} \PY{o}{+} \PY{n}{meanx}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{o}{/}\PY{p}{(}\PY{n}{n}\PY{o}{*}\PY{n}{varx}\PY{p}{)}\PY{p}{)}
             \PY{n}{SE\PYZus{}beta\PYZus{}1} \PY{o}{=} \PY{n}{RSE} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{l+m+mf}{1.0}\PY{o}{/}\PY{p}{(}\PY{n}{n}\PY{o}{*}\PY{n}{varx}\PY{p}{)}\PY{p}{)}
         
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SE(beta\PYZus{}0) = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{, SE(beta\PYZus{}1) = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{SE\PYZus{}beta\PYZus{}0}\PY{p}{,}\PY{n}{SE\PYZus{}beta\PYZus{}1}\PY{p}{)}\PY{p}{)}
         
             \PY{c+c1}{\PYZsh{} null hypothesis}
             \PY{n}{betanull\PYZus{}0} \PY{o}{=} \PY{l+m+mi}{0}
             \PY{n}{betanull\PYZus{}1} \PY{o}{=} \PY{l+m+mi}{0}
         
             \PY{n}{tstatistic1\PYZus{}0} \PY{o}{=} \PY{p}{(}\PY{n}{beta\PYZus{}0} \PY{o}{\PYZhy{}} \PY{n}{betanull\PYZus{}0}\PY{p}{)}\PY{o}{/}\PY{n}{SE\PYZus{}beta\PYZus{}0}
             \PY{n}{tstatistic1\PYZus{}1} \PY{o}{=} \PY{p}{(}\PY{n}{beta\PYZus{}1} \PY{o}{\PYZhy{}} \PY{n}{betanull\PYZus{}1}\PY{p}{)}\PY{o}{/}\PY{n}{SE\PYZus{}beta\PYZus{}1}
         
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{beta\PYZus{}0 t\PYZhy{}statistic = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{tstatistic1\PYZus{}0}\PY{p}{)}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{beta\PYZus{}1 t\PYZhy{}statistic = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{tstatistic1\PYZus{}1}\PY{p}{)}\PY{p}{)}
         
             \PY{c+c1}{\PYZsh{} p\PYZhy{}value}
         
             \PY{c+c1}{\PYZsh{} calculate the p\PYZhy{}value using the tstatistic and degrees of freedom n\PYZhy{}2}
             \PY{c+c1}{\PYZsh{} Multiply by 2 since it\PYZsq{}s a 2 tailed test}
             \PY{k}{if}\PY{p}{(}\PY{n}{tstatistic1\PYZus{}0} \PY{o}{\PYZgt{}} \PY{l+m+mi}{0}\PY{p}{)}\PY{p}{:}
                 \PY{n}{pval\PYZus{}0} \PY{o}{=} \PY{n}{stats}\PY{o}{.}\PY{n}{t}\PY{o}{.}\PY{n}{cdf}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{n}{tstatistic1\PYZus{}0}\PY{p}{,}\PY{n}{n}\PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{)}\PY{o}{*}\PY{l+m+mi}{2}
             \PY{k}{else}\PY{p}{:}
                 \PY{n}{pval\PYZus{}0} \PY{o}{=} \PY{n}{stats}\PY{o}{.}\PY{n}{t}\PY{o}{.}\PY{n}{cdf}\PY{p}{(}\PY{n}{tstatistic1\PYZus{}0}\PY{p}{,}\PY{n}{n}\PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{)}\PY{o}{*}\PY{l+m+mi}{2}
                 
             \PY{k}{if}\PY{p}{(}\PY{n}{tstatistic1\PYZus{}1} \PY{o}{\PYZgt{}} \PY{l+m+mi}{0}\PY{p}{)}\PY{p}{:}
                 \PY{n}{pval\PYZus{}1} \PY{o}{=} \PY{n}{stats}\PY{o}{.}\PY{n}{t}\PY{o}{.}\PY{n}{cdf}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{n}{tstatistic1\PYZus{}1}\PY{p}{,}\PY{n}{n}\PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{)}\PY{o}{*}\PY{l+m+mi}{2}
             \PY{k}{else}\PY{p}{:}
                 \PY{n}{pval\PYZus{}1} \PY{o}{=} \PY{n}{stats}\PY{o}{.}\PY{n}{t}\PY{o}{.}\PY{n}{cdf}\PY{p}{(}\PY{n}{tstatistic1\PYZus{}1}\PY{p}{,}\PY{n}{n}\PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{)}\PY{o}{*}\PY{l+m+mi}{2}
         
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{p\PYZhy{}value for beta\PYZus{}0 = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{pval\PYZus{}0}\PY{p}{)}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{p\PYZhy{}value for beta\PYZus{}1 = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{pval\PYZus{}1}\PY{p}{)}\PY{p}{)}
             \PY{k}{if}\PY{p}{(}\PY{p}{(}\PY{n}{pval\PYZus{}0} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{l+m+mf}{0.05}\PY{p}{)} \PY{o+ow}{and} \PY{p}{(}\PY{n}{pval\PYZus{}1} \PY{o}{\PYZlt{}}\PY{o}{=}\PY{l+m+mf}{0.05}\PY{p}{)}\PY{p}{)}\PY{p}{:}
                 \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{These are both statistically significant!}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{k}{elif}\PY{p}{(}\PY{n}{pval\PYZus{}0} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{l+m+mf}{0.05}\PY{p}{)}\PY{p}{:}
                 \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Only beta\PYZus{}0 is statistically significant!}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{k}{elif}\PY{p}{(}\PY{n}{pval\PYZus{}1} \PY{o}{\PYZlt{}}\PY{o}{=} \PY{l+m+mf}{0.05}\PY{p}{)}\PY{p}{:}
                 \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Only beta\PYZus{}1 is statistically significant!}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             \PY{k}{else}\PY{p}{:}
                 \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{The parameters of this model are not statistically significant!}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    We can do the same calculations for significance for all the models
using this function

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}38}]:} \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model for Y\PYZus{}1: Explanatory variable X for Y\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{calcpvalue}\PY{p}{(}\PY{n}{X}\PY{p}{,}\PY{n}{Y\PYZus{}1}\PY{p}{,}\PY{n}{y1\PYZus{}fitted}\PY{p}{,}\PY{n}{beta1\PYZus{}0}\PY{p}{,}\PY{n}{beta1\PYZus{}1}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model for Y\PYZus{}2: Explanatory variable X for Y\PYZus{}2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{calcpvalue}\PY{p}{(}\PY{n}{X}\PY{p}{,}\PY{n}{Y\PYZus{}2}\PY{p}{,}\PY{n}{y2\PYZus{}fitted}\PY{p}{,}\PY{n}{beta2\PYZus{}0}\PY{p}{,}\PY{n}{beta2\PYZus{}1}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model for Y\PYZus{}2: Explanatory variable X\PYZca{}2 for Y\PYZus{}2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{calcpvalue}\PY{p}{(}\PY{n}{X}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{,}\PY{n}{Y\PYZus{}2}\PY{p}{,}\PY{n}{y22\PYZus{}fitted}\PY{p}{,}\PY{n}{beta22\PYZus{}0}\PY{p}{,}\PY{n}{beta22\PYZus{}1}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model for Y\PYZus{}3: Explanatory variable X for Y\PYZus{}3}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{calcpvalue}\PY{p}{(}\PY{n}{X}\PY{p}{,}\PY{n}{Y\PYZus{}3}\PY{p}{,}\PY{n}{y3\PYZus{}fitted}\PY{p}{,}\PY{n}{beta3\PYZus{}0}\PY{p}{,}\PY{n}{beta3\PYZus{}1}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Model for Y\_1: Explanatory variable X for Y\_1
SE(beta\_0) = 0.6406034056188337, SE(beta\_1) = 0.011151051418375258
beta\_0 t-statistic = 8.587595814509644
beta\_1 t-statistic = 454.150405635995
p-value for beta\_0 = 3.371970565016392e-17
p-value for beta\_1 = 0.0
These are both statistically significant!


Model for Y\_2: Explanatory variable X for Y\_2
SE(beta\_0) = 245.34955295438897, SE(beta\_1) = 4.2708256878947495
beta\_0 t-statistic = -34.424274285888536
beta\_1 t-statistic = 118.51588098729522
p-value for beta\_0 = 8.125468707425302e-172
p-value for beta\_1 = 0.0
These are both statistically significant!


Model for Y\_2: Explanatory variable X\^{}2 for Y\_2
SE(beta\_0) = 24.614546607361707, SE(beta\_1) = 0.005557804748590844
beta\_0 t-statistic = 0.5878663289694033
beta\_1 t-statistic = 913.1340896074505
p-value for beta\_0 = 0.5567550098751695
p-value for beta\_1 = 0.0
Only beta\_1 is statistically significant!


Model for Y\_3: Explanatory variable X for Y\_3
SE(beta\_0) = 0.15212372264589394, SE(beta\_1) = 0.0026480337730023893
beta\_0 t-statistic = 69.09601786545896
beta\_1 t-statistic = -38.21657519695403
p-value for beta\_0 = 0.0
p-value for beta\_1 = 1.3682773718716098e-197
These are both statistically significant!

    \end{Verbatim}

    We can use the statsmodels.api to verify our results

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}39}]:} \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model for Y\PYZus{}1: Explanatory variable X for Y\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} add a column of ones to X}
         \PY{n}{X\PYZus{}new} \PY{o}{=} \PY{n}{sm}\PY{o}{.}\PY{n}{add\PYZus{}constant}\PY{p}{(}\PY{n}{X}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} ordinary least squares approach to optimisation}
         \PY{n}{est} \PY{o}{=} \PY{n}{sm}\PY{o}{.}\PY{n}{OLS}\PY{p}{(}\PY{n}{Y\PYZus{}1}\PY{p}{,} \PY{n}{X\PYZus{}new}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} fit the data to the model using OLS}
         \PY{n}{est2} \PY{o}{=} \PY{n}{est}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} print a summary of the model}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{est2}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{}re\PYZhy{}run the above for all the models}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model for Y\PYZus{}2: Explanatory variable X for Y\PYZus{}2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{X\PYZus{}new} \PY{o}{=} \PY{n}{sm}\PY{o}{.}\PY{n}{add\PYZus{}constant}\PY{p}{(}\PY{n}{X}\PY{p}{)}
         \PY{n}{est} \PY{o}{=} \PY{n}{sm}\PY{o}{.}\PY{n}{OLS}\PY{p}{(}\PY{n}{Y\PYZus{}2}\PY{p}{,} \PY{n}{X\PYZus{}new}\PY{p}{)}
         \PY{n}{est2} \PY{o}{=} \PY{n}{est}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{est2}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model for Y\PYZus{}2: Explanatory variable X\PYZca{}2 for Y\PYZus{}2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{X\PYZus{}new} \PY{o}{=} \PY{n}{sm}\PY{o}{.}\PY{n}{add\PYZus{}constant}\PY{p}{(}\PY{n}{X}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)}
         \PY{n}{est} \PY{o}{=} \PY{n}{sm}\PY{o}{.}\PY{n}{OLS}\PY{p}{(}\PY{n}{Y\PYZus{}2}\PY{p}{,} \PY{n}{X\PYZus{}new}\PY{p}{)}
         \PY{n}{est2} \PY{o}{=} \PY{n}{est}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{est2}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model for Y\PYZus{}3: Explanatory variable X for Y\PYZus{}3}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{X\PYZus{}new} \PY{o}{=} \PY{n}{sm}\PY{o}{.}\PY{n}{add\PYZus{}constant}\PY{p}{(}\PY{n}{X}\PY{p}{)}
         \PY{n}{est} \PY{o}{=} \PY{n}{sm}\PY{o}{.}\PY{n}{OLS}\PY{p}{(}\PY{n}{Y\PYZus{}3}\PY{p}{,} \PY{n}{X\PYZus{}new}\PY{p}{)}
         \PY{n}{est2} \PY{o}{=} \PY{n}{est}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{est2}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model for Y\PYZus{}3: Explanatory variables X,X\PYZca{}2,X\PYZca{}3 for Y\PYZus{}3}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{c+c1}{\PYZsh{} concatenate multiple variables}
         \PY{n}{X\PYZus{}new} \PY{o}{=} \PY{n}{sm}\PY{o}{.}\PY{n}{add\PYZus{}constant}\PY{p}{(}\PY{n}{pd}\PY{o}{.}\PY{n}{concat}\PY{p}{(}\PY{p}{[}\PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{X}\PY{p}{,}\PY{n}{columns}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{p}{,}\PYZbs{}
                                            \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{X}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{,}\PY{n}{columns}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{p}{,}\PYZbs{}
                                            \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{X}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{3}\PY{p}{,}\PY{n}{columns}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X3}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{p}{]}\PY{p}{,}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
         \PY{n}{est} \PY{o}{=} \PY{n}{sm}\PY{o}{.}\PY{n}{OLS}\PY{p}{(}\PY{n}{Y\PYZus{}3}\PY{p}{,} \PY{n}{X\PYZus{}new}\PY{p}{)}
         \PY{n}{est2} \PY{o}{=} \PY{n}{est}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{est2}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Model for Y\_1: Explanatory variable X for Y\_1
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.995
Model:                            OLS   Adj. R-squared:                  0.995
Method:                 Least Squares   F-statistic:                 2.063e+05
Date:                Sat, 27 Jul 2019   Prob (F-statistic):               0.00
Time:                        18:22:13   Log-Likelihood:                -3730.1
No. Observations:                1000   AIC:                             7464.
Df Residuals:                     998   BIC:                             7474.
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const          5.5012      0.641      8.588      0.000       4.244       6.758
x1             5.0643      0.011    454.150      0.000       5.042       5.086
==============================================================================
Omnibus:                        0.350   Durbin-Watson:                   1.952
Prob(Omnibus):                  0.839   Jarque-Bera (JB):                0.376
Skew:                          -0.045   Prob(JB):                        0.828
Kurtosis:                       2.970   Cond. No.                         115.
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.


Model for Y\_2: Explanatory variable X for Y\_2
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.934
Model:                            OLS   Adj. R-squared:                  0.934
Method:                 Least Squares   F-statistic:                 1.405e+04
Date:                Sat, 27 Jul 2019   Prob (F-statistic):               0.00
Time:                        18:22:13   Log-Likelihood:                -9678.1
No. Observations:                1000   AIC:                         1.936e+04
Df Residuals:                     998   BIC:                         1.937e+04
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const      -8445.9803    245.350    -34.424      0.000   -8927.440   -7964.520
x1           506.1607      4.271    118.516      0.000     497.780     514.541
==============================================================================
Omnibus:                      136.837   Durbin-Watson:                   1.872
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              102.303
Skew:                           0.681   Prob(JB):                     6.10e-23
Kurtosis:                       2.227   Cond. No.                         115.
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.


Model for Y\_2: Explanatory variable X\^{}2 for Y\_2
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.999
Model:                            OLS   Adj. R-squared:                  0.999
Method:                 Least Squares   F-statistic:                 8.338e+05
Date:                Sat, 27 Jul 2019   Prob (F-statistic):               0.00
Time:                        18:22:13   Log-Likelihood:                -7670.0
No. Observations:                1000   AIC:                         1.534e+04
Df Residuals:                     998   BIC:                         1.535e+04
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const         14.4701     24.615      0.588      0.557     -33.832      62.772
x1             5.0750      0.006    913.134      0.000       5.064       5.086
==============================================================================
Omnibus:                        5.725   Durbin-Watson:                   2.021
Prob(Omnibus):                  0.057   Jarque-Bera (JB):                7.275
Skew:                           0.018   Prob(JB):                       0.0263
Kurtosis:                       3.416   Cond. No.                     6.64e+03
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 6.64e+03. This might indicate that there are
strong multicollinearity or other numerical problems.


Model for Y\_3: Explanatory variable X for Y\_3
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.594
Model:                            OLS   Adj. R-squared:                  0.594
Method:                 Least Squares   F-statistic:                     1461.
Date:                Sat, 27 Jul 2019   Prob (F-statistic):          1.37e-197
Time:                        18:22:13   Log-Likelihood:                -2292.4
No. Observations:                1000   AIC:                             4589.
Df Residuals:                     998   BIC:                             4599.
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const         10.5111      0.152     69.096      0.000      10.213      10.810
x1            -0.1012      0.003    -38.217      0.000      -0.106      -0.096
==============================================================================
Omnibus:                       26.494   Durbin-Watson:                   1.871
Prob(Omnibus):                  0.000   Jarque-Bera (JB):               28.130
Skew:                          -0.405   Prob(JB):                     7.79e-07
Kurtosis:                       2.860   Cond. No.                         115.
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.


Model for Y\_3: Explanatory variables X,X\^{}2,X\^{}3 for Y\_3
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.923
Model:                            OLS   Adj. R-squared:                  0.923
Method:                 Least Squares   F-statistic:                     3974.
Date:                Sat, 27 Jul 2019   Prob (F-statistic):               0.00
Time:                        18:22:13   Log-Likelihood:                -1461.8
No. Observations:                1000   AIC:                             2932.
Df Residuals:                     996   BIC:                             2951.
Df Model:                           3                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const          3.6644      0.128     28.526      0.000       3.412       3.917
X              0.4871      0.011     43.605      0.000       0.465       0.509
X2            -0.0112      0.000    -42.571      0.000      -0.012      -0.011
X3          5.868e-05   1.74e-06     33.743      0.000    5.53e-05    6.21e-05
==============================================================================
Omnibus:                        0.415   Durbin-Watson:                   1.980
Prob(Omnibus):                  0.813   Jarque-Bera (JB):                0.368
Skew:                           0.046   Prob(JB):                        0.832
Kurtosis:                       3.019   Cond. No.                     1.46e+06
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 1.46e+06. This might indicate that there are
strong multicollinearity or other numerical problems.

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
C:\textbackslash{}Users\textbackslash{}HVAD\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}numpy\textbackslash{}core\textbackslash{}fromnumeric.py:52: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.
  return getattr(obj, method)(*args, **kwds)

    \end{Verbatim}

    It looks like the intercept for \emph{Model for Y\_2: Explanatory
variable X\^{}2 for Y\_2} is not statistically significant. The
intercept can then be omitted from the model and fitted again.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}40}]:} \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model for Y\PYZus{}2: Explanatory variable X\PYZca{}2 for Y\PYZus{}2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{est} \PY{o}{=} \PY{n}{sm}\PY{o}{.}\PY{n}{OLS}\PY{p}{(}\PY{n}{Y\PYZus{}2}\PY{p}{,} \PY{n}{X}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)}
         \PY{n}{est2} \PY{o}{=} \PY{n}{est}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{est2}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Model for Y\_2: Explanatory variable X\^{}2 for Y\_2
                                 OLS Regression Results                                
=======================================================================================
Dep. Variable:                      y   R-squared (uncentered):                   0.999
Model:                            OLS   Adj. R-squared (uncentered):              0.999
Method:                 Least Squares   F-statistic:                          1.878e+06
Date:                Sat, 27 Jul 2019   Prob (F-statistic):                        0.00
Time:                        18:22:14   Log-Likelihood:                         -7670.2
No. Observations:                1000   AIC:                                  1.534e+04
Df Residuals:                     999   BIC:                                  1.535e+04
Df Model:                           1                                                  
Covariance Type:            nonrobust                                                  
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
x1             5.0775      0.004   1370.392      0.000       5.070       5.085
==============================================================================
Omnibus:                        6.001   Durbin-Watson:                   2.020
Prob(Omnibus):                  0.050   Jarque-Bera (JB):                7.710
Skew:                           0.019   Prob(JB):                       0.0212
Kurtosis:                       3.428   Cond. No.                         1.00
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.

    \end{Verbatim}

    This is a good fit also

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}41}]:} \PY{n}{x23} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{99}\PY{p}{,}\PY{l+m+mi}{1000}\PY{p}{)} \PY{c+c1}{\PYZsh{} 1000 linearly spaced numbers}
         \PY{n}{y23} \PY{o}{=} \PY{n}{est2}\PY{o}{.}\PY{n}{params}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{*} \PY{n}{x23}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}
         
         \PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{p}{)}
         \PY{n}{axes} \PY{o}{=} \PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}axes}\PY{p}{(}\PY{p}{[}\PY{l+m+mf}{0.1}\PY{p}{,}\PY{l+m+mf}{0.1}\PY{p}{,}\PY{l+m+mf}{0.8}\PY{p}{,}\PY{l+m+mf}{0.8}\PY{p}{]}\PY{p}{)}
         \PY{n}{axes}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{X}\PY{p}{,}\PY{n}{Y\PYZus{}2}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{x}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{f\PYZus{}2(x)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{axes}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x23}\PY{p}{,}\PY{n}{y23}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}41}]:} [<matplotlib.lines.Line2D at 0x1b551a9aa58>]
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_80_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    If we set \(\beta_0=0\) in the derivation for \(\hat{\beta_0}\) and
\(\hat{\beta_1}\) earlier in the article, we would have obtained the
equation

\[\hat{\beta_1} = \frac{\sum_{i=1}^n y_i x_i}{\sum_{i=1}^n x_i^2}\]

Using this equation, we can reproduce the statsmodels solution above.
Note that removing \(\beta_0\) has changed \(\beta_1\) slightly:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}42}]:} \PY{c+c1}{\PYZsh{} remember that we are fitting the variable X\PYZca{}2}
         \PY{n}{sum1} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{Y\PYZus{}2}\PY{o}{*}\PY{n}{X}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)}
         \PY{n}{sum2} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{X}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{4}\PY{p}{)}
         
         \PY{n}{beta23\PYZus{}1} \PY{o}{=} \PY{n}{sum1}\PY{o}{/}\PY{n}{sum2}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Y \PYZti{} }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{ X\PYZca{}2}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{beta23\PYZus{}1}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Y \textasciitilde{} 5.077455649665152 X\^{}2

    \end{Verbatim}

    \hypertarget{f-statistic}{%
\paragraph{F-Statistic}\label{f-statistic}}

The F-Statistic answers the question `Is there evidence that at least
one of the explanatory variables is related to the response variable?'.
This corresponds to a hypothesis test with:

\[H_O: \beta_0, \beta_1, ..., \beta_p = 0\]
\[H_A: \text{at least one of } \beta_i \text{ is non-zero}\]

The F-Statistic has the form:

\[F = \frac{(TSS - RSS)/p}{RSS/(n-p-1)}\]

where p is the number of explanatory variables/parameters.

If \(H_O\) is not true, the numerator in the above equation becomes
larger, i.e.~F \textgreater{} 1. If \(H_0\) is true, then the
F-Statistic is close to 1.

(PROOF of this - take expectation of numerator and denominator and these
are both equal to Var(\(\epsilon\)). If \(H_A\) is true then the
numerator \textgreater{} Var(\(\epsilon\)))

We can use this to calculate the F-Statistics of the above models:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}43}]:} \PY{k}{def} \PY{n+nf}{FStat}\PY{p}{(}\PY{n}{n}\PY{p}{,}\PY{n}{p}\PY{p}{,}\PY{n}{TSS}\PY{p}{,}\PY{n}{RSS}\PY{p}{)}\PY{p}{:}
             \PY{n}{F} \PY{o}{=} \PY{p}{(}\PY{p}{(}\PY{n}{TSS}\PY{o}{\PYZhy{}}\PY{n}{RSS}\PY{p}{)}\PY{o}{/}\PY{n}{p}\PY{p}{)}\PY{o}{/}\PY{p}{(}\PY{n}{RSS}\PY{o}{/}\PY{p}{(}\PY{n}{n}\PY{o}{\PYZhy{}}\PY{n}{p}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{The F\PYZhy{}Statistic is }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{F}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}44}]:} \PY{c+c1}{\PYZsh{} we didn\PYZsq{}t calculate the last model ourselves, we used sklearn }
         \PY{c+c1}{\PYZsh{} so we retrieve the coefficients}
         \PY{n}{beta32\PYZus{}0} \PY{o}{=} \PY{n}{lm32}\PY{o}{.}\PY{n}{intercept\PYZus{}}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
         \PY{n}{beta32\PYZus{}1} \PY{o}{=} \PY{n}{lm32}\PY{o}{.}\PY{n}{coef\PYZus{}}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
         \PY{n}{beta32\PYZus{}2} \PY{o}{=} \PY{n}{lm32}\PY{o}{.}\PY{n}{coef\PYZus{}}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
         \PY{n}{beta32\PYZus{}3} \PY{o}{=} \PY{n}{lm32}\PY{o}{.}\PY{n}{coef\PYZus{}}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}45}]:} \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model for Y\PYZus{}1: Explanatory variable X for Y\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{FStat}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{n}{TSS\PYZus{}1}\PY{p}{,}\PY{n}{RSS\PYZus{}1}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{}re\PYZhy{}run the above for all the models}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model for Y\PYZus{}2: Explanatory variable X for Y\PYZus{}2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{FStat}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{n}{TSS\PYZus{}2}\PY{p}{,}\PY{n}{RSS\PYZus{}2}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model for Y\PYZus{}2: Explanatory variable X\PYZca{}2 for Y\PYZus{}2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{FStat}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{n}{TSS\PYZus{}22}\PY{p}{,}\PY{n}{RSS\PYZus{}22}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model for Y\PYZus{}3: Explanatory variable X for Y\PYZus{}3}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{FStat}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{n}{TSS\PYZus{}3}\PY{p}{,}\PY{n}{RSS\PYZus{}3}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
         
         \PY{n}{TSS\PYZus{}32}\PY{p}{,}\PY{n}{RSS\PYZus{}32} \PY{o}{=} \PY{n}{TSS\PYZus{}RSS}\PY{p}{(}\PY{n}{Y\PYZus{}3}\PY{p}{,}\PY{n}{y32\PYZus{}fitted\PYZus{}sklearn}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model for Y\PYZus{}3: Explanatory variables X,X\PYZca{}2,X\PYZca{}3 for Y\PYZus{}3}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{c+c1}{\PYZsh{} now we have 3 explanatory variables}
         \PY{n}{FStat}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{,}\PY{n}{TSS\PYZus{}32}\PY{p}{,}\PY{n}{RSS\PYZus{}32}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Model for Y\_1: Explanatory variable X for Y\_1
The F-Statistic is 206252.59093933867


Model for Y\_2: Explanatory variable X for Y\_2
The F-Statistic is 14046.014046194661


Model for Y\_2: Explanatory variable X\^{}2 for Y\_2
The F-Statistic is 833813.8656032282


Model for Y\_3: Explanatory variable X for Y\_3
The F-Statistic is 1460.506619784441


Model for Y\_3: Explanatory variables X,X\^{}2,X\^{}3 for Y\_3
The F-Statistic is 3974.16032266946

    \end{Verbatim}

    These match the \emph{statsmodels} outputs. We can also find the p-value
of a coefficient/intercept using the F-Statistic. The F-Statistic
formula becomes:

\[F = \frac{(RSS_0 - RSS)/q}{RSS/(n-p-1)}\]

where \(RSS_0\) is the residual sum of squares for the model with \(q\)
removed parameters. The corresponding hypothesis test is then

\[H_0: \{ \beta_i = 0 \} \text{ where i takes on the q removed parameters}\]
\[H_A: \text{at least one of those q parameters is non-zero}\]

Above, we ran a model for Y\_2 which had an intercept, coefficient of
X\^{}2 and RSS of:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}46}]:} \PY{n}{beta22\PYZus{}0}\PY{p}{,} \PY{n}{beta22\PYZus{}1}\PY{p}{,} \PY{n}{RSS\PYZus{}22}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}46}]:} (14.470063153316005, 5.075020979320466, 268902718.6114595)
\end{Verbatim}
            
    Here, we are going to calculate the p-value of the intercept for Y\_2
when we try to fit an intercept as well as \(X^2\). We do this by first
fitting the full model including the intercept and getting the RSS
value, then we fit the model without the intercept and get the RSS
value. The Coefficient of X\^{}2 and RSS for the model without the
intercept was calculated to be

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}47}]:} \PY{n}{TSS\PYZus{}23}\PY{p}{,}\PY{n}{RSS\PYZus{}23} \PY{o}{=} \PY{n}{TSS\PYZus{}RSS}\PY{p}{(}\PY{n}{Y\PYZus{}2}\PY{p}{,}\PY{n}{beta23\PYZus{}1} \PY{o}{*} \PY{n}{X}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{beta\PYZus{}1 = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{, RSS\PYZus{}0 = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{beta23\PYZus{}1}\PY{p}{,}\PY{n}{RSS\PYZus{}23}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
beta\_1 = 5.077455649665152, RSS\_0 = 268995834.0780044

    \end{Verbatim}

    We now create a function to apply the formula shown above for
calculating the F-Statistic for comparing models

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}48}]:} \PY{k}{def} \PY{n+nf}{FStatCompare}\PY{p}{(}\PY{n}{n}\PY{p}{,}\PY{n}{p}\PY{p}{,}\PY{n}{q}\PY{p}{,}\PY{n}{RSS0}\PY{p}{,}\PY{n}{RSS}\PY{p}{)}\PY{p}{:}
             \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}
         \PY{l+s+sd}{    A function to calculate the F\PYZhy{}Statistic when we are comparing models }
         \PY{l+s+sd}{        with different number of parameters.}
         \PY{l+s+sd}{    RSS0 is a sub\PYZhy{}model of RSS}
         \PY{l+s+sd}{    \PYZsq{}\PYZsq{}\PYZsq{}}
             \PY{n}{F} \PY{o}{=} \PY{p}{(}\PY{p}{(}\PY{n}{RSS0}\PY{o}{\PYZhy{}}\PY{n}{RSS}\PY{p}{)}\PY{o}{/}\PY{n}{q}\PY{p}{)}\PY{o}{/}\PY{p}{(}\PY{n}{RSS}\PY{o}{/}\PY{p}{(}\PY{n}{n}\PY{o}{\PYZhy{}}\PY{n}{p}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{The F\PYZhy{}Statistic is }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{F}\PY{p}{)}\PY{p}{)}
             \PY{k}{return} \PY{n}{F}
\end{Verbatim}


    Now we can confirm the p-value for the intercept

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}49}]:} \PY{c+c1}{\PYZsh{} This is the fitted values for the model with no intercept}
         \PY{n}{Y23\PYZus{}fitted} \PY{o}{=} \PY{n}{beta23\PYZus{}1} \PY{o}{*} \PY{n}{X}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}
         
         \PY{c+c1}{\PYZsh{} These are the TSS and RSS for this model with no intercept}
         \PY{n}{TSS\PYZus{}2\PYZus{}test}\PY{p}{,}\PY{n}{RSS\PYZus{}2\PYZus{}test} \PY{o}{=} \PY{n}{TSS\PYZus{}RSS}\PY{p}{(}\PY{n}{Y\PYZus{}2}\PY{p}{,}\PY{n}{Y23\PYZus{}fitted}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} RSS\PYZus{}22 is the RSS for the model with the intercept. RSS\PYZus{}23 is the RSS}
         \PY{c+c1}{\PYZsh{} for the model without the intercept. We have p = 0 and q = 1 (i.e. we have }
         \PY{c+c1}{\PYZsh{} removed 1 parameter but there was only 1 parameter to begin with)}
         \PY{n}{F} \PY{o}{=} \PY{n}{FStatCompare}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{n}{RSS\PYZus{}23}\PY{p}{,}\PY{n}{RSS\PYZus{}22}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} the following function calculates the area underneath the cdf F\PYZhy{}distribution }
         \PY{c+c1}{\PYZsh{} with dfn(degrees of freedom in the numerator)=1, }
         \PY{c+c1}{\PYZsh{} dfd(degrees of freedom in the denominator)=len(X)\PYZhy{}2 less than 0.5}
         \PY{n}{stats}\PY{o}{.}\PY{n}{f}\PY{o}{.}\PY{n}{cdf}\PY{p}{(}\PY{l+m+mf}{0.5}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{The p\PYZhy{}value of the intercept is }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{l+m+mi}{1}\PY{o}{\PYZhy{}}\PY{n}{stats}\PY{o}{.}\PY{n}{f}\PY{o}{.}\PY{n}{cdf}\PY{p}{(}\PY{n}{F}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
The F-Statistic is 0.3459331001141355
The p-value of the intercept is 0.5565574505496756

    \end{Verbatim}

    Note that above, we removed the intercept and used the F-Statistic to
calculate the p-value for the intercept. We can also remove the
coefficient of X\^{}2 and calculate the p-value of this coefficient
using the same procedure as above. First fit the model as we have done
before

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}50}]:} \PY{n}{lmOnlyIntercept} \PY{o}{=} \PY{n}{LinearRegression}\PY{p}{(}\PY{p}{)}
         \PY{n}{lmOnlyIntercept}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{p}{(}\PY{n}{X}\PY{o}{*}\PY{l+m+mi}{0}\PY{p}{)}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}\PY{n}{Y\PYZus{}2}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model for Y\PYZus{}2: No explanatory variable for Y\PYZus{}2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{beta\PYZus{}0 = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{lmOnlyIntercept}\PY{o}{.}\PY{n}{intercept\PYZus{}}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}
         \PY{n}{yOnlyIntercept\PYZus{}fitted\PYZus{}sklearn} \PY{o}{=} \PY{n}{lmOnlyIntercept}\PY{o}{.}\PY{n}{intercept\PYZus{}}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{+} \PY{n}{X}\PY{o}{*}\PY{l+m+mi}{0}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{R\PYZca{}2 = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{r2\PYZus{}score}\PY{p}{(}\PY{n}{Y\PYZus{}2}\PY{p}{,}\PY{n}{yOnlyIntercept\PYZus{}fitted\PYZus{}sklearn}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Model for Y\_2: No explanatory variable for Y\_2
beta\_0 = 16763.308428792458
R\^{}2 = 0.0

    \end{Verbatim}

    Next, calculate the RSS for this model we have just fitted

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}51}]:} \PY{n}{TSS\PYZus{}OnlyIntercept}\PY{p}{,}\PY{n}{RSS\PYZus{}OnlyIntercept} \PY{o}{=} \PY{n}{TSS\PYZus{}RSS}\PY{p}{(}\PY{n}{Y\PYZus{}2}\PY{p}{,}\PY{n}{yOnlyIntercept\PYZus{}fitted\PYZus{}sklearn}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{beta\PYZus{}0 = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{, RSS\PYZus{}0 = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{lmOnlyIntercept}\PY{o}{.}\PY{n}{intercept\PYZus{}}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PYZbs{}
                                                \PY{n}{RSS\PYZus{}OnlyIntercept}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
beta\_0 = 16763.308428792458, RSS\_0 = 224933046282.3772

    \end{Verbatim}

    And now we calculate the p-value of the coefficient of X\^{}2

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}52}]:} \PY{c+c1}{\PYZsh{} These are the TSS and RSS for this model with only intercept}
         \PY{n}{TSS\PYZus{}2\PYZus{}test}\PY{p}{,}\PY{n}{RSS\PYZus{}2\PYZus{}test} \PY{o}{=} \PY{n}{TSS\PYZus{}RSS}\PY{p}{(}\PY{n}{Y\PYZus{}2}\PY{p}{,}\PY{n}{yOnlyIntercept\PYZus{}fitted\PYZus{}sklearn}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} RSS\PYZus{}22 is the RSS for the model with the intercept. RSS\PYZus{}23 is the RSS}
         \PY{c+c1}{\PYZsh{} for the model without the intercept. We have p = 0 and q = 1 (i.e. we have }
         \PY{c+c1}{\PYZsh{} removed 1 parameter but there was only 1 parameter to begin with)}
         \PY{n}{F} \PY{o}{=} \PY{n}{FStatCompare}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{n}{RSS\PYZus{}2\PYZus{}test}\PY{p}{,}\PY{n}{RSS\PYZus{}22}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} the following function calculates the area underneath the cdf F\PYZhy{}distribution }
         \PY{c+c1}{\PYZsh{} with dfn(degrees of freedom in the numerator)=1, }
         \PY{c+c1}{\PYZsh{} dfd(degrees of freedom in the denominator)=len(X)\PYZhy{}2 less than 0.5}
         \PY{n}{stats}\PY{o}{.}\PY{n}{f}\PY{o}{.}\PY{n}{cdf}\PY{p}{(}\PY{l+m+mf}{0.5}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{The p\PYZhy{}value of the X\PYZca{}2 coefficient is }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PYZbs{}
               \PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{l+m+mi}{1}\PY{o}{\PYZhy{}}\PY{n}{stats}\PY{o}{.}\PY{n}{f}\PY{o}{.}\PY{n}{cdf}\PY{p}{(}\PY{n}{F}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
The F-Statistic is 834649.3504385022
The p-value of the X\^{}2 coefficient is 1.1102230246251565e-16

    \end{Verbatim}

    \hypertarget{synergy-effect}{%
\subsubsection{Synergy Effect}\label{synergy-effect}}

Suppose we have the following function

\[f(x)=4.67+2*X_1+3*X_2+5.07∗X_1*X_2\]

We can see that there is a mixed term `\(X_1 X_2\)'. This is called a
synergy effect.

Let's define this function and plot it

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}53}]:} \PY{c+c1}{\PYZsh{} We will need to plot in 3D}
         \PY{k+kn}{from} \PY{n+nn}{mpl\PYZus{}toolkits}\PY{n+nn}{.}\PY{n+nn}{mplot3d} \PY{k}{import} \PY{n}{Axes3D}
         
         \PY{c+c1}{\PYZsh{}f(x)=4.67+2*X\PYZus{}1+30*X\PYZus{}2+5.07∗X\PYZus{}1*X\PYZus{}2}
         \PY{k}{def} \PY{n+nf}{f}\PY{p}{(}\PY{n}{x1}\PY{p}{,}\PY{n}{x2}\PY{p}{)}\PY{p}{:}
             \PY{k}{return} \PY{l+m+mf}{4.67}\PY{o}{+}\PY{l+m+mi}{2}\PY{o}{*}\PY{n}{x1}\PY{o}{+}\PY{l+m+mi}{30}\PY{o}{*}\PY{n}{x2}\PY{o}{+}\PY{l+m+mf}{5.07}\PY{o}{*}\PY{n}{x1}\PY{o}{*}\PY{n}{x2}
         \PY{c+c1}{\PYZsh{} Set the seed}
         \PY{n}{r} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{RandomState}\PY{p}{(}\PY{l+m+mi}{101}\PY{p}{)}
         \PY{n}{X\PYZus{}1} \PY{o}{=} \PY{l+m+mi}{100}\PY{o}{*}\PY{n}{r}\PY{o}{.}\PY{n}{rand}\PY{p}{(}\PY{l+m+mi}{1000}\PY{p}{)}
         \PY{n}{X\PYZus{}2} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{20}\PY{o}{*}\PY{n}{r}\PY{o}{.}\PY{n}{rand}\PY{p}{(}\PY{l+m+mi}{1000}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{}Error term with sigma = 10, mu = 0}
         \PY{n}{E} \PY{o}{=} \PY{l+m+mi}{100}\PY{o}{*}\PY{n}{r}\PY{o}{.}\PY{n}{randn}\PY{p}{(}\PY{l+m+mi}{1000}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{}Response variables}
         \PY{n}{Y} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n+nb}{map}\PY{p}{(}\PY{n}{f}\PY{p}{,}\PY{n}{X\PYZus{}1}\PY{p}{,}\PY{n}{X\PYZus{}2}\PY{p}{)}\PY{p}{)}\PY{o}{+}\PY{n}{E}
         
         \PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{p}{)}
         \PY{n}{axes} \PY{o}{=} \PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}axes}\PY{p}{(}\PY{p}{[}\PY{l+m+mf}{0.1}\PY{p}{,}\PY{l+m+mf}{0.1}\PY{p}{,}\PY{l+m+mf}{0.8}\PY{p}{,}\PY{l+m+mf}{0.8}\PY{p}{]}\PY{p}{)}
         \PY{n}{axes}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{X\PYZus{}2}\PY{p}{,}\PY{n}{Y}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{x}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{f(x)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}53}]:} Text(0,0.5,'f(x)')
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_102_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}54}]:} \PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}
         \PY{n}{ax} \PY{o}{=} \PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}subplot}\PY{p}{(}\PY{l+m+mi}{111}\PY{p}{,} \PY{n}{projection}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{3d}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{n}{ax}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{X\PYZus{}1}\PY{p}{,} \PY{n}{X\PYZus{}2}\PY{p}{,} \PY{n}{Y}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{r}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{marker}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X\PYZus{}2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}zlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{n}{plt}\PY{o}{.}\PY{n}{tight\PYZus{}layout}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_103_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Suppose we continued to fit a linear regression model with parameters
\(X_1\) and \(X_2\) with the assumption that there is no synergy effect.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}55}]:} \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model for Y: Explanatory variable X\PYZus{}1 and X\PYZus{}2 for Y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{X\PYZus{}new} \PY{o}{=} \PY{n}{sm}\PY{o}{.}\PY{n}{add\PYZus{}constant}\PY{p}{(}\PY{n}{pd}\PY{o}{.}\PY{n}{concat}\PY{p}{(}\PY{p}{[}\PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{X\PYZus{}1}\PY{p}{,}\PY{n}{columns}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{p}{,}\PYZbs{}
                                            \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{X\PYZus{}2}\PY{p}{,}\PY{n}{columns}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X\PYZus{}2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{p}{]}\PY{p}{,}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
         \PY{n}{est} \PY{o}{=} \PY{n}{sm}\PY{o}{.}\PY{n}{OLS}\PY{p}{(}\PY{n}{Y}\PY{p}{,} \PY{n}{X\PYZus{}new}\PY{p}{)}
         \PY{n}{est2} \PY{o}{=} \PY{n}{est}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{est2}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Model for Y: Explanatory variable X\_1 and X\_2 for Y
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.864
Model:                            OLS   Adj. R-squared:                  0.864
Method:                 Least Squares   F-statistic:                     3169.
Date:                Sat, 27 Jul 2019   Prob (F-statistic):               0.00
Time:                        18:22:16   Log-Likelihood:                -8160.3
No. Observations:                1000   AIC:                         1.633e+04
Df Residuals:                     997   BIC:                         1.634e+04
Df Model:                           2                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const       2562.3530     71.652     35.761      0.000    2421.746    2702.960
X\_1          -49.6977      0.937    -53.063      0.000     -51.536     -47.860
X\_2          279.5368      4.683     59.693      0.000     270.347     288.726
==============================================================================
Omnibus:                        3.561   Durbin-Watson:                   1.909
Prob(Omnibus):                  0.169   Jarque-Bera (JB):                4.035
Skew:                          -0.022   Prob(JB):                        0.133
Kurtosis:                       3.308   Cond. No.                         155.
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
C:\textbackslash{}Users\textbackslash{}HVAD\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}numpy\textbackslash{}core\textbackslash{}fromnumeric.py:52: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.
  return getattr(obj, method)(*args, **kwds)

    \end{Verbatim}

    The above output shows that the \(R^2\) is almost 87\% with both \(X_1\)
and \(X_2\) being statistically significant. Below, we show that
including the synergy term \(X_1 X_2\) into the model as well greatly
improves the \(R^2\) metric.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}56}]:} \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model for f: Explanatory variables X\PYZus{}1, X\PYZus{}2 and X\PYZus{}1 * X\PYZus{}2 for Y\PYZus{}2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{X\PYZus{}new} \PY{o}{=} \PY{n}{sm}\PY{o}{.}\PY{n}{add\PYZus{}constant}\PY{p}{(}\PY{n}{pd}\PY{o}{.}\PY{n}{concat}\PY{p}{(}\PY{p}{[}\PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{X\PYZus{}1}\PY{p}{,}\PY{n}{columns}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{p}{,}\PYZbs{}
                                            \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{X\PYZus{}2}\PY{p}{,}\PY{n}{columns}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X\PYZus{}2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{p}{,}\PYZbs{}
                                            \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{X\PYZus{}1}\PY{o}{*}\PY{n}{X\PYZus{}2}\PY{p}{,}\PY{n}{columns}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X\PYZus{}12}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{p}{]}\PY{p}{,}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
         \PY{n}{est} \PY{o}{=} \PY{n}{sm}\PY{o}{.}\PY{n}{OLS}\PY{p}{(}\PY{n}{Y}\PY{p}{,} \PY{n}{X\PYZus{}new}\PY{p}{)}
         \PY{n}{est2} \PY{o}{=} \PY{n}{est}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{est2}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Model for f: Explanatory variables X\_1, X\_2 and X\_1 * X\_2 for Y\_2
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.998
Model:                            OLS   Adj. R-squared:                  0.998
Method:                 Least Squares   F-statistic:                 1.651e+05
Date:                Sat, 27 Jul 2019   Prob (F-statistic):               0.00
Time:                        18:22:16   Log-Likelihood:                -6052.5
No. Observations:                1000   AIC:                         1.211e+04
Df Residuals:                     996   BIC:                         1.213e+04
Df Model:                           3                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const         17.9262     13.164      1.362      0.174      -7.907      43.759
X\_1            2.0906      0.231      9.054      0.000       1.637       2.544
X\_2           30.3293      1.122     27.035      0.000      28.128      32.531
X\_12           5.0841      0.020    257.798      0.000       5.045       5.123
==============================================================================
Omnibus:                        8.045   Durbin-Watson:                   2.015
Prob(Omnibus):                  0.018   Jarque-Bera (JB):               11.082
Skew:                           0.035   Prob(JB):                      0.00392
Kurtosis:                       3.511   Cond. No.                     2.69e+03
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 2.69e+03. This might indicate that there are
strong multicollinearity or other numerical problems.

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
C:\textbackslash{}Users\textbackslash{}HVAD\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}numpy\textbackslash{}core\textbackslash{}fromnumeric.py:52: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.
  return getattr(obj, method)(*args, **kwds)

    \end{Verbatim}

    We have seen above that adding a term \(X_1 X_2\) significantly
increased the \(R^2\) statistic. Instead of adding this joint term, what
would be the effect on \(R^2\) if we added random noise? We see the
effect below.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}57}]:} \PY{c+c1}{\PYZsh{} Set the seed}
         \PY{n}{r} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{RandomState}\PY{p}{(}\PY{l+m+mi}{11}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{}Error term with sigma = 10, mu = 0}
         \PY{n}{noise} \PY{o}{=} \PY{l+m+mi}{100}\PY{o}{*}\PY{n}{r}\PY{o}{.}\PY{n}{randn}\PY{p}{(}\PY{l+m+mi}{1000}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Model for Y: Explanatory variable X\PYZus{}1 and X\PYZus{}2 for Y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{X\PYZus{}new} \PY{o}{=} \PY{n}{sm}\PY{o}{.}\PY{n}{add\PYZus{}constant}\PY{p}{(}\PY{n}{pd}\PY{o}{.}\PY{n}{concat}\PY{p}{(}\PY{p}{[}\PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{X\PYZus{}1}\PY{p}{,}\PY{n}{columns}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X\PYZus{}1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{p}{,}\PYZbs{}
                                            \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{X\PYZus{}2}\PY{p}{,}\PY{n}{columns}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X\PYZus{}2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{p}{,}\PYZbs{}
                                           \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{noise}\PY{p}{,}\PY{n}{columns}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Noise}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{p}{]}\PY{p}{,}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
         \PY{n}{est} \PY{o}{=} \PY{n}{sm}\PY{o}{.}\PY{n}{OLS}\PY{p}{(}\PY{n}{Y}\PY{p}{,} \PY{n}{X\PYZus{}new}\PY{p}{)}
         \PY{n}{est2} \PY{o}{=} \PY{n}{est}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{est2}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Model for Y: Explanatory variable X\_1 and X\_2 for Y
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.865
Model:                            OLS   Adj. R-squared:                  0.864
Method:                 Least Squares   F-statistic:                     2123.
Date:                Sat, 27 Jul 2019   Prob (F-statistic):               0.00
Time:                        18:22:16   Log-Likelihood:                -8157.8
No. Observations:                1000   AIC:                         1.632e+04
Df Residuals:                     996   BIC:                         1.634e+04
Df Model:                           3                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const       2548.0238     71.812     35.482      0.000    2407.103    2688.945
X\_1          -49.6148      0.936    -53.033      0.000     -51.451     -47.779
X\_2          278.5613      4.695     59.331      0.000     269.348     287.775
Noise         -0.5863      0.267     -2.196      0.028      -1.110      -0.062
==============================================================================
Omnibus:                        3.763   Durbin-Watson:                   1.906
Prob(Omnibus):                  0.152   Jarque-Bera (JB):                4.348
Skew:                          -0.006   Prob(JB):                        0.114
Kurtosis:                       3.323   Cond. No.                         271.
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
C:\textbackslash{}Users\textbackslash{}HVAD\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}numpy\textbackslash{}core\textbackslash{}fromnumeric.py:52: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.
  return getattr(obj, method)(*args, **kwds)

    \end{Verbatim}

    Including an unrelated, random noise term to the model increases the
\(R^2\) statistic. This makes sense since when fitting the model to the
training data, in the worst case, the model could choose a predictor's
coefficient to be zero. This means that the \(R^2\) statistic for the
training data should never decrease as a function of the number of
predictors. The main reason for introducing such a metric is to gauge
how well the model describes the population from which our data
originates from. However, if it never decreases then how can it be
determined whether the added parameter is useful or not?

In order to cater for this, the Adjusted \(R^2\) metric can be used.
This metric applies a penalty to the usual \(R^2\) the more predictors
that are used. This way, it is not possible that the Adjusted \(R^2\)
can increase indefinitely. At some point, the contribution to the
\(R^2\) of adding a new predictor will be overcome by the penalty
attributed to adding that new parameter. The Adjusted \(R^2\) is as
follows:

\[ \text{Adjusted }R^2 = 1 - \frac{ RSS/(n - p - 1) }{ TSS/(n - 1) } \]

where \(p\) is the number of predictors. It can be seen in the above
model that the Adjusted \(R^2\) did not increase with the addition of
another predictor.

Another approach we can apply to take into account that the test \(R^2\)
will always be smaller than the training \(R^2\), is to divide the data
we have into a training set and a testing set. We can then train the
model on the training set and test it on the unseen testing set in order
to determine how well it has performed.

We tackle this in the next section.

    \hypertarget{cross-validation}{%
\subsubsection{Cross Validation}\label{cross-validation}}

    \emph{Cross Validation} is a technique to estimate how well a model will
perform on unseen data. As mentioned in the previous section, the entire
data set available can be divided into two: a training set and a testing
set. The question then becomes, `what portion of the dataset should be
the training set?'. This question can be expressed as follows:

\begin{itemize}
\tightlist
\item
  Let the number of observations be \(n\), then the trainnig set is
  \(n-k\) where \(k \in [1,n-1]\)
\end{itemize}

The reason this question is important is that the choice of \(k\)
greatly influences the bias in our cross validation. If
\(k = \lfloor n/2 \rfloor\) then the test error will be greatly
overestimated since the final model will be trained on \(n\)
observations, not \(\lfloor n/2 \rfloor\) observations. On the other
hand, if \(k=1\), the variance of our test error will be very large
since the technique will depend greatly on which observation we chose as
the test observation.

Going further, we can divide the entire dataset into roughly \(n/k\)
subsets. We can then run \(n/k\) different cross validations leaving a
different subset as the test set at each iteration. The test error (or
\(R^2\)) can then be approximated as the average of the different subset
test errors. This immediately means that if \(n\) is large, choosing to
assess the model performance using cross validation with \(k=1\) could
be computationally intense. Therefore, a value for \(k\) somewhere in
the range \((1,\lfloor n/2 \rfloor)\) may be wiser.

To start things off, let's fit a linear regression model to the house
prices dataset and test it on a portion of the data.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}58}]:} \PY{n}{housePrice}\PY{o}{.}\PY{n}{columns}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}58}]:} Index(['LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd',
                'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF',
                '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath',
                'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr',
                'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea',
                'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch',
                'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold', 'SalePrice'],
               dtype='object')
\end{Verbatim}
            
    We use train test split (using 33\% of the dataset as a test set) to
calculate the MSE on the test set

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}59}]:} \PY{c+c1}{\PYZsh{} The predictor and response}
         \PY{n}{X} \PY{o}{=} \PY{n}{housePrice}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{YearBuilt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{values}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}
         \PY{n}{y} \PY{o}{=} \PY{n}{housePrice}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SalePrice}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{values}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Make 33\PYZpc{} of this dataset a test set}
         \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.33}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{42}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Linear Regression model object}
         \PY{n}{model} \PY{o}{=} \PY{n}{LinearRegression}\PY{p}{(}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Fit this model using the training data}
         \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Predict}
         \PY{n}{predictions} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Get the RSS}
         \PY{n}{tss}\PY{p}{,}\PY{n}{rss} \PY{o}{=} \PY{n}{TSS\PYZus{}RSS}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,}\PY{n}{predictions}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} The MSE is RSS/n\PYZus{}test}
         \PY{n}{MSE} \PY{o}{=} \PY{n}{rss}\PY{o}{/}\PY{n+nb}{len}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{The MSE is }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{MSE}\PY{p}{)}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Plot the predictions}
         \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,}\PY{n}{y\PYZus{}test}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,}\PY{n}{predictions}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
The MSE is [5.29577792e+09]

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}59}]:} <matplotlib.collections.PathCollection at 0x1b551e20b70>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_115_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Let's see what the MSE is when we use the `LotArea' predictor to predict
`SalePrice'.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}60}]:} \PY{c+c1}{\PYZsh{} The predictor and response}
         \PY{n}{X} \PY{o}{=} \PY{n}{housePrice}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{LotArea}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{values}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}
         \PY{n}{y} \PY{o}{=} \PY{n}{housePrice}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SalePrice}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{values}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Make 33\PYZpc{} of this dataset a test set}
         \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.33}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{42}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Linear Regression model object}
         \PY{n}{model} \PY{o}{=} \PY{n}{LinearRegression}\PY{p}{(}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Fit this model using the training data}
         \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Predict}
         \PY{n}{predictions} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Get the RSS}
         \PY{n}{tss}\PY{p}{,}\PY{n}{rss} \PY{o}{=} \PY{n}{TSS\PYZus{}RSS}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,}\PY{n}{predictions}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} The MSE is RSS/n\PYZus{}test}
         \PY{n}{MSE} \PY{o}{=} \PY{n}{rss}\PY{o}{/}\PY{n+nb}{len}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{The MSE is }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{MSE}\PY{p}{)}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Plot the predictions}
         \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,}\PY{n}{y\PYZus{}test}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,}\PY{n}{predictions}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
The MSE is [6.89081973e+09]

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}60}]:} <matplotlib.collections.PathCollection at 0x1b551ea96a0>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_117_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    An important point to note in the above MSE calculations is that these
MSE results are highly biased. We used a train - test split of 33\%.
However, in reality, we have the full dataset to train our model on.
This means that the above is overestimating the test MSE of the model.
In other words, by using only a subset of our dataset to train our
model, we are not making use of the full power of the data we have. We
can go to the other extreme and select one single observation from our
data set of n observations as a test set and the remaining n-1
observations as a training set. This is called Leave One Out Cross
Validation (LOOCV). We do that below.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}61}]:} \PY{c+c1}{\PYZsh{} The predictor and response}
         \PY{n}{X} \PY{o}{=} \PY{n}{housePrice}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{LotArea}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
         \PY{n}{y} \PY{o}{=} \PY{n}{housePrice}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SalePrice}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
         
         \PY{c+c1}{\PYZsh{} Select a random element to be the test set}
         \PY{n}{r} \PY{o}{=} \PY{n}{random}\PY{o}{.}\PY{n}{SystemRandom}\PY{p}{(}\PY{p}{)}
         \PY{n}{testint} \PY{o}{=} \PY{n}{r}\PY{o}{.}\PY{n}{randint}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} The train set}
         \PY{n}{X\PYZus{}train} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{values}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}
         \PY{n}{y\PYZus{}train} \PY{o}{=} \PY{n}{y}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{values}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} The test set is that one observation}
         \PY{n}{X\PYZus{}test} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{p}{[}\PY{n}{testint}\PY{p}{]}
         \PY{n}{X\PYZus{}test} \PY{o}{=} \PY{n}{X\PYZus{}test}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} The train set is all observations except that one observation}
         \PY{n}{np}\PY{o}{.}\PY{n}{delete}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,}\PY{n}{testint}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} The test set consist of one response}
         \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{y\PYZus{}train}\PY{p}{[}\PY{n}{testint}\PY{p}{]}
         \PY{n}{np}\PY{o}{.}\PY{n}{delete}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{,}\PY{n}{testint}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} The Linear Regression model object}
         \PY{n}{model} \PY{o}{=} \PY{n}{LinearRegression}\PY{p}{(}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Fit the model}
         \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Predict}
         \PY{n}{predictions} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Get the MSE. MSE = RSS/n}
         \PY{n}{tss}\PY{p}{,}\PY{n}{rss} \PY{o}{=} \PY{n}{TSS\PYZus{}RSS}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,}\PY{n}{predictions}\PY{p}{)}
         \PY{n}{MSE} \PY{o}{=} \PY{n}{rss}\PY{o}{/}\PY{n+nb}{len}\PY{p}{(}\PY{n}{y}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{MSE = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{MSE}\PY{p}{)}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Plot}
         \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,}\PY{n}{y\PYZus{}test}\PY{p}{)}
         \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{,}\PY{n}{predictions}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
MSE = [782726.22053338]

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}61}]:} <matplotlib.collections.PathCollection at 0x1b552591860>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_119_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Each time we run the above code, we get a completely different test MSE.
This is because the test MSE depends on which observation we chose to
test the model on. So this reduces the bias to a minimum but has a large
variance. We can iterate over all the cases where for each iteration we
leave a different observation as a test observation. Then we calculate
the average MSE over all these Cross Validations.

    First for `YearBuilt' as a predictor variable then the `LotArea' as a
predictor variable.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}62}]:} \PY{c+c1}{\PYZsh{} The predictor and response}
         \PY{n}{X} \PY{o}{=} \PY{n}{housePrice}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{YearBuilt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
         \PY{n}{y} \PY{o}{=} \PY{n}{housePrice}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SalePrice}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
         
         \PY{c+c1}{\PYZsh{} The MSE array. Each element is the MSE of a particular Cross Validation}
         \PY{n}{MSE} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         
         \PY{c+c1}{\PYZsh{} Perform LOOCV on the data using Linear Regression}
         \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{)}\PY{p}{:}
             \PY{c+c1}{\PYZsh{} The training set}
             \PY{n}{X\PYZus{}train} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{values}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}
             \PY{n}{y\PYZus{}train} \PY{o}{=} \PY{n}{y}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{values}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{} The test set is a single observation}
             \PY{n}{X\PYZus{}test} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{p}{[}\PY{n}{i}\PY{p}{]}
             \PY{n}{X\PYZus{}test} \PY{o}{=} \PY{n}{X\PYZus{}test}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
             \PY{n}{X\PYZus{}train} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{delete}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,}\PY{n}{i}\PY{p}{)}
         
             \PY{c+c1}{\PYZsh{} The test set is a single observation}
             \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{y\PYZus{}train}\PY{p}{[}\PY{n}{i}\PY{p}{]}
             \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{y\PYZus{}test}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
             \PY{n}{y\PYZus{}train} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{delete}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{,}\PY{n}{i}\PY{p}{)}
         
             \PY{c+c1}{\PYZsh{} Train the model}
             \PY{n}{model} \PY{o}{=} \PY{n}{LinearRegression}\PY{p}{(}\PY{p}{)}
         
             \PY{c+c1}{\PYZsh{} Fit}
             \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
         
             \PY{c+c1}{\PYZsh{} Predict}
             \PY{n}{predictions} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
         
             \PY{c+c1}{\PYZsh{} Calculate the MSE. MSE = RSS/n\PYZus{}test}
             \PY{n}{tss}\PY{p}{,}\PY{n}{rss} \PY{o}{=} \PY{n}{TSS\PYZus{}RSS}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,}\PY{n}{predictions}\PY{p}{)}
             \PY{n}{MSE}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{rss}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Print the mean MSE value}
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{MSE}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
4597328547.297892

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}63}]:} \PY{n}{X} \PY{o}{=} \PY{n}{housePrice}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{LotArea}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
         \PY{n}{y} \PY{o}{=} \PY{n}{housePrice}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SalePrice}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
         
         \PY{n}{MSE} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         
         \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{)}\PY{p}{:}
             \PY{n}{X\PYZus{}train} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{values}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}
             \PY{n}{y\PYZus{}train} \PY{o}{=} \PY{n}{y}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{values}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}
             
             \PY{n}{X\PYZus{}test} \PY{o}{=} \PY{n}{X\PYZus{}train}\PY{p}{[}\PY{n}{i}\PY{p}{]}
             \PY{n}{X\PYZus{}test} \PY{o}{=} \PY{n}{X\PYZus{}test}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
             \PY{n}{X\PYZus{}train} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{delete}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,}\PY{n}{i}\PY{p}{)}
         
             \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{y\PYZus{}train}\PY{p}{[}\PY{n}{i}\PY{p}{]}
             \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{y\PYZus{}test}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
             \PY{n}{y\PYZus{}train} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{delete}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{,}\PY{n}{i}\PY{p}{)}
         
             \PY{c+c1}{\PYZsh{} Train}
             \PY{n}{model} \PY{o}{=} \PY{n}{LinearRegression}\PY{p}{(}\PY{p}{)}
         
             \PY{c+c1}{\PYZsh{} Fit}
             \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
         
             \PY{c+c1}{\PYZsh{} Predict}
             \PY{n}{predictions} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
         
             \PY{c+c1}{\PYZsh{} MSE}
             \PY{n}{tss}\PY{p}{,}\PY{n}{rss} \PY{o}{=} \PY{n}{TSS\PYZus{}RSS}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,}\PY{n}{predictions}\PY{p}{)}
             \PY{n}{MSE}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{rss}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
         
         \PY{n+nb}{print}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{MSE}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
5954196196.345753

    \end{Verbatim}

    We can leverage the cross\_val\_score method to do the above cross
validation for us

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}64}]:} \PY{n}{X} \PY{o}{=} \PY{n}{housePrice}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{LotArea}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
         \PY{n}{y} \PY{o}{=} \PY{n}{housePrice}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SalePrice}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
         
         \PY{n}{linregCVScores} \PY{o}{=} \PY{n}{cross\PYZus{}val\PYZus{}score}\PY{p}{(}\PY{n}{LinearRegression}\PY{p}{(}\PY{p}{)}\PY{p}{,}\PY{n}{X}\PY{o}{.}\PY{n}{values}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}\PY{n}{y}\PY{o}{.}\PY{n}{values}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}\PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{neg\PYZus{}mean\PYZus{}squared\PYZus{}error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{cv}\PY{o}{=}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{)}
         \PY{o}{\PYZhy{}}\PY{n}{linregCVScores}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}64}]:} 5954196196.345753
\end{Verbatim}
            
    Let's observe now which approach (value of k in k-fold cross validation)
predicts the test MSE best. We split our train and test data. Then
estimate the test MSE using the training data.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}65}]:} \PY{n}{datasetMSEEstimatek\PYZus{}10} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{n}{datasetMSEEstimatek\PYZus{}20} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{n}{datasetMSEEstimatek\PYZus{}100} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{n}{datasetMSEActual} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         
         \PY{c+c1}{\PYZsh{} The predictor and response}
         \PY{n}{X} \PY{o}{=} \PY{n}{housePrice}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{LotArea}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{values}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}
         \PY{n}{y} \PY{o}{=} \PY{n}{housePrice}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SalePrice}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{values}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}
         
         
         \PY{k}{for} \PY{n}{j} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{500}\PY{p}{)}\PY{p}{:}
             \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{n}{j}\PY{p}{)}
         
             \PY{n}{linregCVScores} \PY{o}{=} \PY{n}{cross\PYZus{}val\PYZus{}score}\PY{p}{(}\PY{n}{LinearRegression}\PY{p}{(}\PY{p}{)}\PY{p}{,}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}\PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{neg\PYZus{}mean\PYZus{}squared\PYZus{}error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}
             \PY{n}{datasetMSEEstimatek\PYZus{}10}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{n}{linregCVScores}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{p}{)}
             
             \PY{n}{linregCVScores} \PY{o}{=} \PY{n}{cross\PYZus{}val\PYZus{}score}\PY{p}{(}\PY{n}{LinearRegression}\PY{p}{(}\PY{p}{)}\PY{p}{,}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}\PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{neg\PYZus{}mean\PYZus{}squared\PYZus{}error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{)}
             \PY{n}{datasetMSEEstimatek\PYZus{}20}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{n}{linregCVScores}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{p}{)}
             
             \PY{n}{linregCVScores} \PY{o}{=} \PY{n}{cross\PYZus{}val\PYZus{}score}\PY{p}{(}\PY{n}{LinearRegression}\PY{p}{(}\PY{p}{)}\PY{p}{,}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}\PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{neg\PYZus{}mean\PYZus{}squared\PYZus{}error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{)}
             \PY{n}{datasetMSEEstimatek\PYZus{}100}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{n}{linregCVScores}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{p}{)}
             
             
             \PY{n}{model} \PY{o}{=} \PY{n}{LinearRegression}\PY{p}{(}\PY{p}{)}
             \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{)}
             \PY{n}{predictions} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
             \PY{n}{datasetMSEActual}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{mean\PYZus{}squared\PYZus{}error}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,}\PY{n}{predictions}\PY{p}{)}\PY{p}{)}
             
             \PY{k}{if} \PY{n}{j}\PY{o}{\PYZpc{}}\PY{k}{50} == 0:
                 \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Step = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{j}\PY{p}{)}\PY{p}{)}
             
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{The mean MSE Estimation using K\PYZhy{}fold CV with k = 10 is : }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{datasetMSEEstimatek\PYZus{}10}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{The mean MSE Estimation using K\PYZhy{}fold CV with k = 20 is : }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{datasetMSEEstimatek\PYZus{}20}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{The mean MSE Estimation using K\PYZhy{}fold CV with k = 100 is : }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{datasetMSEEstimatek\PYZus{}100}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{The actual MSE on this data set is : }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{datasetMSEActual}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{fig}\PY{p}{,}\PY{n}{axes} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{n}{nrows} \PY{o}{=} \PY{l+m+mi}{2}\PY{p}{,}\PY{n}{ncols} \PY{o}{=} \PY{l+m+mi}{2}\PY{p}{,}\PY{n}{sharex}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
         \PY{n}{fig}\PY{o}{.}\PY{n}{set\PYZus{}size\PYZus{}inches}\PY{p}{(}\PY{l+m+mi}{20}\PY{p}{,}\PY{l+m+mi}{10}\PY{p}{)}
         
         \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{hist}\PY{p}{(}\PY{n+nb}{list}\PY{p}{(}\PY{n+nb}{map}\PY{p}{(}\PY{n}{math}\PY{o}{.}\PY{n}{log}\PY{p}{,}\PY{n}{datasetMSEEstimatek\PYZus{}10}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{CV with k = 10}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{hist}\PY{p}{(}\PY{n+nb}{list}\PY{p}{(}\PY{n+nb}{map}\PY{p}{(}\PY{n}{math}\PY{o}{.}\PY{n}{log}\PY{p}{,}\PY{n}{datasetMSEEstimatek\PYZus{}20}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{CV with k = 20}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{hist}\PY{p}{(}\PY{n+nb}{list}\PY{p}{(}\PY{n+nb}{map}\PY{p}{(}\PY{n}{math}\PY{o}{.}\PY{n}{log}\PY{p}{,}\PY{n}{datasetMSEEstimatek\PYZus{}100}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{CV with k = 100}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{hist}\PY{p}{(}\PY{n+nb}{list}\PY{p}{(}\PY{n+nb}{map}\PY{p}{(}\PY{n}{math}\PY{o}{.}\PY{n}{log}\PY{p}{,}\PY{n}{datasetMSEActual}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Averaged Actual MSE over different train/test splits}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Step = 0
Step = 50
Step = 100
Step = 150
Step = 200
Step = 250
Step = 300
Step = 350
Step = 400
Step = 450
The mean MSE Estimation using K-fold CV with k = 10 is : 6170090398.833329
The mean MSE Estimation using K-fold CV with k = 20 is : 6161875851.874799
The mean MSE Estimation using K-fold CV with k = 100 is : 6146620803.335432
The actual MSE on this data set is : 6046571007.118181

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}65}]:} Text(0.5,1,'Averaged Actual MSE over different train/test splits')
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_127_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}66}]:} \PY{n}{datasetMSEEstimatek\PYZus{}10} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{n}{datasetMSEEstimatek\PYZus{}20} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{n}{datasetMSEEstimatek\PYZus{}100} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{n}{datasetMSEActual} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         
         \PY{c+c1}{\PYZsh{} The predictor and response}
         \PY{n}{X} \PY{o}{=} \PY{n}{housePrice}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{YearBuilt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{values}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}
         \PY{n}{y} \PY{o}{=} \PY{n}{housePrice}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SalePrice}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{values}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}
         
         
         \PY{k}{for} \PY{n}{j} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{500}\PY{p}{)}\PY{p}{:}
             \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{n}{j}\PY{p}{)}
         
             \PY{n}{linregCVScores} \PY{o}{=} \PY{n}{cross\PYZus{}val\PYZus{}score}\PY{p}{(}\PY{n}{LinearRegression}\PY{p}{(}\PY{p}{)}\PY{p}{,}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}\PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{neg\PYZus{}mean\PYZus{}squared\PYZus{}error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}
             \PY{n}{datasetMSEEstimatek\PYZus{}10}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{n}{linregCVScores}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{p}{)}
             
             \PY{n}{linregCVScores} \PY{o}{=} \PY{n}{cross\PYZus{}val\PYZus{}score}\PY{p}{(}\PY{n}{LinearRegression}\PY{p}{(}\PY{p}{)}\PY{p}{,}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}\PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{neg\PYZus{}mean\PYZus{}squared\PYZus{}error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{)}
             \PY{n}{datasetMSEEstimatek\PYZus{}20}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{n}{linregCVScores}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{p}{)}
             
             \PY{n}{linregCVScores} \PY{o}{=} \PY{n}{cross\PYZus{}val\PYZus{}score}\PY{p}{(}\PY{n}{LinearRegression}\PY{p}{(}\PY{p}{)}\PY{p}{,}\PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}\PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{neg\PYZus{}mean\PYZus{}squared\PYZus{}error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{100}\PY{p}{)}
             \PY{n}{datasetMSEEstimatek\PYZus{}100}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{n}{linregCVScores}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{p}{)}
             
             
             \PY{n}{model} \PY{o}{=} \PY{n}{LinearRegression}\PY{p}{(}\PY{p}{)}
             \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{)}
             \PY{n}{predictions} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)}
             \PY{n}{datasetMSEActual}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{mean\PYZus{}squared\PYZus{}error}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{,}\PY{n}{predictions}\PY{p}{)}\PY{p}{)}
             
             \PY{k}{if} \PY{n}{j}\PY{o}{\PYZpc{}}\PY{k}{50} == 0:
                 \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Step = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{j}\PY{p}{)}\PY{p}{)}
             
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{The mean MSE Estimation using K\PYZhy{}fold CV with k = 10 is : }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{datasetMSEEstimatek\PYZus{}10}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{The mean MSE Estimation using K\PYZhy{}fold CV with k = 20 is : }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{datasetMSEEstimatek\PYZus{}20}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{The mean MSE Estimation using K\PYZhy{}fold CV with k = 100 is : }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{datasetMSEEstimatek\PYZus{}100}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{The actual MSE on this data set is : }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{datasetMSEActual}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         
         \PY{n}{fig}\PY{p}{,}\PY{n}{axes} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{n}{nrows} \PY{o}{=} \PY{l+m+mi}{2}\PY{p}{,}\PY{n}{ncols} \PY{o}{=} \PY{l+m+mi}{2}\PY{p}{,}\PY{n}{sharex}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
         \PY{n}{fig}\PY{o}{.}\PY{n}{set\PYZus{}size\PYZus{}inches}\PY{p}{(}\PY{l+m+mi}{20}\PY{p}{,}\PY{l+m+mi}{10}\PY{p}{)}
         
         \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{hist}\PY{p}{(}\PY{n+nb}{list}\PY{p}{(}\PY{n+nb}{map}\PY{p}{(}\PY{n}{math}\PY{o}{.}\PY{n}{log}\PY{p}{,}\PY{n}{datasetMSEEstimatek\PYZus{}10}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{CV with k = 10}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{hist}\PY{p}{(}\PY{n+nb}{list}\PY{p}{(}\PY{n+nb}{map}\PY{p}{(}\PY{n}{math}\PY{o}{.}\PY{n}{log}\PY{p}{,}\PY{n}{datasetMSEEstimatek\PYZus{}20}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{CV with k = 20}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{hist}\PY{p}{(}\PY{n+nb}{list}\PY{p}{(}\PY{n+nb}{map}\PY{p}{(}\PY{n}{math}\PY{o}{.}\PY{n}{log}\PY{p}{,}\PY{n}{datasetMSEEstimatek\PYZus{}100}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{CV with k = 100}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         
         \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{hist}\PY{p}{(}\PY{n+nb}{list}\PY{p}{(}\PY{n+nb}{map}\PY{p}{(}\PY{n}{math}\PY{o}{.}\PY{n}{log}\PY{p}{,}\PY{n}{datasetMSEActual}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Averaged Actual MSE over different train/test splits}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Step = 0
Step = 50
Step = 100
Step = 150
Step = 200
Step = 250
Step = 300
Step = 350
Step = 400
Step = 450
The mean MSE Estimation using K-fold CV with k = 10 is : 4622083078.925203
The mean MSE Estimation using K-fold CV with k = 20 is : 4621267854.562423
The mean MSE Estimation using K-fold CV with k = 100 is : 4620965999.249676
The actual MSE on this data set is : 4587675012.9899845

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}66}]:} Text(0.5,1,'Averaged Actual MSE over different train/test splits')
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_128_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    We can see above that the MSE when we choose the `LotArea' predictor is
not as good as using `YearBuilt'. So in this case we choose `YearBuilt'
over `LotArea' to include in our linear regression model.

It can be seen that there is a general pattern in the above when
comparing the MSE estimates from varying blocks (k) in Cross Validation.
Namely, the larger k is, the more blocks we use to split the data up and
the less portion of the data there is for the test set and the more
iteration that is required per Cross Validation. For example, suppose n
= 1000, when k = 10 we have 10 blocks with 100 observations per block.
So we fit the model 10 times, each time leaving out a different k block
when training. When comparing this with the case where k = 100, we have
100 blocks each with 10 observations. This means that the training set
for each of these model fits is a lot closer to reality in that we will
be using the entire dataset to fit the model. However, this comes at a
computational cost, in this case we would need to run a model to fit and
predict 100 times instead of 10.

Depending on the computational cost of the model used, we may choose a
smaller value of k = 10 when comparing the same model but with different
parameters.

We can try out all the predictors and choose the one that minimises the
mean squared errors. This can be done by using a loop as below.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}67}]:} \PY{c+c1}{\PYZsh{} Run through each model in the correct order and run CV on it and save the best CV score}
         \PY{n}{bestMeanCV} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}
         \PY{n}{bestMeanCVModel} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         
         \PY{n}{X} \PY{o}{=} \PY{n}{housePrice}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SalePrice}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} y is the response variable}
         \PY{n}{y} \PY{o}{=} \PY{n}{housePrice}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SalePrice}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
         
         \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{X}\PY{o}{.}\PY{n}{columns}\PY{p}{:}
             \PY{c+c1}{\PYZsh{} First set X to be the full set of remaining parameters}
             \PY{n}{X} \PY{o}{=} \PY{n}{housePrice}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{n}{i}\PY{p}{]}
             
             \PY{n}{linregCVScores} \PY{o}{=} \PY{n}{cross\PYZus{}val\PYZus{}score}\PY{p}{(}\PY{n}{LinearRegression}\PY{p}{(}\PY{p}{)}\PY{p}{,}\PY{n}{X}\PY{o}{.}\PY{n}{values}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}\PY{n}{y}\PY{p}{,}\PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{neg\PYZus{}mean\PYZus{}squared\PYZus{}error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}
             
             \PY{k}{if} \PY{n}{bestMeanCV} \PY{o}{\PYZgt{}} \PY{o}{\PYZhy{}}\PY{n}{linregCVScores}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{p}{:}
                 \PY{n}{bestMeanCV} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{n}{linregCVScores}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}
                 \PY{n}{bestMeanCVModel} \PY{o}{=} \PY{n}{i}
             \PY{k}{elif} \PY{n}{bestMeanCV} \PY{o}{==} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{:}
                 \PY{n}{bestMeanCV} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{n}{linregCVScores}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}
                 \PY{n}{bestMeanCVModel} \PY{o}{=} \PY{n}{i}
                 
         \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{The final best model is }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{ and its TEST MSE is }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{bestMeanCVModel}\PY{p}{,}\PY{n}{bestMeanCV}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
The final best model is OverallQual and its TEST MSE is 2371260934.069026

    \end{Verbatim}

    We can then iterate through the predictors adding it to the model each
time in order to improve the test MSE of the model. For instance, in the
above, we have selected as the first predictor in our model, the
predictor `OverallQual'. Next, we cycle through all the remaining
predictors to include in our model along with `OverallQual' and repeat.
The final result will be a list of all predictors in the order they were
added. Once we get to a point where adding another predictor to the
model does not improve the test MSE, then we stop there.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}112}]:} \PY{c+c1}{\PYZsh{} Run through each model in the correct order and run CV on it and save the best CV score}
          \PY{n}{bestMeanCV} \PY{o}{=} \PY{k+kc}{None}
          \PY{n}{bestMeanCVModel} \PY{o}{=} \PY{p}{[}\PY{p}{]}
          \PY{n}{oldArraySize} \PY{o}{=} \PY{l+m+mi}{0}
          
          \PY{n}{X} \PY{o}{=} \PY{n}{housePrice}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SalePrice}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
          
          \PY{n}{columnsArray} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{columns}
          
          \PY{c+c1}{\PYZsh{} y is the response variable}
          \PY{n}{y} \PY{o}{=} \PY{n}{housePrice}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SalePrice}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
          
          \PY{k}{while} \PY{n}{oldArraySize} \PY{o}{!=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{:}
              \PY{n}{bestPredictor} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsq{}}
              \PY{n}{oldArraySize} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{columns}\PY{p}{)}
              \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{columnsArray}\PY{p}{:}
                  \PY{n}{thisModel} \PY{o}{=} \PY{n}{bestMeanCVModel}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}
                  \PY{n}{thisModel}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{i}\PY{p}{)}
                  \PY{c+c1}{\PYZsh{} First set X to be the full set of remaining parameters}
                  \PY{n}{x} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{n}{thisModel}\PY{p}{]}
          
                  \PY{k}{if} \PY{n+nb}{len}\PY{p}{(}\PY{n}{x}\PY{o}{.}\PY{n}{columns}\PY{p}{)} \PY{o}{==} \PY{l+m+mi}{1}\PY{p}{:}
                      \PY{n}{linregCVScores} \PY{o}{=} \PY{n}{cross\PYZus{}val\PYZus{}score}\PY{p}{(}\PY{n}{LinearRegression}\PY{p}{(}\PY{p}{)}\PY{p}{,}\PY{n}{x}\PY{o}{.}\PY{n}{values}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}\PY{n}{y}\PY{p}{,}\PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{neg\PYZus{}mean\PYZus{}squared\PYZus{}error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}
                  \PY{k}{else}\PY{p}{:}
                      \PY{n}{linregCVScores} \PY{o}{=} \PY{n}{cross\PYZus{}val\PYZus{}score}\PY{p}{(}\PY{n}{LinearRegression}\PY{p}{(}\PY{p}{)}\PY{p}{,}\PY{n}{x}\PY{p}{,}\PY{n}{y}\PY{p}{,}\PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{neg\PYZus{}mean\PYZus{}squared\PYZus{}error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}
                    
                  \PY{k}{if} \PY{o+ow}{not} \PY{n}{bestMeanCV}\PY{p}{:}
                      \PY{n}{bestMeanCV} \PY{o}{=} \PY{n}{linregCVScores}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}
                      \PY{n}{bestPredictor} \PY{o}{=} \PY{n}{i}
                  \PY{k}{elif} \PY{n}{bestMeanCV} \PY{o}{\PYZlt{}} \PY{n}{linregCVScores}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{p}{:}
                      \PY{n}{bestMeanCV} \PY{o}{=} \PY{n}{linregCVScores}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}
                      \PY{n}{bestPredictor} \PY{o}{=} \PY{n}{i}
              
              \PY{k}{if} \PY{n}{bestPredictor} \PY{o+ow}{not} \PY{o+ow}{in} \PY{n}{columnsArray}\PY{p}{:}
                  \PY{k}{break}
                  
              \PY{n}{columnsArray}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{bestPredictor}\PY{p}{)}
              \PY{n}{bestMeanCVModel}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{bestPredictor}\PY{p}{)}
              \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{ was added with test MSE }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{bestMeanCVModel}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{n}{bestMeanCV}\PY{p}{)}\PY{p}{)}
          
                  
          \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{The final best model is }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{ and its TEST MSE is }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{bestMeanCVModel}\PY{p}{,}\PY{n}{bestMeanCV}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
OverallQual was added with test MSE -2371260934.069026
GrLivArea was added with test MSE -1821343747.2253425
BsmtFinSF1 was added with test MSE -1653396814.3900447
GarageCars was added with test MSE -1522575852.80887
YearRemodAdd was added with test MSE -1477506784.3227758
LotArea was added with test MSE -1445259871.69055
MasVnrArea was added with test MSE -1418244120.600768
KitchenAbvGr was added with test MSE -1399446462.6115127
1stFlrSF was added with test MSE -1376812086.0548759
YearBuilt was added with test MSE -1366762325.3833966
OverallCond was added with test MSE -1352021476.9079351
ScreenPorch was added with test MSE -1346347855.653913
WoodDeckSF was added with test MSE -1339278365.3061535
TotRmsAbvGrd was added with test MSE -1334799185.148554
BedroomAbvGr was added with test MSE -1315922220.78058
EnclosedPorch was added with test MSE -1315305925.9789593
PoolArea was added with test MSE -1314438332.6507847
GrLivArea was added with test MSE -1314438332.6507568
LotArea was added with test MSE -1314414828.8009648
OverallQual was added with test MSE -1313300135.6850371
WoodDeckSF was added with test MSE -1313028916.9084868
The final best model is ['OverallQual', 'GrLivArea', 'BsmtFinSF1', 'GarageCars', 'YearRemodAdd', 'LotArea', 'MasVnrArea', 'KitchenAbvGr', '1stFlrSF', 'YearBuilt', 'OverallCond', 'ScreenPorch', 'WoodDeckSF', 'TotRmsAbvGrd', 'BedroomAbvGr', 'EnclosedPorch', 'PoolArea', 'GrLivArea', 'LotArea', 'OverallQual', 'WoodDeckSF'] and its TEST MSE is -1313028916.9084868

    \end{Verbatim}

    Our final model is now contained in bestMeanCVModel.

    \hypertarget{ridge-regression}{%
\subsubsection{Ridge Regression}\label{ridge-regression}}

Ridge Regression adds a twist to Linear Regression with the aim of
reducing the variance of the model and managing multicollinearity.

We begin with the normal equation as we did for Linear Regression and
arrive at a method of calculating the parameters of the regression
formula.

    As before, we pose a hypothesis (\(h_{\theta}(X)\)) and a cost function
(\(J(\theta)\)) and proceed to minimise this cost function. Here, \(X\)
is the data and \(\theta\) is a vector of parameters (such as the
\(\beta\) in the Linear Regression models above).

For Linear Regression as stated above, the hypothesis function is that
there is a straight line passing through all the data points:

\[h_{\theta}(X) = \theta_0 + \theta_1 X_1 + \theta_2 X_2 + \theta_3 X_3 + ... = X \theta\]

The Cost function is the least squares sum residuals (eventually written
in index notation):

\[J(\theta) = \sum_{i=1}^n e_i^2 = \sum_{i=1}^n (h_{\theta}(X^{(i)}) - Y^{(i)})^2 = (X \theta - Y)^T (X \theta - Y) = (X \theta)^T X\theta - 2 (X \theta)^T Y + Y^T Y = \theta_j x_{ji} x_{ij} \theta_{j} - 2 \theta_j x_{ji} y_i\]

where the superscript \(^{(i)}\) refers to the ith observation. The
extra step we will be taking here is to add an additional term in this
equation which is the L2 norm
\(\lambda||\theta||^2 = \lambda \sum_{i=1}^n \theta_i^2 = \lambda \theta'^T \theta' = \lambda \theta'_j \theta'_j\),
where \(\theta'\) is the parameter vector \(\theta\) but with the first
term corresponding to the coefficient of the constant term set to zero
and \(\lambda\) is a scaling factor or shrinkage factor:

\[J(\theta) = \theta_j x_{ji} x_{ij} \theta_{j} - 2 \theta_j x_{ji} y_i + \lambda \theta'_j \theta'_j\]

Taking the derivative of the cost function:

\[\frac{\partial J(\theta)}{\partial \theta_k} = 2 x_{ki} x_{ik} \theta_k - 2 x_{ki} y_i + 2\lambda \theta'_k\]

where \(\theta'_k = 0\) for \(k=0\).

Setting this to zero for all \(k\) and solving:

\[\theta = (X^T X + \lambda I')^{-1} X^T Y\]

where we have used the fact that \(\theta' I = \theta I'\) with \(I\)
the identity matrix and \(I'\) the identity matrix where element
\(I_{11} = 0\) (i.e.~we are transfering effect of the first element of
\(\theta\) being zero to the identity matrix).

In summary, this additional regularisation term serves to attach a
penalty to large coefficients in the minimisation process. It should be
added that if we set \(\lambda = 0\), no additional constraint is
performed and we just get the Linear Regression solution.

Let's test this out the sklearn module:

    \(\lambda||\theta||^2 = \lambda \sum_{i=1}^n \theta_i^2 = \lambda \theta'^T \theta' = \lambda \theta'_j \theta'_j\)

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}115}]:} \PY{n}{L} \PY{o}{=} \PY{l+m+mf}{0.2}
          \PY{n}{normalize} \PY{o}{=} \PY{k+kc}{False}
          
          \PY{c+c1}{\PYZsh{} X is the predictor variable }
          \PY{n}{X} \PY{o}{=} \PY{n}{housePrice}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SalePrice}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{[}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{LotArea}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{YearBuilt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{]}
          
          \PY{c+c1}{\PYZsh{} y is the response variable}
          \PY{n}{y} \PY{o}{=} \PY{n}{housePrice}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SalePrice}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
          
          \PY{c+c1}{\PYZsh{} Convert the predictor dataframe and the response dataframe to arrays to be consistent with data type}
          \PY{n}{X} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{concatenate}\PY{p}{(}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{ones}\PY{p}{(}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{,}\PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{)}\PY{p}{,}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
          \PY{n}{y} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{y}\PY{p}{)}
          
          \PY{n}{ncols} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
          
          \PY{k}{if} \PY{n}{normalize}\PY{p}{:}
              \PY{c+c1}{\PYZsh{} standardise X if required}
              \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{n}{ncols}\PY{p}{)}\PY{p}{:}
                  \PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{n}{i}\PY{p}{]} \PY{o}{=} \PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{n}{i}\PY{p}{]} \PY{o}{\PYZhy{}} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{n}{i}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{o}{/}\PY{n}{np}\PY{o}{.}\PY{n}{std}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{n}{i}\PY{p}{]}\PY{p}{)}
          
              \PY{c+c1}{\PYZsh{} standardise y if required}
              \PY{n}{y} \PY{o}{=} \PY{p}{(}\PY{n}{y} \PY{o}{\PYZhy{}} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{y}\PY{p}{)}\PY{p}{)}\PY{o}{/}\PY{n}{np}\PY{o}{.}\PY{n}{std}\PY{p}{(}\PY{n}{y}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Create transpose (3 X n)}
          \PY{n}{X\PYZus{}T} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{transpose}\PY{p}{(}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Calculate X\PYZca{}T X (3 X 3)}
          \PY{n}{XTX} \PY{o}{=} \PY{n}{X\PYZus{}T}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{X}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Create I\PYZsq{}}
          \PY{n}{Id} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{eye}\PY{p}{(}\PY{n}{XTX}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
          \PY{n}{Id}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{0}
          
          \PY{c+c1}{\PYZsh{} Add the shrinkage factor part}
          \PY{n}{XTX} \PY{o}{=} \PY{n}{XTX} \PY{o}{+} \PY{n}{L}\PY{o}{*}\PY{n}{Id}
          
          \PY{c+c1}{\PYZsh{} calculate inverse of XTX + lambda I\PYZsq{} (3 X 3)}
          \PY{n}{XTX\PYZus{}inv} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{inv}\PY{p}{(}\PY{n}{XTX}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Calculate theta}
          \PY{n}{theta} \PY{o}{=} \PY{n}{XTX\PYZus{}inv}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{X\PYZus{}T}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{y}\PY{p}{)}\PY{p}{)}
          
          \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Y\PYZus{}3 = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{ + }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{ * LotArea + }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{ * YearBuilt}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{theta}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PY{n}{theta}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{n}{theta}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Y\_3 = -2532855.620003462 + 2.041172157436325 * LotArea + 1365.7759478454827 * YearBuilt

    \end{Verbatim}

    We can run this using the package and we see the exact same results

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}116}]:} \PY{c+c1}{\PYZsh{} X is the predictor variable }
          \PY{n}{X} \PY{o}{=} \PY{n}{housePrice}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SalePrice}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{[}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{LotArea}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{YearBuilt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{]}
          
          \PY{c+c1}{\PYZsh{} y is the response variable}
          \PY{n}{y} \PY{o}{=} \PY{n}{housePrice}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SalePrice}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
          
          \PY{n}{ridgeModel} \PY{o}{=} \PY{n}{Ridge}\PY{p}{(}\PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{,} \PY{n}{normalize}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
          
          \PY{n}{ridgeModel}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X}\PY{p}{,}\PY{n}{y}\PY{p}{)}
          
          \PY{n}{ridgeModel}\PY{o}{.}\PY{n}{intercept\PYZus{}}\PY{p}{,}\PY{n}{ridgeModel}\PY{o}{.}\PY{n}{coef\PYZus{}}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}116}]:} (-2532855.620003232, array([   2.04117216, 1365.77594785]))
\end{Verbatim}
            
    Let's see how different this performs compared to the usual Linear
Regression model. We would like to test if we can reduce the MSE on
unseen test data if we use a non-zero shrinkage factor. This will
demonstrate that the solution to the Ridge Regression line is less
effected by a change in the data.

First Linear Regression

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}118}]:} \PY{c+c1}{\PYZsh{} Run through each model in the correct order and run CV on it and save the best CV score}
          \PY{n}{bestMeanCV} \PY{o}{=} \PY{k+kc}{None}
          \PY{n}{bestMeanCVModel} \PY{o}{=} \PY{p}{[}\PY{p}{]}
          \PY{n}{oldArraySize} \PY{o}{=} \PY{l+m+mi}{0}
          
          \PY{n}{X} \PY{o}{=} \PY{n}{housePrice}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SalePrice}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
          
          \PY{n}{columnsArray} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{columns}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} y is the response variable}
          \PY{n}{y} \PY{o}{=} \PY{n}{housePrice}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SalePrice}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
          
          \PY{k}{while} \PY{n}{oldArraySize} \PY{o}{!=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{:}
              \PY{n}{bestPredictor} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsq{}}
              \PY{n}{oldArraySize} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{columns}\PY{p}{)}
              \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{columnsArray}\PY{p}{:}
                  \PY{n}{thisModel} \PY{o}{=} \PY{n}{bestMeanCVModel}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}
                  \PY{n}{thisModel}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{i}\PY{p}{)}
                  \PY{c+c1}{\PYZsh{} First set X to be the full set of remaining parameters}
                  \PY{n}{x} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{n}{thisModel}\PY{p}{]}
          
                  \PY{k}{if} \PY{n+nb}{len}\PY{p}{(}\PY{n}{x}\PY{o}{.}\PY{n}{columns}\PY{p}{)} \PY{o}{==} \PY{l+m+mi}{1}\PY{p}{:}
                      \PY{n}{linregCVScores} \PY{o}{=} \PY{n}{cross\PYZus{}val\PYZus{}score}\PY{p}{(}\PY{n}{Ridge}\PY{p}{(}\PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.0001}\PY{p}{)}\PY{p}{,}\PY{n}{x}\PY{o}{.}\PY{n}{values}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}\PY{n}{y}\PY{p}{,}\PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{neg\PYZus{}mean\PYZus{}squared\PYZus{}error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}
                  \PY{k}{else}\PY{p}{:}
                      \PY{n}{linregCVScores} \PY{o}{=} \PY{n}{cross\PYZus{}val\PYZus{}score}\PY{p}{(}\PY{n}{Ridge}\PY{p}{(}\PY{n}{alpha}\PY{o}{=}\PY{l+m+mf}{0.0001}\PY{p}{)}\PY{p}{,}\PY{n}{x}\PY{p}{,}\PY{n}{y}\PY{p}{,}\PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{neg\PYZus{}mean\PYZus{}squared\PYZus{}error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}
                    
                  \PY{k}{if} \PY{o+ow}{not} \PY{n}{bestMeanCV}\PY{p}{:}
                      \PY{n}{bestMeanCV} \PY{o}{=} \PY{n}{linregCVScores}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}
                      \PY{n}{bestPredictor} \PY{o}{=} \PY{n}{i}
                  \PY{k}{elif} \PY{n}{bestMeanCV} \PY{o}{\PYZlt{}} \PY{n}{linregCVScores}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{p}{:}
                      \PY{n}{bestMeanCV} \PY{o}{=} \PY{n}{linregCVScores}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}
                      \PY{n}{bestPredictor} \PY{o}{=} \PY{n}{i}
              
              \PY{k}{if} \PY{n}{bestPredictor} \PY{o+ow}{not} \PY{o+ow}{in} \PY{n}{columnsArray}\PY{p}{:}
                  \PY{k}{break}
              
              \PY{n}{columnsArray} \PY{o}{=} \PY{n}{columnsArray}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{bestPredictor}\PY{p}{)}
              \PY{n}{bestMeanCVModel}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{bestPredictor}\PY{p}{)}
              \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{ was added with test MSE }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{bestMeanCVModel}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{n}{bestMeanCV}\PY{p}{)}\PY{p}{)}
          
                  
          \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{The final best model is }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{ and its TEST MSE is }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{bestMeanCVModel}\PY{p}{,}\PY{n}{bestMeanCV}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
OverallQual was added with test MSE -2371260933.576952
GrLivArea was added with test MSE -1821343747.2524974
BsmtFinSF1 was added with test MSE -1653396815.643581
GarageCars was added with test MSE -1522575854.1067336
YearRemodAdd was added with test MSE -1477506784.5675159
LotArea was added with test MSE -1445259871.8107793
MasVnrArea was added with test MSE -1418244120.7510715
KitchenAbvGr was added with test MSE -1399446459.9374409
1stFlrSF was added with test MSE -1376812080.8984106
YearBuilt was added with test MSE -1366762319.9338892
OverallCond was added with test MSE -1352021471.1023602
ScreenPorch was added with test MSE -1346347849.9059243
WoodDeckSF was added with test MSE -1339278358.4927483
TotRmsAbvGrd was added with test MSE -1334799178.888196
BedroomAbvGr was added with test MSE -1315922213.1356974
EnclosedPorch was added with test MSE -1315305918.2179291
PoolArea was added with test MSE -1314438325.8843122
The final best model is ['OverallQual', 'GrLivArea', 'BsmtFinSF1', 'GarageCars', 'YearRemodAdd', 'LotArea', 'MasVnrArea', 'KitchenAbvGr', '1stFlrSF', 'YearBuilt', 'OverallCond', 'ScreenPorch', 'WoodDeckSF', 'TotRmsAbvGrd', 'BedroomAbvGr', 'EnclosedPorch', 'PoolArea'] and its TEST MSE is -1314438325.8843122

    \end{Verbatim}

    Now with a larger \(\lambda\)

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}119}]:} \PY{c+c1}{\PYZsh{} Run through each model in the correct order and run CV on it and save the best CV score}
          \PY{n}{bestMeanCV} \PY{o}{=} \PY{k+kc}{None}
          \PY{n}{bestMeanCVModel} \PY{o}{=} \PY{p}{[}\PY{p}{]}
          \PY{n}{oldArraySize} \PY{o}{=} \PY{l+m+mi}{0}
          
          \PY{n}{X} \PY{o}{=} \PY{n}{housePrice}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SalePrice}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
          
          \PY{n}{columnsArray} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{columns}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} y is the response variable}
          \PY{n}{y} \PY{o}{=} \PY{n}{housePrice}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SalePrice}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
          
          \PY{k}{while} \PY{n}{oldArraySize} \PY{o}{!=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{:}
              \PY{n}{bestPredictor} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsq{}}
              \PY{n}{oldArraySize} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{columns}\PY{p}{)}
              \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{columnsArray}\PY{p}{:}
                  \PY{n}{thisModel} \PY{o}{=} \PY{n}{bestMeanCVModel}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}
                  \PY{n}{thisModel}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{i}\PY{p}{)}
                  \PY{c+c1}{\PYZsh{} First set X to be the full set of remaining parameters}
                  \PY{n}{x} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{n}{thisModel}\PY{p}{]}
          
                  \PY{k}{if} \PY{n+nb}{len}\PY{p}{(}\PY{n}{x}\PY{o}{.}\PY{n}{columns}\PY{p}{)} \PY{o}{==} \PY{l+m+mi}{1}\PY{p}{:}
                      \PY{n}{linregCVScores} \PY{o}{=} \PY{n}{cross\PYZus{}val\PYZus{}score}\PY{p}{(}\PY{n}{Ridge}\PY{p}{(}\PY{n}{alpha}\PY{o}{=}\PY{l+m+mi}{6}\PY{p}{)}\PY{p}{,}\PY{n}{x}\PY{o}{.}\PY{n}{values}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}\PY{n}{y}\PY{p}{,}\PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{neg\PYZus{}mean\PYZus{}squared\PYZus{}error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}
                  \PY{k}{else}\PY{p}{:}
                      \PY{n}{linregCVScores} \PY{o}{=} \PY{n}{cross\PYZus{}val\PYZus{}score}\PY{p}{(}\PY{n}{Ridge}\PY{p}{(}\PY{n}{alpha}\PY{o}{=}\PY{l+m+mi}{6}\PY{p}{)}\PY{p}{,}\PY{n}{x}\PY{p}{,}\PY{n}{y}\PY{p}{,}\PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{neg\PYZus{}mean\PYZus{}squared\PYZus{}error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}
                      
                  \PY{k}{if} \PY{o+ow}{not} \PY{n}{bestMeanCV}\PY{p}{:}
                      \PY{n}{bestMeanCV} \PY{o}{=} \PY{n}{linregCVScores}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}
                      \PY{n}{bestPredictor} \PY{o}{=} \PY{n}{i}
                  \PY{k}{elif} \PY{n}{bestMeanCV} \PY{o}{\PYZlt{}} \PY{n}{linregCVScores}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{p}{:}
                      \PY{n}{bestMeanCV} \PY{o}{=} \PY{n}{linregCVScores}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}
                      \PY{n}{bestPredictor} \PY{o}{=} \PY{n}{i}
              
              \PY{k}{if} \PY{n}{bestPredictor} \PY{o+ow}{not} \PY{o+ow}{in} \PY{n}{columnsArray}\PY{p}{:}
                  \PY{k}{break}
              
              \PY{n}{columnsArray} \PY{o}{=} \PY{n}{columnsArray}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{bestPredictor}\PY{p}{)}
              \PY{n}{bestMeanCVModel}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{bestPredictor}\PY{p}{)}
              \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{ was added with test MSE }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{bestMeanCVModel}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{n}{bestMeanCV}\PY{p}{)}\PY{p}{)}
          
                  
          \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{The final best model is }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{ and its TEST MSE is }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{bestMeanCVModel}\PY{p}{,}\PY{n}{bestMeanCV}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
OverallQual was added with test MSE -2371254029.0760694
GrLivArea was added with test MSE -1821363518.5244935
BsmtFinSF1 was added with test MSE -1653488270.7651806
GarageCars was added with test MSE -1522674235.152098
YearRemodAdd was added with test MSE -1477543064.7221112
LotArea was added with test MSE -1445288001.4210088
MasVnrArea was added with test MSE -1418272851.0154374
KitchenAbvGr was added with test MSE -1399503544.1065402
1stFlrSF was added with test MSE -1376798534.1186767
YearBuilt was added with test MSE -1366723443.0781755
OverallCond was added with test MSE -1351925127.2305455
ScreenPorch was added with test MSE -1346240217.2577364
WoodDeckSF was added with test MSE -1339095696.4405167
TotRmsAbvGrd was added with test MSE -1334732891.2793584
BedroomAbvGr was added with test MSE -1315808790.2185073
EnclosedPorch was added with test MSE -1315182547.8506525
PoolArea was added with test MSE -1314370013.4542491
Fireplaces was added with test MSE -1314248045.3630013
The final best model is ['OverallQual', 'GrLivArea', 'BsmtFinSF1', 'GarageCars', 'YearRemodAdd', 'LotArea', 'MasVnrArea', 'KitchenAbvGr', '1stFlrSF', 'YearBuilt', 'OverallCond', 'ScreenPorch', 'WoodDeckSF', 'TotRmsAbvGrd', 'BedroomAbvGr', 'EnclosedPorch', 'PoolArea', 'Fireplaces'] and its TEST MSE is -1314248045.3630013

    \end{Verbatim}

    A small improvement in the test MSE. Running Ridge Regression with the
identified features gives us the coefficients:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}120}]:} \PY{n}{X} \PY{o}{=} \PY{n}{housePrice}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SalePrice}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{[}\PY{n}{bestMeanCVModel}\PY{p}{]}
          
          \PY{c+c1}{\PYZsh{} y is the response variable}
          \PY{n}{y} \PY{o}{=} \PY{n}{housePrice}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SalePrice}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
          
          \PY{n}{rm} \PY{o}{=} \PY{n}{Ridge}\PY{p}{(}\PY{n}{alpha}\PY{o}{=}\PY{l+m+mi}{6}\PY{p}{)}
          \PY{n}{rm}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X}\PY{p}{,}\PY{n}{y}\PY{p}{)}
          
          \PY{n}{s} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SalePrice = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n+nb}{round}\PY{p}{(}\PY{n}{rm}\PY{o}{.}\PY{n}{intercept\PYZus{}}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
          
          \PY{k}{for} \PY{n}{i}\PY{p}{,}\PY{n}{j} \PY{o+ow}{in} \PY{n+nb}{zip}\PY{p}{(}\PY{n}{rm}\PY{o}{.}\PY{n}{coef\PYZus{}}\PY{p}{,}\PY{n}{bestMeanCVModel}\PY{p}{)}\PY{p}{:}
              \PY{n}{s} \PY{o}{=} \PY{n}{s} \PY{o}{+} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{ + }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{*}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n+nb}{round}\PY{p}{(}\PY{n}{i}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,}\PY{n}{j}\PY{p}{)}
              
          \PY{n}{s}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}120}]:} 'SalePrice = -1114268.8 + 17481.52*OverallQual + 39.74*GrLivArea + 16.75*BsmtFinSF1 + 11371.26*GarageCars + 193.67*YearRemodAdd + 0.5*LotArea + 28.95*MasVnrArea + -21496.85*KitchenAbvGr + 18.28*1stFlrSF + 338.07*YearBuilt + 4390.41*OverallCond + 54.94*ScreenPorch + 26.66*WoodDeckSF + 5626.97*TotRmsAbvGrd + -8289.6*BedroomAbvGr + 18.93*EnclosedPorch + -37.22*PoolArea + 3096.51*Fireplaces'
\end{Verbatim}
            
    \hypertarget{reproducing-p-values}{%
\subsubsection{Reproducing P-Values}\label{reproducing-p-values}}

First we will use statsmodels to give us the p-values of the
coefficients. Then we will show that we can calculate these ourselves
using linear algebra. Then we will use our method to calculate the
p-values for the Ridge Regression model.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}121}]:} \PY{n}{X} \PY{o}{=} \PY{n}{housePrice}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SalePrice}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{[}\PY{n}{bestMeanCVModel}\PY{p}{]}
          
          \PY{c+c1}{\PYZsh{} y is the response variable}
          \PY{n}{y} \PY{o}{=} \PY{n}{housePrice}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SalePrice}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
          
          \PY{n}{X\PYZus{}new} \PY{o}{=} \PY{n}{sm}\PY{o}{.}\PY{n}{add\PYZus{}constant}\PY{p}{(}\PY{n}{X}\PY{p}{)}
          \PY{n}{est} \PY{o}{=} \PY{n}{sm}\PY{o}{.}\PY{n}{OLS}\PY{p}{(}\PY{n}{y}\PY{p}{,} \PY{n}{X\PYZus{}new}\PY{p}{)}
          \PY{n}{est2} \PY{o}{=} \PY{n}{est}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{p}{)}
          \PY{n+nb}{print}\PY{p}{(}\PY{n}{est2}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
                            OLS Regression Results                            
==============================================================================
Dep. Variable:              SalePrice   R-squared:                       0.804
Model:                            OLS   Adj. R-squared:                  0.802
Method:                 Least Squares   F-statistic:                     329.2
Date:                Sat, 27 Jul 2019   Prob (F-statistic):               0.00
Time:                        18:37:58   Log-Likelihood:                -17353.
No. Observations:                1460   AIC:                         3.474e+04
Df Residuals:                    1441   BIC:                         3.484e+04
Df Model:                          18                                         
Covariance Type:            nonrobust                                         
=================================================================================
                    coef    std err          t      P>|t|      [0.025      0.975]
---------------------------------------------------------------------------------
const           -1.1e+06   1.22e+05     -9.011      0.000   -1.34e+06    -8.6e+05
OverallQual    1.745e+04   1156.715     15.082      0.000    1.52e+04    1.97e+04
GrLivArea        39.4748      4.039      9.774      0.000      31.552      47.398
BsmtFinSF1       16.7307      2.424      6.903      0.000      11.976      21.485
GarageCars     1.151e+04   1714.064      6.716      0.000    8148.780    1.49e+04
YearRemodAdd    190.8258     65.700      2.905      0.004      61.948     319.703
LotArea           0.4987      0.101      4.932      0.000       0.300       0.697
MasVnrArea       28.8277      5.961      4.836      0.000      17.135      40.521
KitchenAbvGr  -2.381e+04   4777.891     -4.983      0.000   -3.32e+04   -1.44e+04
1stFlrSF         18.4213      3.377      5.456      0.000      11.798      25.045
YearBuilt       334.5033     56.393      5.932      0.000     223.882     445.124
OverallCond    4361.6048   1020.554      4.274      0.000    2359.674    6363.535
ScreenPorch      54.3900     17.323      3.140      0.002      20.408      88.372
WoodDeckSF       26.3639      8.007      3.293      0.001      10.658      42.070
TotRmsAbvGrd   5850.6359   1243.198      4.706      0.000    3411.964    8289.308
BedroomAbvGr  -8433.9916   1674.575     -5.036      0.000   -1.17e+04   -5149.126
EnclosedPorch    18.4944     16.992      1.088      0.277     -14.838      51.827
PoolArea        -37.1000     23.830     -1.557      0.120     -83.846       9.646
Fireplaces     2986.9485   1761.315      1.696      0.090    -468.067    6441.964
==============================================================================
Omnibus:                      627.897   Durbin-Watson:                   1.962
Prob(Omnibus):                  0.000   Jarque-Bera (JB):           106807.567
Skew:                          -0.886   Prob(JB):                         0.00
Kurtosis:                      44.864   Cond. No.                     1.94e+06
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 1.94e+06. This might indicate that there are
strong multicollinearity or other numerical problems.

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
C:\textbackslash{}Users\textbackslash{}HVAD\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}numpy\textbackslash{}core\textbackslash{}fromnumeric.py:52: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.
  return getattr(obj, method)(*args, **kwds)

    \end{Verbatim}

    Let \(\hat{\theta}\) be an unbiased estimator of \(\theta\) and
\(\hat{Y}\) be the fitted values for \(Y\). i.e.~we expect that
\(E[\hat{\theta}] = \theta\) and \(E[\hat{Y}] = Y\).

Then the variance of the estimator can be calculated as follows:

\[\sigma_{\hat{\theta}}^2 = E[(\hat{\theta} - E[\hat{\theta}])(\hat{\theta} - E[\hat{\theta}])^T] = E[(\hat{\theta} - \theta)(\hat{\theta} - \theta)^T] = E[((X^T X)^{-1} X^T (\hat{Y} - Y))((X^T X)^{-1} X^T (\hat{Y} - Y))^T] = (X^T X)^{-1} X^T E[(\hat{Y} - Y)(\hat{Y} - Y)^T] X (X^T X)^{-1} = (X^T X)^{-1} X^T E[e^2] X (X^T X)^{-1} = (X^T X)^{-1} X^T \sigma_e^2 I X (X^T X)^{-1} = \sigma_e^2 (X^T X)^{-1} X^T X (X^T X)^{-1} = \sigma_e^2 (X^T X)^{-1} = MSE \times (X^T X)^{-1}\]

where \(\hat{Y} - Y = e\) are the residuals and
\(MSE = \frac{\sum_{i=1}^{n} (\hat{Y}^{(i)} - Y^{(i)})^2}{n-p}\) is the
Mean Squared Error with \(p\) predictors.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}122}]:} \PY{c+c1}{\PYZsh{} Get the coefficient solutions from the model}
          \PY{n}{coefs} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{est2}\PY{o}{.}\PY{n}{params}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PY{n}{est2}\PY{o}{.}\PY{n}{params}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Get the predictions (X\PYZus{}new includes the constant term)}
          \PY{n}{predictions} \PY{o}{=} \PY{n}{est2}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}new}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Calculate the MSE}
          \PY{n}{MSE} \PY{o}{=} \PY{p}{(}\PY{n+nb}{sum}\PY{p}{(}\PY{p}{(}\PY{n}{y}\PY{o}{\PYZhy{}}\PY{n}{predictions}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{o}{/}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X\PYZus{}new}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X\PYZus{}new}\PY{o}{.}\PY{n}{columns}\PY{p}{)}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Calculate the variance}
          \PY{n}{var} \PY{o}{=} \PY{n}{MSE}\PY{o}{*}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{inv}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{X\PYZus{}new}\PY{o}{.}\PY{n}{T}\PY{p}{,}\PY{n}{X\PYZus{}new}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{diagonal}\PY{p}{(}\PY{p}{)}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Calculate the standard deviation}
          \PY{n}{sd} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{var}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Calculate the t\PYZhy{}statistics}
          \PY{n}{t} \PY{o}{=} \PY{n}{coefs}\PY{o}{/} \PY{n}{sd}
          
          \PY{c+c1}{\PYZsh{} Calculate the p\PYZhy{}values using the t\PYZhy{}statistics and the t\PYZhy{}distribution (2 is two\PYZhy{}sided)}
          \PY{n}{p\PYZus{}values} \PY{o}{=}\PY{p}{[}\PY{l+m+mi}{2}\PY{o}{*}\PY{p}{(}\PY{l+m+mi}{1}\PY{o}{\PYZhy{}}\PY{n}{stats}\PY{o}{.}\PY{n}{t}\PY{o}{.}\PY{n}{cdf}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{abs}\PY{p}{(}\PY{n}{i}\PY{p}{)}\PY{p}{,}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X\PYZus{}new}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{)} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{t}\PY{p}{]}
          
          \PY{c+c1}{\PYZsh{} 3 decimal places to match statsmodels output}
          \PY{n}{var} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{n}{var}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}
          \PY{n}{t} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{n}{t}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}
          \PY{n}{p\PYZus{}values} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{n}{p\PYZus{}values}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} 4 decimal places to match statsmodels}
          \PY{n}{coefs} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{n}{coefs}\PY{p}{,}\PY{l+m+mi}{4}\PY{p}{)}
          
          \PY{n}{summary\PYZus{}df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{p}{)}
          \PY{n}{summary\PYZus{}df}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Features}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{,}\PY{n}{summary\PYZus{}df}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{coef}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{,}\PY{n}{summary\PYZus{}df}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{std err}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{,}\PY{n}{summary\PYZus{}df}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{t}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{,}\PY{n}{summary\PYZus{}df}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{P \PYZgt{} |t|}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]} \PY{o}{=} \PY{p}{[}
              \PY{n}{X\PYZus{}new}\PY{o}{.}\PY{n}{columns}\PY{p}{,}\PY{n}{coefs}\PY{p}{,}\PY{n}{sd}\PY{p}{,}\PY{n}{t}\PY{p}{,}\PY{n}{p\PYZus{}values}\PY{p}{]}
          \PY{n+nb}{print}\PY{p}{(}\PY{n}{summary\PYZus{}df}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
         Features          coef        std err       t  P > |t|
0           const -1.099679e+06  122041.446311  -9.011    0.000
1     OverallQual  1.744563e+04    1156.714726  15.082    0.000
2       GrLivArea  3.947480e+01       4.038924   9.774    0.000
3      BsmtFinSF1  1.673070e+01       2.423746   6.903    0.000
4      GarageCars  1.151111e+04    1714.064086   6.716    0.000
5    YearRemodAdd  1.908258e+02      65.699730   2.905    0.004
6         LotArea  4.987000e-01       0.101129   4.932    0.000
7      MasVnrArea  2.882770e+01       5.960942   4.836    0.000
8    KitchenAbvGr -2.380787e+04    4777.891291  -4.983    0.000
9        1stFlrSF  1.842130e+01       3.376628   5.456    0.000
10      YearBuilt  3.345033e+02      56.393014   5.932    0.000
11    OverallCond  4.361605e+03    1020.553865   4.274    0.000
12    ScreenPorch  5.439000e+01      17.323448   3.140    0.002
13     WoodDeckSF  2.636390e+01       8.006592   3.293    0.001
14   TotRmsAbvGrd  5.850636e+03    1243.198150   4.706    0.000
15   BedroomAbvGr -8.433992e+03    1674.574959  -5.036    0.000
16  EnclosedPorch  1.849440e+01      16.992309   1.088    0.277
17       PoolArea -3.710000e+01      23.830194  -1.557    0.120
18     Fireplaces  2.986948e+03    1761.314694   1.696    0.090

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}123}]:} \PY{k}{def} \PY{n+nf}{fitAndPValues}\PY{p}{(}\PY{n}{model}\PY{p}{,}\PY{n}{X}\PY{p}{,}\PY{n}{y}\PY{p}{)}\PY{p}{:}
              \PY{c+c1}{\PYZsh{} Get the coefficient solutions from the model}
              \PY{n}{coefs} \PY{o}{=} \PY{p}{[}\PY{p}{]}
              \PY{k}{if} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{params}\PY{l+s+s1}{\PYZsq{}} \PY{o+ow}{in} \PY{n+nb}{dir}\PY{p}{(}\PY{n}{model}\PY{p}{)}\PY{p}{:}
                  \PY{n}{coefs} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{params}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PY{n}{model}\PY{o}{.}\PY{n}{params}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{)}
              \PY{k}{else}\PY{p}{:}
                  \PY{n}{coefs} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{intercept\PYZus{}}\PY{p}{,}\PY{n}{model}\PY{o}{.}\PY{n}{coef\PYZus{}}\PY{p}{)}
          
              \PY{c+c1}{\PYZsh{} Get the predictions (X\PYZus{}new includes the constant term)}
              \PY{n}{predictions} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X}\PY{p}{)}
          
              \PY{k}{if} \PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{columns}\PY{p}{)} \PY{o}{\PYZlt{}} \PY{n+nb}{len}\PY{p}{(}\PY{n}{coefs}\PY{p}{)}\PY{p}{:}
                  \PY{n}{X} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}
                  \PY{n}{X}\PY{o}{.}\PY{n}{insert}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Const}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}
              
              \PY{c+c1}{\PYZsh{} Calculate the MSE}
              \PY{n}{MSE} \PY{o}{=} \PY{p}{(}\PY{n+nb}{sum}\PY{p}{(}\PY{p}{(}\PY{n}{y}\PY{o}{\PYZhy{}}\PY{n}{predictions}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{o}{/}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{columns}\PY{p}{)}\PY{p}{)}
          
              \PY{c+c1}{\PYZsh{} Calculate the variance}
              \PY{n}{var} \PY{o}{=} \PY{n}{MSE}\PY{o}{*}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{inv}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{T}\PY{p}{,}\PY{n}{X}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{diagonal}\PY{p}{(}\PY{p}{)}\PY{p}{)}
          
              \PY{c+c1}{\PYZsh{} Calculate the standard deviation}
              \PY{n}{sd} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{var}\PY{p}{)}
          
              \PY{c+c1}{\PYZsh{} Calculate the t\PYZhy{}statistics}
              \PY{n}{t} \PY{o}{=} \PY{n}{coefs}\PY{o}{/} \PY{n}{sd}
          
              \PY{c+c1}{\PYZsh{} Calculate the p\PYZhy{}values using the t\PYZhy{}statistics and the t\PYZhy{}distribution (2 is two\PYZhy{}sided)}
              \PY{n}{p\PYZus{}values} \PY{o}{=}\PY{p}{[}\PY{l+m+mi}{2}\PY{o}{*}\PY{p}{(}\PY{l+m+mi}{1}\PY{o}{\PYZhy{}}\PY{n}{stats}\PY{o}{.}\PY{n}{t}\PY{o}{.}\PY{n}{cdf}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{abs}\PY{p}{(}\PY{n}{i}\PY{p}{)}\PY{p}{,}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{)} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{t}\PY{p}{]}
          
              \PY{c+c1}{\PYZsh{} 3 decimal places to match statsmodels output}
              \PY{n}{var} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{n}{var}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}
              \PY{n}{t} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{n}{t}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}
              \PY{n}{p\PYZus{}values} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{n}{p\PYZus{}values}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}
          
              \PY{c+c1}{\PYZsh{} 4 decimal places to match statsmodels}
              \PY{n}{coefs} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{n}{coefs}\PY{p}{,}\PY{l+m+mi}{4}\PY{p}{)}
          
              \PY{n}{summary\PYZus{}df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{p}{)}
              \PY{n}{summary\PYZus{}df}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Features}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{,}\PY{n}{summary\PYZus{}df}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{coef}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{,}\PY{n}{summary\PYZus{}df}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{std err}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{,}\PY{n}{summary\PYZus{}df}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{t}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{,}\PY{n}{summary\PYZus{}df}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{P \PYZgt{} |t|}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]} \PY{o}{=} \PY{p}{[}\PY{n}{X}\PY{o}{.}\PY{n}{columns}\PY{p}{,}
                                                                                                          \PY{n}{coefs}\PY{p}{,}\PY{n}{sd}\PY{p}{,}\PY{n}{t}\PY{p}{,}\PY{n}{p\PYZus{}values}\PY{p}{]}
              \PY{n+nb}{print}\PY{p}{(}\PY{n}{summary\PYZus{}df}\PY{p}{)}    
\end{Verbatim}


    Let's make sure our Ridge Regression p-values match the statsmodels
outputs when we set the shrinkage term to 0 (i.e.~Linear Regression
without the L2 norm term)

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}124}]:} \PY{n}{X} \PY{o}{=} \PY{n}{housePrice}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SalePrice}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{[}\PY{n}{bestMeanCVModel}\PY{p}{]}
          
          \PY{c+c1}{\PYZsh{} y is the response variable}
          \PY{n}{y} \PY{o}{=} \PY{n}{housePrice}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SalePrice}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
          
          \PY{n}{rm} \PY{o}{=} \PY{n}{Ridge}\PY{p}{(}\PY{n}{alpha}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
          \PY{n}{rm}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X}\PY{p}{,}\PY{n}{y}\PY{p}{)}
          
          \PY{n}{fitAndPValues}\PY{p}{(}\PY{n}{rm}\PY{p}{,}\PY{n}{X}\PY{p}{,}\PY{n}{y}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
         Features          coef        std err       t  P > |t|
0           Const -1.099679e+06  122041.446311  -9.011    0.000
1     OverallQual  1.744563e+04    1156.714726  15.082    0.000
2       GrLivArea  3.947480e+01       4.038924   9.774    0.000
3      BsmtFinSF1  1.673070e+01       2.423746   6.903    0.000
4      GarageCars  1.151111e+04    1714.064086   6.716    0.000
5    YearRemodAdd  1.908258e+02      65.699730   2.905    0.004
6         LotArea  4.987000e-01       0.101129   4.932    0.000
7      MasVnrArea  2.882770e+01       5.960942   4.836    0.000
8    KitchenAbvGr -2.380787e+04    4777.891291  -4.983    0.000
9        1stFlrSF  1.842130e+01       3.376628   5.456    0.000
10      YearBuilt  3.345033e+02      56.393014   5.932    0.000
11    OverallCond  4.361605e+03    1020.553865   4.274    0.000
12    ScreenPorch  5.439000e+01      17.323448   3.140    0.002
13     WoodDeckSF  2.636390e+01       8.006592   3.293    0.001
14   TotRmsAbvGrd  5.850636e+03    1243.198150   4.706    0.000
15   BedroomAbvGr -8.433992e+03    1674.574959  -5.036    0.000
16  EnclosedPorch  1.849440e+01      16.992309   1.088    0.277
17       PoolArea -3.710000e+01      23.830194  -1.557    0.120
18     Fireplaces  2.986948e+03    1761.314694   1.696    0.090

    \end{Verbatim}

    Now we are able to return p-values for Ridge Regression for our model
with shrinkage term 6

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}125}]:} \PY{n}{X} \PY{o}{=} \PY{n}{housePrice}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SalePrice}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{[}\PY{n}{bestMeanCVModel}\PY{p}{]}
          
          \PY{c+c1}{\PYZsh{} y is the response variable}
          \PY{n}{y} \PY{o}{=} \PY{n}{housePrice}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SalePrice}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
          
          \PY{n}{rm} \PY{o}{=} \PY{n}{Ridge}\PY{p}{(}\PY{n}{alpha}\PY{o}{=}\PY{l+m+mi}{6}\PY{p}{)}
          \PY{n}{rm}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X}\PY{p}{,}\PY{n}{y}\PY{p}{)}
          
          \PY{n}{fitAndPValues}\PY{p}{(}\PY{n}{rm}\PY{p}{,}\PY{n}{X}\PY{p}{,}\PY{n}{y}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
         Features          coef        std err       t  P > |t|
0           Const -1.114269e+06  122052.144169  -9.129    0.000
1     OverallQual  1.748152e+04    1156.816121  15.112    0.000
2       GrLivArea  3.974460e+01       4.039278   9.840    0.000
3      BsmtFinSF1  1.674920e+01       2.423958   6.910    0.000
4      GarageCars  1.137126e+04    1714.214337   6.634    0.000
5    YearRemodAdd  1.936712e+02      65.705489   2.948    0.003
6         LotArea  5.009000e-01       0.101138   4.953    0.000
7      MasVnrArea  2.895370e+01       5.961464   4.857    0.000
8    KitchenAbvGr -2.149685e+04    4778.310109  -4.499    0.000
9        1stFlrSF  1.827980e+01       3.376924   5.413    0.000
10      YearBuilt  3.380716e+02      56.397957   5.994    0.000
11    OverallCond  4.390405e+03    1020.643324   4.302    0.000
12    ScreenPorch  5.493510e+01      17.324966   3.171    0.002
13     WoodDeckSF  2.665780e+01       8.007293   3.329    0.001
14   TotRmsAbvGrd  5.626965e+03    1243.307126   4.526    0.000
15   BedroomAbvGr -8.289604e+03    1674.721749  -4.950    0.000
16  EnclosedPorch  1.892600e+01      16.993798   1.114    0.266
17       PoolArea -3.721990e+01      23.832283  -1.562    0.119
18     Fireplaces  3.096515e+03    1761.469087   1.758    0.079

    \end{Verbatim}

    \hypertarget{module-for-ridge-regression}{%
\subsubsection{Module for Ridge
Regression}\label{module-for-ridge-regression}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}150}]:} \PY{k}{class} \PY{n+nc}{RidgeRegression}\PY{p}{(}\PY{n}{Ridge}\PY{p}{)}\PY{p}{:}
              \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}
          \PY{l+s+sd}{    This class inherits from the Ridge class in the sklearn package. It extends that class by}
          \PY{l+s+sd}{    adding the capability to produce p\PYZhy{}values, run feature selection and find the best alpha}
          \PY{l+s+sd}{    which minimises the MSE}
          \PY{l+s+sd}{    \PYZsq{}\PYZsq{}\PYZsq{}}
              
              \PY{k}{def} \PY{n+nf}{summary}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,}\PY{n}{X}\PY{p}{,}\PY{n}{y}\PY{p}{)}\PY{p}{:}
                  \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}
          \PY{l+s+sd}{        This method produces a summary similar to the one produced by statsmodels.api.}
          \PY{l+s+sd}{        It includes the coefficients and their p\PYZhy{}values in a summary table}
          \PY{l+s+sd}{        :param X: features array}
          \PY{l+s+sd}{        :param y: response array}
          \PY{l+s+sd}{        \PYZsq{}\PYZsq{}\PYZsq{}}
                  
                  \PY{c+c1}{\PYZsh{} This will store the coefficients of the model that has already been run}
                  \PY{n}{coefs} \PY{o}{=} \PY{p}{[}\PY{p}{]}
                  
                  \PY{c+c1}{\PYZsh{} If the model was fit with an intercept}
                  \PY{k}{if} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{intercept\PYZus{}}\PY{l+s+s1}{\PYZsq{}} \PY{o+ow}{in} \PY{n+nb}{dir}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
                      \PY{n}{coefs} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{intercept\PYZus{}}\PY{p}{,}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{coef\PYZus{}}\PY{p}{)}
                  \PY{k}{else}\PY{p}{:}
                      \PY{n}{coefs} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{coef\PYZus{}}
          
                  \PY{c+c1}{\PYZsh{} Get the predictions}
                  \PY{n}{predictions} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X}\PY{p}{)}
          
                  \PY{c+c1}{\PYZsh{} If a constant column needs to be added (determine this dynamically)}
                  \PY{k}{if} \PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{columns}\PY{p}{)} \PY{o}{\PYZlt{}} \PY{n+nb}{len}\PY{p}{(}\PY{n}{coefs}\PY{p}{)}\PY{p}{:}
                      \PY{n}{X} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}
                      \PY{n}{X}\PY{o}{.}\PY{n}{insert}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Const}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}
                  
                  \PY{c+c1}{\PYZsh{} Calculate the MSE}
                  \PY{n}{MSE} \PY{o}{=} \PY{p}{(}\PY{n+nb}{sum}\PY{p}{(}\PY{p}{(}\PY{n}{y}\PY{o}{\PYZhy{}}\PY{n}{predictions}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{o}{/}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{columns}\PY{p}{)}\PY{p}{)}
          
                  \PY{c+c1}{\PYZsh{} Calculate the variance}
                  \PY{n}{var} \PY{o}{=} \PY{n}{MSE}\PY{o}{*}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{inv}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{T}\PY{p}{,}\PY{n}{X}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{diagonal}\PY{p}{(}\PY{p}{)}\PY{p}{)}
          
                  \PY{c+c1}{\PYZsh{} Calculate the standard deviation}
                  \PY{n}{sd} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{var}\PY{p}{)}
          
                  \PY{c+c1}{\PYZsh{} Calculate the t\PYZhy{}statistics}
                  \PY{n}{t} \PY{o}{=} \PY{n}{coefs}\PY{o}{/} \PY{n}{sd}
          
                  \PY{c+c1}{\PYZsh{} Calculate the p\PYZhy{}values using the t\PYZhy{}statistics and the t\PYZhy{}distribution (2 is two\PYZhy{}sided)}
                  \PY{n}{p\PYZus{}values} \PY{o}{=}\PY{p}{[}\PY{l+m+mi}{2}\PY{o}{*}\PY{p}{(}\PY{l+m+mi}{1}\PY{o}{\PYZhy{}}\PY{n}{stats}\PY{o}{.}\PY{n}{t}\PY{o}{.}\PY{n}{cdf}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{abs}\PY{p}{(}\PY{n}{i}\PY{p}{)}\PY{p}{,}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{)} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{t}\PY{p}{]}
          
                  \PY{c+c1}{\PYZsh{} 3 decimal places to match statsmodels output}
                  \PY{n}{var} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{n}{var}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}
                  \PY{n}{t} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{n}{t}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}
                  \PY{n}{p\PYZus{}values} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{n}{p\PYZus{}values}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}
          
                  \PY{c+c1}{\PYZsh{} 4 decimal places to match statsmodels}
                  \PY{n}{coefs} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{n}{coefs}\PY{p}{,}\PY{l+m+mi}{4}\PY{p}{)}
                  
                  \PY{c+c1}{\PYZsh{} Summary dataframe}
                  \PY{n}{summary\PYZus{}df} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{p}{)}
                  \PY{n}{summary\PYZus{}df}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Features}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{,}\PY{n}{summary\PYZus{}df}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{coef}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{,}\PY{n}{summary\PYZus{}df}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{std err}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{,}\PY{n}{summary\PYZus{}df}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{t}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{,}\PY{n}{summary\PYZus{}df}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{P \PYZgt{} |t|}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]} \PY{o}{=} \PY{p}{[}\PY{n}{X}\PY{o}{.}\PY{n}{columns}\PY{p}{,}
                                                                                                              \PY{n}{coefs}\PY{p}{,}\PY{n}{sd}\PY{p}{,}\PY{n}{t}\PY{p}{,}\PY{n}{p\PYZus{}values}\PY{p}{]}
                  \PY{n+nb}{print}\PY{p}{(}\PY{n}{summary\PYZus{}df}\PY{p}{)} 
                  
              \PY{k}{def} \PY{n+nf}{findBestAlpha}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,}\PY{n}{X}\PY{p}{,}\PY{n}{y}\PY{p}{,}\PY{n}{silent}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{p}{:}
                  \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}
          \PY{l+s+sd}{        This method keeps changing alpha until the MSE is reduced as much as it can be reduced. This}
          \PY{l+s+sd}{        alpha selection depends on input datasets}
          \PY{l+s+sd}{        :param X: features array}
          \PY{l+s+sd}{        :param y: response array}
          \PY{l+s+sd}{        :param silent: if True, then progress is omitted}
          \PY{l+s+sd}{        \PYZsq{}\PYZsq{}\PYZsq{}}
          
                  \PY{n}{silent} \PY{o}{=} \PY{k+kc}{True}
                  \PY{n}{alpha} \PY{o}{=} \PY{l+m+mi}{1}
                  \PY{n}{prevAlpha} \PY{o}{=} \PY{k+kc}{None}
                  \PY{n}{bestMSE} \PY{o}{=} \PY{k+kc}{None}
                  \PY{n}{tol} \PY{o}{=} \PY{l+m+mf}{0.000001}
                  \PY{n}{doublingMode} \PY{o}{=} \PY{k+kc}{True}
                  
                  \PY{c+c1}{\PYZsh{} Here, we start by continuously doubling alpha until we get to a point where the MSE doesn\PYZsq{}t increase}
                  \PY{c+c1}{\PYZsh{} Then, at this point, we switch to incrementing (or decrementing) by smaller amounts until the tolerance is reached}
                  \PY{k}{while} \PY{k+kc}{True}\PY{p}{:}
                      \PY{c+c1}{\PYZsh{} Calculate the MSE using this alpha}
                      \PY{n}{thisMSE} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{cross\PYZus{}val\PYZus{}score}\PY{p}{(}\PY{n}{RidgeRegression}\PY{p}{(}\PY{n}{alpha}\PY{o}{=}\PY{n}{alpha}\PY{p}{)}\PY{p}{,}\PY{n}{X}\PY{p}{,}\PY{n}{y}\PY{p}{,}\PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{neg\PYZus{}mean\PYZus{}squared\PYZus{}error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}
          
                      \PY{k}{if} \PY{o+ow}{not} \PY{n}{silent}\PY{p}{:}
                          \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{alpha = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{bestMSE = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{thisMSE = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{alpha}\PY{p}{,}\PY{n}{bestMSE}\PY{p}{,}\PY{n}{thisMSE}\PY{p}{)}\PY{p}{)}
          
                      \PY{c+c1}{\PYZsh{} if doubling mode}
                      \PY{k}{if} \PY{n}{doublingMode}\PY{p}{:}
                          \PY{c+c1}{\PYZsh{} update bestMSE}
                          \PY{k}{if} \PY{p}{(}\PY{o+ow}{not} \PY{n}{bestMSE}\PY{p}{)} \PY{o+ow}{or} \PY{p}{(}\PY{n}{bestMSE} \PY{o}{\PYZlt{}} \PY{n}{thisMSE}\PY{p}{)}\PY{p}{:}
                              \PY{n}{bestMSE} \PY{o}{=} \PY{n}{thisMSE}
                          \PY{k}{else}\PY{p}{:}
                              \PY{k}{if} \PY{o+ow}{not} \PY{n}{silent}\PY{p}{:}
                                  \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Doubling Finished!!!!}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                                  
                              \PY{c+c1}{\PYZsh{} switch the mode and roll back alpha to the previous one}
                              \PY{n}{doublingMode} \PY{o}{=} \PY{k+kc}{False}
                              \PY{n}{tempAlpha} \PY{o}{=} \PY{n}{prevAlpha}
                              \PY{n}{prevAlpha} \PY{o}{=} \PY{n}{alpha}
                              \PY{n}{alpha} \PY{o}{=} \PY{n}{tempAlpha}
                              \PY{k}{continue}
          
                          \PY{c+c1}{\PYZsh{} update alpha}
                          \PY{n}{prevAlpha} \PY{o}{=} \PY{n}{alpha}
                          \PY{n}{alpha} \PY{o}{=} \PY{p}{(}\PY{n}{alpha} \PY{o}{+} \PY{l+m+mf}{0.001}\PY{p}{)}\PY{o}{*}\PY{l+m+mi}{2}
                      \PY{k}{else}\PY{p}{:}        
                          \PY{c+c1}{\PYZsh{} update alpha to |alpha\PYZhy{}prevAlpha|/2 away from where it currently is (in either direction)}
                          \PY{n}{ghostPoint} \PY{o}{=} \PY{n}{alpha} \PY{o}{+} \PY{p}{(}\PY{n}{alpha} \PY{o}{\PYZhy{}} \PY{n}{prevAlpha}\PY{p}{)}
                          \PY{n}{nextAlpha1} \PY{o}{=} \PY{p}{(}\PY{n}{prevAlpha} \PY{o}{+} \PY{n}{alpha}\PY{p}{)}\PY{o}{/}\PY{l+m+mi}{2}
                          \PY{n}{nextAlpha2} \PY{o}{=} \PY{p}{(}\PY{n}{alpha} \PY{o}{+} \PY{n}{ghostPoint}\PY{p}{)}\PY{o}{/}\PY{l+m+mi}{2}
                          
                          \PY{c+c1}{\PYZsh{} The Ridge class has numerical issues when alpha is close to zero}
                          \PY{k}{if}\PY{p}{(}\PY{n}{nextAlpha1} \PY{o}{\PYZlt{}} \PY{l+m+mf}{0.0001}\PY{p}{)}\PY{p}{:}
                              \PY{n}{nextAlpha1} \PY{o}{=} \PY{l+m+mf}{0.0001}
                          \PY{k}{if}\PY{p}{(}\PY{n}{nextAlpha2} \PY{o}{\PYZlt{}} \PY{l+m+mf}{0.0001}\PY{p}{)}\PY{p}{:}
                              \PY{n}{nextAlpha2} \PY{o}{=} \PY{l+m+mf}{0.0001}
                              
                          \PY{c+c1}{\PYZsh{} Calculate the MSE on either side of alpha}
                          \PY{n}{MSE1} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{cross\PYZus{}val\PYZus{}score}\PY{p}{(}\PY{n}{RidgeRegression}\PY{p}{(}\PY{n}{alpha}\PY{o}{=}\PY{n}{nextAlpha1}\PY{p}{)}\PY{p}{,}\PY{n}{X}\PY{p}{,}\PY{n}{y}\PY{p}{,}\PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{neg\PYZus{}mean\PYZus{}squared\PYZus{}error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}
                          \PY{n}{MSE2} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{cross\PYZus{}val\PYZus{}score}\PY{p}{(}\PY{n}{RidgeRegression}\PY{p}{(}\PY{n}{alpha}\PY{o}{=}\PY{n}{nextAlpha2}\PY{p}{)}\PY{p}{,}\PY{n}{X}\PY{p}{,}\PY{n}{y}\PY{p}{,}\PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{neg\PYZus{}mean\PYZus{}squared\PYZus{}error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}
          
                          \PY{c+c1}{\PYZsh{} Choose the MSE and the corresponding alpha of the one that is better}
                          \PY{k}{if} \PY{p}{(}\PY{n}{MSE1} \PY{o}{\PYZgt{}} \PY{n}{MSE2}\PY{p}{)} \PY{o+ow}{and} \PY{p}{(}\PY{n}{MSE1} \PY{o}{\PYZgt{}} \PY{n}{bestMSE}\PY{p}{)} \PY{o+ow}{and} \PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{abs}\PY{p}{(}\PY{n}{prevAlpha} \PY{o}{\PYZhy{}} \PY{n}{alpha}\PY{p}{)} \PY{o}{\PYZgt{}} \PY{n}{tol}\PY{p}{)}\PY{p}{:}
                              \PY{n}{prevAlpha} \PY{o}{=} \PY{n}{alpha}
                              \PY{n}{alpha} \PY{o}{=} \PY{n}{nextAlpha1}
                              \PY{n}{bestMSE} \PY{o}{=} \PY{n}{MSE1}
                          \PY{k}{elif} \PY{p}{(}\PY{n}{MSE2} \PY{o}{\PYZgt{}} \PY{n}{MSE1}\PY{p}{)} \PY{o+ow}{and} \PY{p}{(}\PY{n}{MSE2} \PY{o}{\PYZgt{}} \PY{n}{bestMSE}\PY{p}{)} \PY{o+ow}{and} \PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{abs}\PY{p}{(}\PY{n}{prevAlpha} \PY{o}{\PYZhy{}} \PY{n}{alpha}\PY{p}{)} \PY{o}{\PYZgt{}} \PY{n}{tol}\PY{p}{)}\PY{p}{:}
                              \PY{n}{prevAlpha} \PY{o}{=} \PY{n}{alpha}
                              \PY{n}{alpha} \PY{o}{=} \PY{n}{nextAlpha2}
                              \PY{n}{bestMSE} \PY{o}{=} \PY{n}{MSE2}
                          \PY{k}{else}\PY{p}{:}
                              \PY{k}{if} \PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{abs}\PY{p}{(}\PY{n}{prevAlpha} \PY{o}{\PYZhy{}} \PY{n}{alpha}\PY{p}{)} \PY{o}{\PYZgt{}} \PY{n}{tol}\PY{p}{)}\PY{p}{:}
                                  \PY{c+c1}{\PYZsh{} pull prevAlpha closer to alpha}
                                  \PY{n}{prevAlpha} \PY{o}{=} \PY{p}{(}\PY{n}{prevAlpha} \PY{o}{+} \PY{n}{alpha}\PY{p}{)}\PY{o}{/}\PY{l+m+mi}{2}
                              \PY{k}{else}\PY{p}{:}
                                  \PY{n}{alpha} \PY{o}{=} \PY{n}{prevAlpha}
                                  \PY{k}{break}
          
                  \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{alpha} \PY{o}{=} \PY{n}{alpha}
                  \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Ridge Regression MSE = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{, best alpha = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{bestMSE}\PY{p}{,}\PY{n}{alpha}\PY{p}{)}\PY{p}{)}
          
              \PY{k}{def} \PY{n+nf}{featureSelection}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,}\PY{n}{X}\PY{p}{,}\PY{n}{y}\PY{p}{)}\PY{p}{:}
                  \PY{l+s+sd}{\PYZsq{}\PYZsq{}\PYZsq{}}
          \PY{l+s+sd}{        This method iterates and adds a new feature to the features list in the}
          \PY{l+s+sd}{        order of best improvement of MSE}
          \PY{l+s+sd}{        :param X: features array}
          \PY{l+s+sd}{        :param y: response array}
          \PY{l+s+sd}{        \PYZsq{}\PYZsq{}\PYZsq{}}
                  
                  \PY{c+c1}{\PYZsh{} Run through each model in the correct order and run CV on it and save the best CV score}
                  \PY{n}{bestMeanCV} \PY{o}{=} \PY{k+kc}{None}
                  \PY{n}{bestMeanCVModel} \PY{o}{=} \PY{p}{[}\PY{p}{]}
                  \PY{n}{oldArraySize} \PY{o}{=} \PY{l+m+mi}{0}
          
                  \PY{n}{columnsArray} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{columns}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}
          
                  \PY{k}{while} \PY{n}{oldArraySize} \PY{o}{!=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{:}
                      \PY{n}{bestPredictor} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZsq{}}
                      \PY{n}{oldArraySize} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{columns}\PY{p}{)}
                      \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{columnsArray}\PY{p}{:}
                          \PY{n}{thisModel} \PY{o}{=} \PY{n}{bestMeanCVModel}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}
                          \PY{n}{thisModel}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{i}\PY{p}{)}
                          \PY{c+c1}{\PYZsh{} First set X to be the full set of remaining parameters}
                          \PY{n}{x} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{loc}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{n}{thisModel}\PY{p}{]}
          
                          \PY{k}{if} \PY{n+nb}{len}\PY{p}{(}\PY{n}{x}\PY{o}{.}\PY{n}{columns}\PY{p}{)} \PY{o}{==} \PY{l+m+mi}{1}\PY{p}{:}
                              \PY{n}{linregCVScores} \PY{o}{=} \PY{n}{cross\PYZus{}val\PYZus{}score}\PY{p}{(}\PY{n}{Ridge}\PY{p}{(}\PY{n}{alpha}\PY{o}{=}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{alpha}\PY{p}{)}\PY{p}{,}\PY{n}{x}\PY{o}{.}\PY{n}{values}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}\PY{n}{y}\PY{p}{,}\PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{neg\PYZus{}mean\PYZus{}squared\PYZus{}error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}
                          \PY{k}{else}\PY{p}{:}
                              \PY{n}{linregCVScores} \PY{o}{=} \PY{n}{cross\PYZus{}val\PYZus{}score}\PY{p}{(}\PY{n}{Ridge}\PY{p}{(}\PY{n}{alpha}\PY{o}{=}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{alpha}\PY{p}{)}\PY{p}{,}\PY{n}{x}\PY{p}{,}\PY{n}{y}\PY{p}{,}\PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{neg\PYZus{}mean\PYZus{}squared\PYZus{}error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}
          
                      \PY{k}{if} \PY{o+ow}{not} \PY{n}{bestMeanCV}\PY{p}{:}
                          \PY{n}{bestMeanCV} \PY{o}{=} \PY{n}{linregCVScores}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}
                          \PY{n}{bestPredictor} \PY{o}{=} \PY{n}{i}
                      \PY{k}{elif} \PY{n}{bestMeanCV} \PY{o}{\PYZlt{}} \PY{n}{linregCVScores}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}\PY{p}{:}
                          \PY{n}{bestMeanCV} \PY{o}{=} \PY{n}{linregCVScores}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}
                          \PY{n}{bestPredictor} \PY{o}{=} \PY{n}{i}
          
                      \PY{k}{if} \PY{n}{bestPredictor} \PY{o+ow}{not} \PY{o+ow}{in} \PY{n}{columnsArray}\PY{p}{:}
                          \PY{k}{break}
          
                      \PY{n}{columnsArray} \PY{o}{=} \PY{n}{columnsArray}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{n}{bestPredictor}\PY{p}{)}
                      \PY{n}{bestMeanCVModel}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{bestPredictor}\PY{p}{)}
                      \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{ was added with test MSE }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{bestMeanCVModel}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{n}{bestMeanCV}\PY{p}{)}\PY{p}{)}
          
          
                  \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{bestMeanCVModel} \PY{o}{=} \PY{n}{bestMeanCVModel}
                  \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{bestMeanCV} \PY{o}{=} \PY{n}{bestMeanCV}
                  \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{The final best model is }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{ and its TEST MSE is }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{bestMeanCVModel}\PY{p}{,}\PY{n}{bestMeanCV}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}151}]:} \PY{n}{X} \PY{o}{=} \PY{n}{housePrice}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SalePrice}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{[}\PY{n}{bestMeanCVModel}\PY{p}{]}
          
          \PY{c+c1}{\PYZsh{} y is the response variable}
          \PY{n}{y} \PY{o}{=} \PY{n}{housePrice}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SalePrice}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
          
          \PY{n}{rm} \PY{o}{=} \PY{n}{RidgeRegression}\PY{p}{(}\PY{n}{alpha}\PY{o}{=}\PY{l+m+mi}{6}\PY{p}{)}
          \PY{n}{rm}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X}\PY{p}{,}\PY{n}{y}\PY{p}{)}
          \PY{n}{rm}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{n}{X}\PY{p}{,}\PY{n}{y}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
         Features          coef        std err       t  P > |t|
0           Const -1.114269e+06  122052.144169  -9.129    0.000
1     OverallQual  1.748152e+04    1156.816121  15.112    0.000
2       GrLivArea  3.974460e+01       4.039278   9.840    0.000
3      BsmtFinSF1  1.674920e+01       2.423958   6.910    0.000
4      GarageCars  1.137126e+04    1714.214337   6.634    0.000
5    YearRemodAdd  1.936712e+02      65.705489   2.948    0.003
6         LotArea  5.009000e-01       0.101138   4.953    0.000
7      MasVnrArea  2.895370e+01       5.961464   4.857    0.000
8    KitchenAbvGr -2.149685e+04    4778.310109  -4.499    0.000
9        1stFlrSF  1.827980e+01       3.376924   5.413    0.000
10      YearBuilt  3.380716e+02      56.397957   5.994    0.000
11    OverallCond  4.390405e+03    1020.643324   4.302    0.000
12    ScreenPorch  5.493510e+01      17.324966   3.171    0.002
13     WoodDeckSF  2.665780e+01       8.007293   3.329    0.001
14   TotRmsAbvGrd  5.626965e+03    1243.307126   4.526    0.000
15   BedroomAbvGr -8.289604e+03    1674.721749  -4.950    0.000
16  EnclosedPorch  1.892600e+01      16.993798   1.114    0.266
17       PoolArea -3.721990e+01      23.832283  -1.562    0.119
18     Fireplaces  3.096515e+03    1761.469087   1.758    0.079

    \end{Verbatim}

    \hypertarget{ridge-regression-and-multicollinearity}{%
\subsubsection{Ridge Regression and
Multicollinearity}\label{ridge-regression-and-multicollinearity}}

    Steps:

Create data and 2 variables to create a multicollinear problem

Split the data into train and test

Show using statsmodels.api that we have multicollinearity

Linear Regression

Show cross-validated mean squared error

Train on training dataset and show mean squared error on test dataset

Ridge Regression

Show cross-validated mean squared error

Find the value of alpha in Ridge Regression which minimises MSE. This
should better than Linear Regression

Compare coefficients between Linear Regression and Ridge Regression

Compare summary outputs between Ridge Regression and Linear Regression

Predict a new set of points and find MSE for Linear Regression and Ridge
Regression

Confirm that the standard deviation of predictions is smaller for Ridge
Regression than for Linear Regression

Apply Ridge Regression and Linear Regression to the housing dataset and
confirm that the MSE and prediction standard deviation are smaller for
Ridge Regression

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}152}]:} \PY{c+c1}{\PYZsh{} Create data and 2 variables to create a multicollinear problem}
          
          \PY{k}{def} \PY{n+nf}{f}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{:}
              \PY{k}{return} \PY{l+m+mf}{4.67} \PY{o}{+} \PY{l+m+mi}{507}\PY{o}{*}\PY{n}{x}
          
          \PY{c+c1}{\PYZsh{} Set the seed}
          \PY{n}{r} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{RandomState}\PY{p}{(}\PY{l+m+mi}{101}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Choose 1000 random observations for x between 0 and 100}
          \PY{n}{X1} \PY{o}{=} \PY{l+m+mi}{100}\PY{o}{*}\PY{n}{r}\PY{o}{.}\PY{n}{rand}\PY{p}{(}\PY{l+m+mi}{1000}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Create a correlated variable}
          \PY{n}{X2} \PY{o}{=} \PY{n}{X1} \PY{o}{+} \PY{l+m+mi}{100} \PY{o}{+} \PY{l+m+mi}{1}\PY{o}{*}\PY{n}{r}\PY{o}{.}\PY{n}{randn}\PY{p}{(}\PY{l+m+mi}{1000}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{}Error term with sigma = 100, mu = 0, randn samples from the standard normal distribution}
          \PY{n}{E} \PY{o}{=} \PY{l+m+mi}{20000}\PY{o}{*}\PY{n}{r}\PY{o}{.}\PY{n}{randn}\PY{p}{(}\PY{l+m+mi}{1000}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{}Response variables}
          \PY{n}{Y} \PY{o}{=} \PY{n+nb}{list}\PY{p}{(}\PY{n+nb}{map}\PY{p}{(}\PY{n}{f}\PY{p}{,}\PY{n}{X1}\PY{p}{)}\PY{p}{)}\PY{o}{+}\PY{n}{E}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}153}]:} \PY{c+c1}{\PYZsh{} Plot}
          \PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,}\PY{l+m+mi}{8}\PY{p}{)}\PY{p}{)}
          \PY{n}{axes} \PY{o}{=} \PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}axes}\PY{p}{(}\PY{p}{[}\PY{l+m+mf}{0.1}\PY{p}{,}\PY{l+m+mf}{0.1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
          \PY{n}{axes}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{X1}\PY{p}{,}\PY{n}{Y}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{.}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Set labels and title}
          \PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{x}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          \PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{f(x)}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          \PY{n}{axes}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Scatter plot of f}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          
          \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_161_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}154}]:} \PY{c+c1}{\PYZsh{} Split the data into train test}
          
          \PY{n}{X} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{concat}\PY{p}{(}\PY{p}{[}\PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{X1}\PY{p}{,}\PY{n}{columns}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{p}{,}\PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{X2}\PY{p}{,}\PY{n}{columns}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X2}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{p}{]}\PY{p}{,}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
          \PY{n}{X\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}test}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}test} \PY{o}{=} \PY{n}{train\PYZus{}test\PYZus{}split}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{Y}\PY{p}{,} \PY{n}{test\PYZus{}size}\PY{o}{=}\PY{l+m+mf}{0.33}\PY{p}{,} \PY{n}{random\PYZus{}state}\PY{o}{=}\PY{l+m+mi}{42}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}155}]:} \PY{c+c1}{\PYZsh{} Show correlation}
          \PY{n}{X\PYZus{}train}\PY{o}{.}\PY{n}{corr}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}155}]:}           X1        X2
          X1  1.000000  0.999391
          X2  0.999391  1.000000
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}156}]:} \PY{c+c1}{\PYZsh{} Show using statsmodels.api that we have multicollinearity}
          
          \PY{c+c1}{\PYZsh{} add a column of ones to X}
          \PY{n}{X\PYZus{}new} \PY{o}{=} \PY{n}{sm}\PY{o}{.}\PY{n}{add\PYZus{}constant}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} ordinary least squares approach to optimisation}
          \PY{n}{est} \PY{o}{=} \PY{n}{sm}\PY{o}{.}\PY{n}{OLS}\PY{p}{(}\PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{X\PYZus{}new}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} fit the data to the model using OLS}
          \PY{n}{est2} \PY{o}{=} \PY{n}{est}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} print a summary of the model}
          \PY{n+nb}{print}\PY{p}{(}\PY{n}{est2}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.365
Model:                            OLS   Adj. R-squared:                  0.363
Method:                 Least Squares   F-statistic:                     191.9
Date:                Sat, 27 Jul 2019   Prob (F-statistic):           1.43e-66
Time:                        18:49:08   Log-Likelihood:                -7616.5
No. Observations:                 670   AIC:                         1.524e+04
Df Residuals:                     667   BIC:                         1.525e+04
Df Model:                           2                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const      -3.352e+04    8.1e+04     -0.414      0.679   -1.93e+05    1.26e+05
X1           215.3056    808.737      0.266      0.790   -1372.672    1803.283
X2           337.7488    808.747      0.418      0.676   -1250.248    1925.746
==============================================================================
Omnibus:                        6.860   Durbin-Watson:                   1.933
Prob(Omnibus):                  0.032   Jarque-Bera (JB):                9.425
Skew:                           0.051   Prob(JB):                      0.00898
Kurtosis:                       3.572   Cond. No.                     1.63e+04
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 1.63e+04. This might indicate that there are
strong multicollinearity or other numerical problems.

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
C:\textbackslash{}Users\textbackslash{}HVAD\textbackslash{}Anaconda3\textbackslash{}lib\textbackslash{}site-packages\textbackslash{}numpy\textbackslash{}core\textbackslash{}fromnumeric.py:52: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.
  return getattr(obj, method)(*args, **kwds)

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}157}]:} \PY{c+c1}{\PYZsh{} LINEAR REGRESSION}
          \PY{c+c1}{\PYZsh{} Show cross\PYZhy{}validated mean squared error}
          
          \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{cross\PYZus{}val\PYZus{}score}\PY{p}{(}\PY{n}{LinearRegression}\PY{p}{(}\PY{p}{)}\PY{p}{,}\PY{n}{X\PYZus{}train}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{,}\PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{neg\PYZus{}mean\PYZus{}squared\PYZus{}error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}157}]:} -443116270.3419094
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}158}]:} \PY{c+c1}{\PYZsh{} Train on training set and show mean squared error on test set}
          
          \PY{n}{lm} \PY{o}{=} \PY{n}{LinearRegression}\PY{p}{(}\PY{p}{)}
          \PY{n}{lm}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{)}
          \PY{p}{(}\PY{n}{lm}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{y\PYZus{}test}\PY{p}{)}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{lm}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{y\PYZus{}test}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}158}]:} 139251509589.51065
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}159}]:} \PY{c+c1}{\PYZsh{} RIDGE REGRESSION}
          \PY{c+c1}{\PYZsh{} Show cross\PYZhy{}validated mean squared error}
          
          \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{cross\PYZus{}val\PYZus{}score}\PY{p}{(}\PY{n}{RidgeRegression}\PY{p}{(}\PY{n}{alpha}\PY{o}{=}\PY{l+m+mi}{1000}\PY{p}{)}\PY{p}{,}\PY{n}{X\PYZus{}train}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{,}\PY{n}{scoring}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{neg\PYZus{}mean\PYZus{}squared\PYZus{}error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{cv}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}159}]:} -441532513.0246685
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}160}]:} \PY{c+c1}{\PYZsh{} Find the value of alpha in Ridge Regression which minimises MSE}
          \PY{n}{rmtest} \PY{o}{=} \PY{n}{RidgeRegression}\PY{p}{(}\PY{p}{)}
          \PY{n}{rmtest}\PY{o}{.}\PY{n}{findBestAlpha}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Ridge Regression MSE = -441146916.2805335, best alpha = 11468.420305810927

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}161}]:} \PY{n}{bestAlpha} \PY{o}{=} \PY{n}{rmtest}\PY{o}{.}\PY{n}{alpha}
          \PY{n}{bestAlpha}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}161}]:} 11468.420305810927
\end{Verbatim}
            
    This is an improvement over Linear Regression. Let's look at the
differences in the parameter estimations of the two models

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}162}]:} \PY{c+c1}{\PYZsh{} Compare coefficients between Linear Regression and Ridge Regression}
          \PY{n}{lm} \PY{o}{=} \PY{n}{LinearRegression}\PY{p}{(}\PY{p}{)}
          \PY{n}{lm}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{)}
          \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Parameters of the linear regression model = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{p}{[}\PY{n}{lm}\PY{o}{.}\PY{n}{intercept\PYZus{}}\PY{p}{,}\PY{n}{lm}\PY{o}{.}\PY{n}{coef\PYZus{}}\PY{p}{]}\PY{p}{)}\PY{p}{)}
          
          \PY{n}{rm} \PY{o}{=} \PY{n}{RidgeRegression}\PY{p}{(}\PY{n}{alpha}\PY{o}{=}\PY{n}{bestAlpha}\PY{p}{)}
          \PY{n}{rm}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{)}
          \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Parameters of the linear regression model = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{p}{[}\PY{n}{rm}\PY{o}{.}\PY{n}{intercept\PYZus{}}\PY{p}{,}\PY{n}{rm}\PY{o}{.}\PY{n}{coef\PYZus{}}\PY{p}{]}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Parameters of the linear regression model = [-33517.19098806189, array([215.30558151, 337.74881039])]
Parameters of the linear regression model = [-26991.170068539966, array([271.94186319, 275.42897954])]

    \end{Verbatim}

    Now let's look at the standard deviations of the parameters of the Ridge
Regression model and compare them to the Linear Regression output from
the statsmodels.api output

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}163}]:} \PY{c+c1}{\PYZsh{} Compare summary outputs between Ridge Regression and Linear Regression}
          \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{RIDGE REGRESSION SUMMARY}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          \PY{n}{rm}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{)}
          
          \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s1}{LINEAR REGRESSION SUMMARY}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
          \PY{n}{rmlm} \PY{o}{=} \PY{n}{RidgeRegression}\PY{p}{(}\PY{n}{alpha}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
          \PY{n}{rmlm}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{)}
          \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Parameters of the linear regression model = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{p}{[}\PY{n}{rmlm}\PY{o}{.}\PY{n}{intercept\PYZus{}}\PY{p}{,}\PY{n}{rmlm}\PY{o}{.}\PY{n}{coef\PYZus{}}\PY{p}{]}\PY{p}{)}\PY{p}{)}
          \PY{n}{rmlm}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{n}{X\PYZus{}train}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
RIDGE REGRESSION SUMMARY
  Features        coef       std err      t  P > |t|
0    Const -26991.1701  80996.557139 -0.333    0.739
1       X1    271.9419    808.765278  0.336    0.737
2       X2    275.4290    808.775094  0.341    0.734


LINEAR REGRESSION SUMMARY
Parameters of the linear regression model = [-33517.19098807497, array([215.30558151, 337.74881039])]
  Features        coef       std err      t  P > |t|
0    Const -33517.1910  80993.767760 -0.414    0.679
1       X1    215.3056    808.737425  0.266    0.790
2       X2    337.7488    808.747241  0.418    0.676

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}164}]:} \PY{c+c1}{\PYZsh{} Predict a new set of points and find MSE for Linear Regression and Ridge Regression}
          
          \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Linear Regression MSE = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{p}{(}\PY{n}{rmlm}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{y\PYZus{}test}\PY{p}{)}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{rmlm}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{)}\PY{p}{)}
          \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Ridge Regression MSE = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{p}{(}\PY{n}{rm}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{y\PYZus{}test}\PY{p}{)}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{rm}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Linear Regression MSE = 139251509589.5106
Ridge Regression MSE = 139075651590.56067

    \end{Verbatim}

    And how much do the predictions vary

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}165}]:} \PY{c+c1}{\PYZsh{} Confirm that the standard deviation of predictions is smaller for Ridge Regression than for Linear Regression}
          
          \PY{n}{rmpredictions} \PY{o}{=} \PY{p}{[}\PY{p}{]}
          \PY{k}{for} \PY{n}{i}\PY{p}{,}\PY{n}{j} \PY{o+ow}{in} \PY{n+nb}{list}\PY{p}{(}\PY{n+nb}{zip}\PY{p}{(}\PY{n+nb}{list}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{o}{.}\PY{n}{values}\PY{p}{)}\PY{p}{,}\PY{n+nb}{list}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{)}\PY{p}{)}\PY{p}{:}
              \PY{n}{rmpredictions}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{rm}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{i}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{)}
              
          \PY{n}{rmlmpredictions} \PY{o}{=} \PY{p}{[}\PY{p}{]}
          \PY{k}{for} \PY{n}{i}\PY{p}{,}\PY{n}{j} \PY{o+ow}{in} \PY{n+nb}{list}\PY{p}{(}\PY{n+nb}{zip}\PY{p}{(}\PY{n+nb}{list}\PY{p}{(}\PY{n}{X\PYZus{}test}\PY{o}{.}\PY{n}{values}\PY{p}{)}\PY{p}{,}\PY{n+nb}{list}\PY{p}{(}\PY{n}{y\PYZus{}test}\PY{p}{)}\PY{p}{)}\PY{p}{)}\PY{p}{:}
              \PY{n}{rmlmpredictions}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{rmlm}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{i}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{)}
              
          \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Standard deviation of Linear Regression predictions = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{std}\PY{p}{(}\PY{n}{rmlmpredictions}\PY{p}{)}\PY{p}{)}\PY{p}{)}
          \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Standard deviation of Ridge Regression predictions = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{std}\PY{p}{(}\PY{n}{rmpredictions}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Standard deviation of Linear Regression predictions = 15687.352084606513
Standard deviation of Ridge Regression predictions = 15526.206678003531

    \end{Verbatim}

    \hypertarget{summary-steps}{%
\subsubsection{Summary steps}\label{summary-steps}}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}167}]:} \PY{c+c1}{\PYZsh{} The local module containing our custom RidgeRegression class}
          \PY{k+kn}{import} \PY{n+nn}{Regression}
          
          \PY{c+c1}{\PYZsh{} Predictor variables}
          \PY{n}{X} \PY{o}{=} \PY{n}{housePrice}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SalePrice}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Response variable}
          \PY{n}{y} \PY{o}{=} \PY{n}{housePrice}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SalePrice}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
          
          \PY{c+c1}{\PYZsh{} Create a RidgeRegression object}
          \PY{n}{rm} \PY{o}{=} \PY{n}{Regression}\PY{o}{.}\PY{n}{RidgeRegression}\PY{p}{(}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Run the feature selection method as above to limit to the most important variables. }
          \PY{c+c1}{\PYZsh{} Stores the order of variables to the object}
          \PY{n}{rm}\PY{o}{.}\PY{n}{featureSelection}\PY{p}{(}\PY{n}{X}\PY{p}{,}\PY{n}{y}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Find the value of alpha minimising MSE for these features}
          \PY{n}{rm}\PY{o}{.}\PY{n}{findBestAlpha}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{n}{rm}\PY{o}{.}\PY{n}{bestMeanCVModel}\PY{p}{]}\PY{p}{,}\PY{n}{y}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Print the alpha}
          \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Alpha = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{rm}\PY{o}{.}\PY{n}{alpha}\PY{p}{)}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Fit the model with the best features and the alpha}
          \PY{n}{rm}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{n}{rm}\PY{o}{.}\PY{n}{bestMeanCVModel}\PY{p}{]}\PY{p}{,}\PY{n}{y}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Display a summary including p\PYZhy{}values}
          \PY{n}{rm}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{n}{rm}\PY{o}{.}\PY{n}{bestMeanCVModel}\PY{p}{]}\PY{p}{,}\PY{n}{y}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
OverallQual was added with test neg\_mean\_squared\_error -2371256644.090804
GrLivArea was added with test neg\_mean\_squared\_error -1821344525.7839134
BsmtFinSF1 was added with test neg\_mean\_squared\_error -1653409803.8064983
GarageCars was added with test neg\_mean\_squared\_error -1522589407.420225
YearRemodAdd was added with test neg\_mean\_squared\_error -1477509838.9287422
LotArea was added with test neg\_mean\_squared\_error -1445261661.6914485
MasVnrArea was added with test neg\_mean\_squared\_error -1418246177.6939814
KitchenAbvGr was added with test neg\_mean\_squared\_error -1399426759.141863
1stFlrSF was added with test neg\_mean\_squared\_error -1376770121.511752
YearBuilt was added with test neg\_mean\_squared\_error -1366717179.473287
OverallCond was added with test neg\_mean\_squared\_error -1351971584.6744504
ScreenPorch was added with test neg\_mean\_squared\_error -1346298056.478502
WoodDeckSF was added with test neg\_mean\_squared\_error -1339217526.5940492
TotRmsAbvGrd was added with test neg\_mean\_squared\_error -1334746769.416609
BedroomAbvGr was added with test neg\_mean\_squared\_error -1315857153.2608094
EnclosedPorch was added with test neg\_mean\_squared\_error -1315239600.5930305
PoolArea was added with test neg\_mean\_squared\_error -1314381817.375801
Fireplaces was added with test neg\_mean\_squared\_error -1314367411.2780588
The final best model is ['OverallQual', 'GrLivArea', 'BsmtFinSF1', 'GarageCars', 'YearRemodAdd', 'LotArea', 'MasVnrArea', 'KitchenAbvGr', '1stFlrSF', 'YearBuilt', 'OverallCond', 'ScreenPorch', 'WoodDeckSF', 'TotRmsAbvGrd', 'BedroomAbvGr', 'EnclosedPorch', 'PoolArea', 'Fireplaces'] and its TEST neg\_mean\_squared\_error is -1314367411.2780588
Ridge Regression neg\_mean\_squared\_error = -1314242814.4290702, best alpha = 5.0823069858551015
Alpha = 5.0823069858551015
         Features          coef        std err       t  P > |t|
0           Const -1.112135e+06  122049.345972  -9.112    0.000
1     OverallQual  1.747749e+04    1156.789600  15.109    0.000
2       GrLivArea  3.970460e+01       4.039186   9.830    0.000
3      BsmtFinSF1  1.674660e+01       2.423903   6.909    0.000
4      GarageCars  1.139215e+04    1714.175037   6.646    0.000
5    YearRemodAdd  1.932458e+02      65.703982   2.941    0.003
6         LotArea  5.006000e-01       0.101136   4.950    0.000
7      MasVnrArea  2.893470e+01       5.961328   4.854    0.000
8    KitchenAbvGr -2.181997e+04    4778.200560  -4.567    0.000
9        1stFlrSF  1.829860e+01       3.376846   5.419    0.000
10      YearBuilt  3.375494e+02      56.396664   5.985    0.000
11    OverallCond  4.386581e+03    1020.619925   4.298    0.000
12    ScreenPorch  5.485660e+01      17.324569   3.166    0.002
13     WoodDeckSF  2.661620e+01       8.007110   3.324    0.001
14   TotRmsAbvGrd  5.658864e+03    1243.278622   4.552    0.000
15   BedroomAbvGr -8.310760e+03    1674.683354  -4.963    0.000
16  EnclosedPorch  1.886360e+01      16.993409   1.110    0.267
17       PoolArea -3.720020e+01      23.831736  -1.561    0.119
18     Fireplaces  3.081496e+03    1761.428703   1.749    0.080

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}168}]:} \PY{c+c1}{\PYZsh{} The local module containing our custom RidgeRegression class}
          \PY{k+kn}{from} \PY{n+nn}{model\PYZus{}arena} \PY{k}{import} \PY{n}{RidgeRegression}
          
          \PY{c+c1}{\PYZsh{} Predictor variables}
          \PY{n}{X} \PY{o}{=} \PY{n}{housePrice}\PY{o}{.}\PY{n}{drop}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SalePrice}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Response variable}
          \PY{n}{y} \PY{o}{=} \PY{n}{housePrice}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{SalePrice}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
          
          \PY{c+c1}{\PYZsh{} Create a RidgeRegression object}
          \PY{n}{rm} \PY{o}{=} \PY{n}{RidgeRegression}\PY{p}{(}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Run the feature selection method as above to limit to the most important variables. }
          \PY{c+c1}{\PYZsh{} Stores the order of variables to the object}
          \PY{n}{rm}\PY{o}{.}\PY{n}{featureSelection}\PY{p}{(}\PY{n}{X}\PY{p}{,}\PY{n}{y}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Find the value of alpha minimising MSE for these features}
          \PY{n}{rm}\PY{o}{.}\PY{n}{findBestAlpha}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{n}{rm}\PY{o}{.}\PY{n}{bestMeanCVModel}\PY{p}{]}\PY{p}{,}\PY{n}{y}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Print the alpha}
          \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Alpha = }\PY{l+s+si}{\PYZob{}\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{o}{.}\PY{n}{format}\PY{p}{(}\PY{n}{rm}\PY{o}{.}\PY{n}{alpha}\PY{p}{)}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Fit the model with the best features and the alpha}
          \PY{n}{rm}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{n}{rm}\PY{o}{.}\PY{n}{bestMeanCVModel}\PY{p}{]}\PY{p}{,}\PY{n}{y}\PY{p}{)}
          
          \PY{c+c1}{\PYZsh{} Display a summary including p\PYZhy{}values}
          \PY{n}{rm}\PY{o}{.}\PY{n}{summary}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{n}{rm}\PY{o}{.}\PY{n}{bestMeanCVModel}\PY{p}{]}\PY{p}{,}\PY{n}{y}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
OverallQual was added with test MSE 2371254029.0760694
GrLivArea was added with test MSE 1821363518.5244935
BsmtFinSF1 was added with test MSE 1653488270.7651806
GarageCars was added with test MSE 1522674235.152098
YearRemodAdd was added with test MSE 1477543064.7221112
LotArea was added with test MSE 1445288001.4210088
MasVnrArea was added with test MSE 1418272851.0154374
KitchenAbvGr was added with test MSE 1399503544.1065402
1stFlrSF was added with test MSE 1376798534.1186767
YearBuilt was added with test MSE 1366723443.0781755
OverallCond was added with test MSE 1351925127.2305455
ScreenPorch was added with test MSE 1346240217.2577364
WoodDeckSF was added with test MSE 1339095696.4405167
TotRmsAbvGrd was added with test MSE 1334732891.2793584
BedroomAbvGr was added with test MSE 1315808790.2185073
EnclosedPorch was added with test MSE 1315182547.8506525
PoolArea was added with test MSE 1314370013.4542491
Fireplaces was added with test MSE 1314248045.3630013
The final best model is ['OverallQual', 'GrLivArea', 'BsmtFinSF1', 'GarageCars', 'YearRemodAdd', 'LotArea', 'MasVnrArea', 'KitchenAbvGr', '1stFlrSF', 'YearBuilt', 'OverallCond', 'ScreenPorch', 'WoodDeckSF', 'TotRmsAbvGrd', 'BedroomAbvGr', 'EnclosedPorch', 'PoolArea', 'Fireplaces'] and its TEST MSE is 1314248045.3630013
Ridge Regression MSE = -1314242814.4290702, best alpha = 5.0823069858551015
Alpha = 5.0823069858551015
         Features          coef        std err       t  P > |t|
0           Const -1.112135e+06  122049.345972  -9.112    0.000
1     OverallQual  1.747749e+04    1156.789600  15.109    0.000
2       GrLivArea  3.970460e+01       4.039186   9.830    0.000
3      BsmtFinSF1  1.674660e+01       2.423903   6.909    0.000
4      GarageCars  1.139215e+04    1714.175037   6.646    0.000
5    YearRemodAdd  1.932458e+02      65.703982   2.941    0.003
6         LotArea  5.006000e-01       0.101136   4.950    0.000
7      MasVnrArea  2.893470e+01       5.961328   4.854    0.000
8    KitchenAbvGr -2.181997e+04    4778.200560  -4.567    0.000
9        1stFlrSF  1.829860e+01       3.376846   5.419    0.000
10      YearBuilt  3.375494e+02      56.396664   5.985    0.000
11    OverallCond  4.386581e+03    1020.619925   4.298    0.000
12    ScreenPorch  5.485660e+01      17.324569   3.166    0.002
13     WoodDeckSF  2.661620e+01       8.007110   3.324    0.001
14   TotRmsAbvGrd  5.658864e+03    1243.278622   4.552    0.000
15   BedroomAbvGr -8.310760e+03    1674.683354  -4.963    0.000
16  EnclosedPorch  1.886360e+01      16.993409   1.110    0.267
17       PoolArea -3.720020e+01      23.831736  -1.561    0.119
18     Fireplaces  3.081496e+03    1761.428703   1.749    0.080

    \end{Verbatim}

    \hypertarget{appendix}{%
\subsection{Appendix}\label{appendix}}

    \hypertarget{a1---2n-2-sum_i1n-x_i2---2-sum_i1n-x_i2-0-for-non-trivial-x}{%
\subsubsection{\texorpdfstring{A1 -
\((2n) (2 \sum_{i=1}^n x_i^2) - (2 \sum_{i=1}^n x_i)^2 > 0\) for
non-trivial
X}{A1 - (2n) (2 \textbackslash{}sum\_\{i=1\}\^{}n x\_i\^{}2) - (2 \textbackslash{}sum\_\{i=1\}\^{}n x\_i)\^{}2 \textgreater{} 0 for non-trivial X}}\label{a1---2n-2-sum_i1n-x_i2---2-sum_i1n-x_i2-0-for-non-trivial-x}}

Statement:
\((2n) (2 \sum_{i=1}^n x_i^2) - (2 \sum_{i=1}^n x_i)^2 > 0 \; \forall \; n > 1\)

Proof: We prove this by induction on \(n\). If n = 1, we have
\((2n) (2 \sum_{i=1}^n x_i^2) - (2 \sum_{i=1}^n x_i)^2 = 0\), but this
is not what we want.

Let n = 2 \textgreater{} 1. Then

\[(2n) (2 \sum_{i=1}^n x_i^2) - (2 \sum_{i=1}^n x_i)^2 = 2 x_1^2 + 2 x_2^2 - (x_1 + x_2)^2\]

\[= 2 x_1^2 + 2 x_2^2 - x_1^2 - x_2^2 - 2x_1 x_2 = x_1^2 + x_2^2 - 2x_1 x_2 = (x_1 - x_2)^2  > 0\]

So we have proved the assertion for n = 2.

Let us prove the statement for n+1 assuming it is true for n.

i.e.~Assume \(n \sum_{i=1}^n x_i^2 - (\sum_{i=1}^n x_i)^2 > 0\)

Then

\[(n+1) \sum_{i=1}^{n+1} x_i^2 - (\sum_{i=1}^{n+1} x_i)^2 = (n+1)[\sum_{i=1}^{n} x_i^2 + x_{n+1}^2] - (\sum_{i=1}^{n} x_i + x_{n+1})^2\]

\[= [n \sum_{i=1}^n x_i^2 + \sum_{i=1}^n x_i^2 + (n+1)x_{n+1}^2] - (\sum_{i=1}^n x_i)^2 - x_{n+1}^2 + 2x_{n+1} \sum_{i=1}^n x_i\]

\[= n \sum_{i=1}^n x_i^2 - (\sum_{i=1}^n x_i)^2 + \sum_{i=1}^n x_i^2 + (n+1)x_{n+1}^2 - x_{n+1}^2 + 2x_{n+1} \sum_{i=1}^n x_i\]

by the assumption for n we have

\[> \sum_{i=1}^n x_i^2 + (n+1)x_{n+1}^2 - x_{n+1}^2 + 2x_{n+1} \sum_{i=1}^n x_i\]

by the assumption for n that
\(\sum_{i=1}^n x_i^2 > \frac{1}{n}(\sum_{i=1}^n x_i)^2\) we have

\[> \frac{1}{n}(\sum_{i=1}^n x_i)^2 + (n+1)x_{n+1}^2 - x_{n+1}^2 + 2x_{n+1} \sum_{i=1}^n x_i =\frac{1}{n}(\sum_{i=1}^n x_i)^2 + nx_{n+1}^2 + 2x_{n+1} \sum_{i=1}^n x_i \]

\[= \frac{1}{n}[(\sum_{i=1}^n x_i)^2 + n^2 x_{n+1}^2 + 2n x_{n+1} \sum_{i=1}^n x_i]\]

\[=\frac{1}{n}\left( \sum_{i=1}^n x_i + n x_{n+1} \right)^2 > 0\]

This proves the statement. This assumes that at least one \(X_i\) is
non-zero.

    \hypertarget{a2---maximum-likelihood-estimation-mle}{%
\subsubsection{A2 - Maximum Likelihood Estimation
(MLE)}\label{a2---maximum-likelihood-estimation-mle}}

Let's assume that there is a linear relationship between the response
and predictor variables and that any discrepency is due to random noise,
this is expressed as

\[Y_i = \beta_0 + \beta_1 X_i + \epsilon_i\]

where the errors are normally distributed,
\(\epsilon \sim N(0,\sigma^2)\). Then, the response variable given the
data are normally distributed

\[Y_i|X_i \sim N(\beta_0 + \beta_1 X_i,\sigma^2)\]

where the mean or expectation is

\[E \left[  Y_i|X_i  \right] = E[\beta_0 + \beta_1 X_i + \epsilon_i] = E[\beta_0] + E[\beta_1 X_i] + E[\epsilon_i] = \beta_0 + \beta_1 X_i\]

The probability density function for \(Y_i\) is then

\[P(Y_i=y_i|X_i) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left( -\frac{1}{2\sigma^2} [y_i - (\beta_0 + \beta_1 x_i) ]^2 \right)\]

Then, if the \(Y_i\) observations are independent of each other, we have
that the likelihood of \(\beta = (\beta_0,\beta_1)\) (the probability of
observing this data given these parameters) is

\[L(\beta) = P(Y|\beta,X) = P(Y_1=y_1,Y_2=y_2,...,Y_n=y_n|\beta,X) = P(Y_1=y_1|\beta,X_1)P(Y_2=y_2|\beta,X_2)...,P(Y_n=y_n|\beta,X_n)\]

where the last equality is due to the independence of each observation
and that \(Y_i\) is only dependent on \(\beta\) and \(X_i\). Using the
probability density function above, this becomes

\[L(\beta) = \prod_{i=1}^n \frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left( -\frac{1}{2\sigma^2} [y_i - (\beta_0 + \beta_1 x_i) ]^2 \right) = \left( \frac{1}{\sqrt{2 \pi \sigma^2}} \right)^n \exp \left( -\frac{1}{2\sigma^2} \sum_{i=1}^n [y_i - (\beta_0 + \beta_1 x_i) ]^2 \right)\]

Therefore, maximising this function with respect to \(\beta\),
corresponds to finding values for \(\beta\) which maximises the
probability of obtaining this response data given the predictor data.
Instead of working with this equation as it stands, we note that the
right hand side of the above equation is positive for all values of
\(\beta\) and \(x_i\). This means that we can apply a handy trick in
that since the \(\log\) function is a monotonically increasing function,
maximising \(\log(L(\beta))\) is the same as maximising \(L(\beta)\).
Due to the existence of \(\exp\) in \(L(\beta)\), we may choose the
natural logarithm so that the exponential disappears (we will still
denote this as \(\log\)).

\[l(\beta) = \log(L(\beta)) = \log \left(\left( \frac{1}{\sqrt{2 \pi \sigma^2}} \right)^n \right) -\frac{1}{2\sigma^2} \sum_{i=1}^n [y_i - (\beta_0 + \beta_1 x_i) ]^2\]

Since the first term on the right-hand side is indifferent to the choice
of \(\beta\), maximising \(l(\beta)\) corresponds to maximising the last
term on the right-hand side

\[\max_{\beta} l(\beta) = \max_{\beta} \left( - \frac{1}{2\sigma^2} \sum_{i=1}^n [y_i - (\beta_0 + \beta_1 x_i) ]^2 \right)\]

which is equivalent to

\[\max_{\beta} l(\beta) = \min_{\beta} \left( \sum_{i=1}^n [y_i - (\beta_0 + \beta_1 x_i) ]^2 \right) = \min_{\beta} RSS\]

where \(RSS = \sum_{i=1}^n \epsilon_i^2\). Note that for multiple
predictors (\(p\) predictors), the above becomes

\[\max_{\beta} l(\beta) = \min_{\beta} \left( \sum_{i=1}^n \left[y_i - \left(\beta_0 + \sum_{j=1}^p \beta_j x_{ij} \right) \right]^2 \right) = \min_{\beta} RSS\]

where \(x_{ij}\) is the \(j^{th}\) predictor for observation \(i\).

    \hypertarget{a3---the-mean-point-barxbary-lies-on-the-linear-regression-line}{%
\subsubsection{\texorpdfstring{A3 - The mean point
(\(\bar{X}\),\(\bar{Y}\)) lies on the linear regression
line}{A3 - The mean point (\textbackslash{}bar\{X\},\textbackslash{}bar\{Y\}) lies on the linear regression line}}\label{a3---the-mean-point-barxbary-lies-on-the-linear-regression-line}}

Let's assume that the random variable that represents the response be
assumed to be linearly dependent on the predictors:

\[Y = \beta_0 + \beta_1 X + \epsilon\]

We approximate the coefficients using the data we have observed:

\[\hat{Y} = \hat{\beta_0} + \hat{\beta_1} X\]

Note that it is assumed that \(\beta_i\) and \(\hat{\beta}_i\) are
constant and determined such that they satisfy the line of best fit.
Taking the expectation of both sides of the above equations:

\[\mu_Y = E[Y] = E[\beta_0 + \beta_1 X + \epsilon] = E[\beta_0] + E[\beta_1 X] + E[\epsilon] = \beta_0 + \beta_1 E[X] + 0 = \beta_0 + \beta_1 \mu_X\]

\[\mu_{\hat{Y}} = E[\hat{Y}] = E[\hat{\beta_0} + \hat{\beta_1} X] = E[\hat{\beta_0}] + E[\hat{\beta_1} X] = \hat{\beta_0} + \hat{\beta_1} E[X] + 0 = \hat{\beta_0} + \hat{\beta_1} \mu_{\hat{X}}\]

The first equation above says that if we assume the linear model, then
the population means \((\mu_X,\mu_Y)\) must be a solution to this model.
The second equation says that the point
\((\mu_{\hat{X}},\mu_{\hat{Y}})\) must lie on any linear model we fit to
the data regardless of the coefficients we have chosen. Now the sample
means are easily obtained and have the exact equality below:

\[\mu_{\hat{Y}} = \bar{Y}\] \[\mu_{\hat{X}} = \bar{X}\]

This result also holds when \(\boldsymbol{X}\) is a vector of
predictors.

    \hypertarget{a4---for-a-single-predictor-r2-corxy2}{%
\subsubsection{\texorpdfstring{A4 - For a single predictor,
\(R^2 = Cor(X,Y)^2\)}{A4 - For a single predictor, R\^{}2 = Cor(X,Y)\^{}2}}\label{a4---for-a-single-predictor-r2-corxy2}}

We start with the definition of \(R^2\):

\[R^2 = \frac{ TSS - RSS }{ RSS }\]

Using \(TSS = \sum_{i=1}^n (y_i - \bar{y})^2\) and
\(RSS = \sum_{i=1}^n (y_i - \hat{y})^2\)

\[R^2 = \frac{ \sum_{i=1}^n (y_i - \bar{y})^2 - \sum_{i=1}^n (y_i - \hat{y})^2 }{ \sum_{i=1}^n (y_i - \hat{y})^2 } = \frac{ \sum_{i=1}^n [ (y_i - \bar{y}) - (y_i - \hat{y}) ][ (y_i - \bar{y}) + (y_i - \hat{y}) ] }{ \sum_{i=1}^n (y_i - \bar{y})^2 }\]

\[= \frac{ \sum_{i=1}^n [ (y_i - \bar{y}) - (y_i - \hat{y}) ][ (y_i - \bar{y}) + (y_i - \hat{y}) ] }{ \sum_{i=1}^n (y_i - \bar{y})^2 }\]

Using \(\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}\) and
\(\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i = \bar{y} - \hat{\beta}_1 \bar{x} + \hat{\beta}_1 x_i = \bar{y} -\hat{\beta}_1(\bar{x} - x_i)\)

\[R^2= \frac{ \sum_{i=1}^n [ (y_i - \bar{y}) - (y_i - \bar{y} -\hat{\beta}_1(\bar{x} - x_i)) ][ (y_i - \bar{y}) + (y_i - \bar{y} -\hat{\beta}_1(\bar{x} - x_i)) ] }{ \sum_{i=1}^n (y_i - \bar{y})^2 }\]

\[= \frac{ \sum_{i=1}^n [ \hat{\beta}_1(\bar{x} - x_i) ][ 2(y_i - \bar{y}) -\hat{\beta}_1(\bar{x} - x_i) ] }{ \sum_{i=1}^n (y_i - \bar{y})^2 }\]

\[= \frac{ \hat{\beta}_1 \left[ 2 \sum_{i=1}^n (\bar{x} - x_i)(y_i - \bar{y}) - \hat{\beta}_1\sum_{i=1}^n (\bar{x} - x_i)^2 \right] }{ \sum_{i=1}^n (y_i - \bar{y})^2 }\]

Using
\(\hat{\beta}_1 = \frac{ \sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y}) }{ \sum_{i=1}^n (x_i - \bar{x})^2 }\)

\[R^2 = \frac{ \hat{\beta}_1 \left[ 2 \sum_{i=1}^n (\bar{x} - x_i)(y_i - \bar{y}) - \sum_{i=1}^n (\bar{x} - x_i)(y_i - \bar{y}) \right] }{ \sum_{i=1}^n (y_i - \bar{y})^2 }\]

\[= \frac{ \hat{\beta}_1 \left[ \sum_{i=1}^n (\bar{x} - x_i)(y_i - \bar{y}) \right] }{ \sum_{i=1}^n (y_i - \bar{y})^2 }\]

\[= \frac{ \left[ \sum_{i=1}^n (\bar{x} - x_i)(y_i - \bar{y}) \right]^2 }{ \sum_{i=1}^n (y_i - \bar{y})^2 \sum_{i=1}^n (x_i - \bar{x})^2 }\]

\[= \frac{ \left[ \sum_{i=1}^n (\bar{x} - x_i)(y_i - \bar{y}) \right]^2 }{ \left[ \sqrt{ \sum_{i=1}^n (y_i - \bar{y})^2 \sum_{i=1}^n (x_i - \bar{x})^2} \right]^2}\]

\[= corr(X,Y)^2\]

    \hypertarget{a5---variance-of-beta_0-and-beta_1}{%
\subsubsection{\texorpdfstring{A5 - Variance of \(\beta_{0}\) and
\(\beta_{1}\)}{A5 - Variance of \textbackslash{}beta\_\{0\} and \textbackslash{}beta\_\{1\}}}\label{a5---variance-of-beta_0-and-beta_1}}

    First, note that \(y\) is a dependent variable on \(x\). This means that
any linear model and subsequently the calculations of \(\beta_{0}\) and
\(\beta_{1}\) are susceptible to a variation of \(y\) for a given \(x\)
value. Hence in the derivation of the variance of those parameters \(x\)
values are treated as a constant.

We start with the definition of \(\beta_{1}\):
\[\beta_{1}=\frac{\sum_{i=1}^{n}(x_{i}-\bar{x})(y_{i}-\bar{y})}{\sum_{i=1}^{n}(x_{i}-\bar{x})^{2}}\]

The variance of \(\beta_{1}\) is therefore given by:
\[Var(\beta_{1})=Var\left[\frac{\sum_{i=1}^{n}(x_{i}-\bar{x})(y_{i}-\bar{y})}{\sum_{i=1}^{n}(x_{i}-\bar{x})^{2}}\right]\\
=\frac{1}{\left(\sum_{i=1}^{n}(x_{i}-\bar{x})^{2}\right)^{2}}Var\bigg[\sum_{i=1}^{n}(x_{i}-\bar{x})(y_{i}-\bar{y})\bigg]\\
=\frac{1}{\left(\sum_{i=1}^{n}(x_{i}-\bar{x})^{2}\right)^{2}}Var\bigg[\sum_{i=1}^{n}(x_{i}y_{i}-x_{i}\bar{y}-\bar{x}y_{i}+\bar{x}\bar{y})\bigg]\\
=\frac{1}{\left(\sum_{i=1}^{n}(x_{i}-\bar{x})^{2}\right)^{2}}Var\bigg[\sum_{i=1}^{n} x_{i}y_{i}-\sum_{i=1}^{n} x_{i}\bar{y}-\sum_{i=1}^{n} \bar{x}y_{i}+\sum_{i=1}^{n} \bar{x}\bar{y}\bigg]\\
=\frac{1}{\left(\sum_{i=1}^{n}(x_{i}-\bar{x})^{2}\right)^{2}}Var\bigg[\sum_{i=1}^{n} x_{i}y_{i}- n \bar{x}\bar{y}-\sum_{i=1}^{n} \bar{x}y_{i}+ n \bar{x}\bar{y}\bigg]\\
=\frac{1}{\left(\sum_{i=1}^{n}(x_{i}-\bar{x})^{2}\right)^{2}}Var\bigg[\sum_{i=1}^{n}(x_{i}y_{i}-\bar{x}y_{i})\bigg]\]

As each observation is indepedent from another (\(y_{i}\) are
independent of each other) we have:

\[Var(\beta_{1})=\frac{1}{\left(\sum_{i=1}^{n}(x_{i}-\bar{x})^{2}\right)^{2}}\sum_{i=1}^{n}Var(x_{i}y_{i}-\bar{x}y_{i})\\
=\frac{1}{\left(\sum_{i=1}^{n}(x_{i}-\bar{x})^{2}\right)^{2}}\sum_{i=1}^{n}(x_{i}-\bar{x})^{2}Var(y_{i})\\
=\frac{1}{\left(\sum_{i=1}^{n}(x_{i}-\bar{x})^{2}\right)^{2}}\sum_{i=1}^{n}(x_{i}-\bar{x})^{2}\sigma_{y}^{2}\\
=\frac{\sigma_{y}^{2}}{\sum_{i=1}^{n}(x_{i}-\bar{x})^{2}}\]

However since \(y_{i}=\beta_{0}+\beta_{1}x_{i}+\epsilon_{i}\) and
\(\epsilon_{i}\) is the only random variable on the right hand side, we
have:

\[Var(y_{i})=Var(\epsilon_{i})=\sigma^{2}\].

Then our expression above becomes:

\[Var(\beta_{1})= \frac{\sigma^{2}}{\sum_{i=1}^{n}(x_{i}-\bar{x})^{2}}  \]

Since \(\beta_{0}=\bar{y}-\beta_{1}\bar{x}\) we have:

\begin{equation}
\begin{split}
E(\beta_0) &= E[\bar{y}] + \bar{x}E[\beta_1]\\
&= E[\frac{1}{n} \sum_{i=1}^n y_i] + \bar{x}\frac{1}{\sum_{k=1}^n (x_i - \bar{x})^2}E[\sum_{i=1}^n x_i y_i - n\bar{x}\bar{y}]\\
&= \mu_Y + \bar{x}\frac{1}{\sum_{k=1}^n (x_i - \bar{x})^2}[\sum_{i=1}^n x_i E[y_i] - n\bar{x}E[\bar{y}]]\\
&= \mu_Y + \bar{x}\frac{1}{\sum_{k=1}^n (x_i - \bar{x})^2}[\mu_Y \sum_{i=1}^n x_i - n\bar{x}\mu_Y]\\
&= \mu_Y
\end{split}
\end{equation}

and

\begin{equation}
\begin{split}
E(\beta_0^2) &= E[\bar{y}^2 + 2 \beta_1 \bar{x} \bar{y} + \beta_1^2 \bar{x}^2]\\
&= E[\bar{y}^2] + 2 \bar{x} E[\beta_1 \bar{y}] + \bar{x}^2 E[\beta_1^2]\\
&= Var(\bar{y}) + E[\bar{y}]^2 + \bar{x}^2 \left[ Var(\beta_1) + E[\beta_1]^2  \right]\\
&= Var(\bar{y}) + \mu_Y^2 + \bar{x}^2 \left[ Var(\beta_1) \right]\\
&= \frac{\sigma^2}{n} + \mu_Y^2 + \frac{\bar{x}^2 \sigma^2}{\sum_{k=1}^n (x_k - \bar{x})}
\end{split}
\end{equation}

finally

\begin{equation}
\begin{split}
Var(\beta_0) &= E[\beta_0^2] - E[\beta_0]^2\\
&= \sigma^{2}\left[ \frac{1}{n} + \frac{\bar{x}^{2}}{\sum_{i=1}^{n}(x_{i}-\bar{x})^{2}}  \right]
\end{split}
\end{equation}

Note that, with a bit of algebraic manipulation \(\big(\)hint:
\(\sum(x_{i}-\bar{x})^{2}=\sum x_{i}^{2}-n\bar{x}^{2})\)\(\big)\), this
is also equal to:

\[Var(\beta_{0}) = \frac{\sigma^{2}\sum_{i=1}^{n}x_{i}^{2}}{n\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2} } \]


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
